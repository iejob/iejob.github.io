<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[note]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>note</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Mon, 09 Sep 2024 07:17:19 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Mon, 09 Sep 2024 07:17:06 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[카프카를 데이터 파이프라인에 통합하기]]></title><description><![CDATA[ 
 <br><a data-href="python 간단 파이프라인" href="root\dev\data-pipe-line\데이터-파이프라인-구축\python-간단-파이프라인.html" class="internal-link" target="_self" rel="noopener">python 간단 파이프라인</a><br>물론, 카프카(Kafka)를 붙여서 데이터 파이프라인을 확장할 수 있어. Apache Kafka는 대규모 데이터 스트리밍을 처리하는 데 강력한 도구야. 카프카를 데이터 파이프라인에 통합하면 실시간 데이터 스트리밍과 처리를 더 유연하게 할 수 있어.<br><br>
<br>
Kafka 설치 및 설정:

<br>카프카 설치: 카프카와 Zookeeper를 설치해야 해. <a data-tooltip-position="top" aria-label="https://kafka.apache.org/quickstart" rel="noopener" class="external-link" href="https://kafka.apache.org/quickstart" target="_blank">카프카 공식 문서</a>에서 설치 방법을 확인할 수 있어.
<br>카프카 브로커 실행: 카프카 브로커와 Zookeeper를 시작해.


<br>
카프카 Python 클라이언트 라이브러리 설치:
카프카와 상호작용할 파이썬 라이브러리인 confluent-kafka를 설치해야 해:
pip install confluent-kafka
복사

<br>
데이터 생성 및 카프카에 데이터 전송:
CSV 파일의 데이터를 카프카 토픽으로 보내는 예제를 보여줄게:
from confluent_kafka import Producer
import pandas as pd
import json

# 카프카 프로듀서 설정
conf = {
    'bootstrap.servers': 'localhost:9092',  # 카프카 브로커 주소
}
producer = Producer(conf)

# CSV 파일에서 데이터 추출
data = pd.read_csv('data.csv')

# 데이터를 카프카 토픽으로 전송
def delivery_report(err, msg):
    if err is not None:
        print(f"Message delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

for index, row in data.iterrows():
    message = json.dumps({
        'name': row['name'],
        'age': row['age']
    })
    producer.produce('my_topic', key=str(index), value=message, callback=delivery_report)

producer.flush()
Run복사

<br>
카프카에서 데이터를 읽어 MySQL에 적재하기:
카프카에서 데이터를 소비하고, MySQL 데이터베이스에 적재하는 스크립트야:
from confluent_kafka import Consumer
import pymysql
import json

# MySQL 연결 설정
connection = pymysql.connect(
    host='localhost',
    user='ubuntu',
    password='1234',
    database='testdb'
)

# 카프카 소비자 설정
conf = {
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'my_group',
    'auto.offset.reset': 'earliest'
}
consumer = Consumer(conf)
consumer.subscribe(['my_topic'])

try:
    with connection.cursor() as cursor:
        # 테이블 생성 (존재하지 않는 경우)
        create_table_query = """
        CREATE TABLE IF NOT EXISTS users (
            id INT AUTO_INCREMENT PRIMARY KEY,
            name VARCHAR(100),
            age INT
        )
        """
        cursor.execute(create_table_query)

        for message in consumer:
            data = json.loads(message.value().decode('utf-8'))
            insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
            cursor.execute(insert_query, (data['name'], data['age']))
            connection.commit()
finally:
    connection.close()
    consumer.close()
Run복사

<br><br>
<br>
데이터 전송 (카프카 프로듀서):

<br>CSV 파일에서 데이터를 읽고, 각 행을 JSON 형식으로 변환한 후 카프카 토픽(my_topic)으로 전송해.


<br>
데이터 소비 및 MySQL 적재 (카프카 소비자):

<br>카프카에서 메시지를 소비하고, JSON 형식의 데이터를 파싱하여 MySQL 데이터베이스에 적재해.


<br>이렇게 하면 데이터 파이프라인이 확장되어 실시간으로 카프카에서 데이터를 처리하고, MySQL에 적재할 수 있어. 카프카를 추가함으로써 데이터 스트리밍, 처리, 그리고 저장을 훨씬 유연하고 강력하게 할 수 있어.<br><br><br>위 코드에서 MySQL 데이터베이스와 Kafka를 이용하여 데이터를 저장하고 전송하는 작업을 수행하고 있습니다. 각 부분의 작업 기록을 확인할 수 있는 방법은 다음과 같습니다:<br>
<br>
MySQL 작업 기록:

<br>데이터베이스와 테이블 생성: my_data라는 데이터베이스와 users 테이블이 생성되었는지 확인하려면 MySQL에 접속하여 SHOW DATABASES;와 SHOW TABLES IN my_data; 명령을 실행하면 됩니다.
<br>데이터 삽입 기록: users 테이블에 삽입된 데이터는 SELECT * FROM users; 쿼리를 실행하여 확인할 수 있습니다. 이는 코드 내에서도 확인하고 있습니다.


<br>
Kafka 메시지 기록:

<br>Kafka는 주로 로그 파일이나 Kafka 커맨드 라인을 통해 메시지의 기록을 확인할 수 있습니다. Kafka의 kafka-console-consumer 유틸리티를 사용하여 특정 토픽(my_topic)의 메시지를 소비하면서 실시간으로 전송된 데이터를 확인할 수 있습니다.

예를 들어, 터미널에서 다음과 같은 명령을 사용할 수 있습니다:

<br>kafka-console-consumer --bootstrap-server localhost:9092 --topic my_topic --from-beginning
Run복사<br>이 명령은 my_topic에 게시된 모든 메시지를 표시합니다. 이를 통해 어떤 데이터가 전송되었는지 확인할 수 있습니다.]]></description><link>root\dev\data-pipe-line\데이터-파이프라인-구축\간단-파이프-라인에-카프카-붙이기.html</link><guid isPermaLink="false">root/dev/data pipe line/데이터 파이프라인 구축/간단 파이프 라인에 카프카 붙이기.md</guid><pubDate>Sun, 28 Jul 2024 08:28:58 GMT</pubDate></item><item><title><![CDATA[flask kafka 연동]]></title><description><![CDATA[ 
 <br>import json
from flask import Flask, request, jsonify
from kafka import KafkaProducer, KafkaConsumer
import psycopg2
from threading import Thread

app = Flask(__name__)
broker1 = "172.31.6.227:9092"
broker2 = "172.31.14.208:9092"
broker3 = "172.31.1.189:9092"

kafka_bootstrap_servers = [broker1, broker2, broker3]
kafka_topic = "testtopic"

db_host = "172.31.14.208"
db_port = 5432
db_name = "myuser"
db_user = "postgres"
db_password = "postgres"
db_table = "user_table"

def kafka_producer(data):
	producer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers)
	producer.send(kafka_topic, value=json.dumps(data).encode('utf-8'))
	producer.flush()
	pass

def kafka_consumer():
	consumer = KafkaConsumer(kafka_topic, bootstrap_servers=kafka_bootstrap_servers)
	for message in consumer:
		data = json.loads(message.value.decode("utf-8"))
		inser_data_into_postgres(data)
	pass

def inser_data_into_postgres(data):
	conn = psycopg2.connect(host=db_host, port=db_port, database=db_name, user=db_user, password=db_password)
	cursor = conn.cursor()

	query = f"INSERT INTO {db_table} (name, level) VALUES (%s, %s)"
	values = (data['name'], data['level'])

	cursor.execute(query, values)
	conn.commit()

	cursor.close()
	conn.close()
	pass


@app.route("/")
def hello():
	return "hello world"

@app.route("/sendinfo", methods=["POST"])
def receive_data():
	data = request.get_json()
	kafka_producer(data)
	return jsonify(message="success")

if __name__ == "__main__":
	kafka_consumer_thread = Thread(target=kafka_consumer)
	kafka_consumer_thread.start()
	app.run(host="0.0.0.0", port=5000)

	pass


Run복사]]></description><link>root\dev\data-pipe-line\데이터-파이프라인-구축\flask-kafka-연동.html</link><guid isPermaLink="false">root/dev/data pipe line/데이터 파이프라인 구축/flask kafka 연동.md</guid><pubDate>Tue, 23 Jul 2024 16:07:53 GMT</pubDate></item><item><title><![CDATA[python 간단 파이프라인]]></title><description><![CDATA[ 
 <br>간단한 데이터 파이프라인 예제를 통해 데이터 추출, 변환, 적재를 보여줄게. 여기서는 CSV 파일에서 데이터를 추출하고, 변환한 후 MySQL 데이터베이스에 적재하는 과정을 포함할 거야.<br>예제: CSV 파일에서 데이터를 추출하고, 간단한 변환을 한 후 MySQL 데이터베이스에 적재하기<br>
<br>
환경 설정:

<br>pandas 라이브러리: 데이터 처리
<br>sqlalchemy 라이브러리: 데이터베이스 연결 및 데이터 적재
<br>pymysql 라이브러리: MySQL 연결

필요한 라이브러리를 설치해:
pip install pandas sqlalchemy pymysql
복사

<br>
CSV 파일 준비:
예를 들어, data.csv 파일이 다음과 같은 형식을 가질 수 있어:
name,age
Alice,30
Bob,24
Charlie,29
복사

<br>
파이썬 스크립트 작성:
아래는 CSV 파일에서 데이터를 읽고, 변환한 후 MySQL 데이터베이스에 적재하는 스크립트야:

<br>import pandas as pd
from sqlalchemy import create_engine, text
import pymysql

# 데이터베이스 연결 설정
DATABASE_USER = 'ubuntu'
DATABASE_PASSWORD = '1234'
DATABASE_HOST = 'localhost'
DATABASE_NAME = 'my_data'

# 데이터 추출
data = pd.read_csv('data.csv')

# MySQL 서버에 연결 (데이터베이스 생성용)
connection = pymysql.connect(
	host=DATABASE_HOST,
	user=DATABASE_USER,
	password=DATABASE_PASSWORD
)

# 데이터베이스 생성
with connection.cursor() as cursor:
	cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}")
connection.close()

# 데이터베이스에 연결
engine = create_engine(f'mysql+pymysql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}/{DATABASE_NAME}')

# 테이블 존재 여부 확인 및 생성
with engine.connect() as conn:
	result = conn.execute(text("SHOW TABLES LIKE 'test_a'"))
	table_exists = result.fetchone() is not None

if not table_exists:
	# 테이블이 없으면 생성
	with engine.connect() as conn:
		conn.execute(text('''
			CREATE TABLE test_a (
				column1 VARCHAR(255),
				column2 INT,
				column3 DATE
			)
		'''))

# 데이터 적재 (테이블 이름은 'test_a')
data.to_sql('test_a', engine, if_exists='replace', index=False)

print("데이터가 성공적으로 적재되었습니다.")

Run복사<br>
<br>
스크립트 실행:
위의 파이썬 스크립트를 저장하고 실행해보자:
python your_script_name.py
복사

<br>스크립트 설명:<br>
<br>데이터 추출: pandas를 사용해 CSV 파일을 읽어 data 데이터프레임으로 저장.
<br>데이터 변환: 나이 컬럼의 값을 1.5배로 변환. (변환은 예시일 뿐이야, 필요에 따라 다른 변환을 할 수 있어.)
<br>데이터베이스 연결: sqlalchemy의 create_engine을 사용해 MySQL 데이터베이스에 연결.
<br>데이터 적재: to_sql 메서드를 사용해 데이터프레임을 MySQL 데이터베이스에 적재. if_exists='replace'는 테이블이 이미 존재하면 교체하도록 설정해.
<br>이렇게 하면, CSV 파일의 데이터를 MySQL 데이터베이스에 적재하는 간단한 데이터 파이프라인을 구현할 수 있어. 필요에 따라 이 구조를 확장하거나, 추가적인 데이터 처리 단계를 넣어볼 수 있어.]]></description><link>root\dev\data-pipe-line\데이터-파이프라인-구축\python-간단-파이프라인.html</link><guid isPermaLink="false">root/dev/data pipe line/데이터 파이프라인 구축/python 간단 파이프라인.md</guid><pubDate>Mon, 29 Jul 2024 07:42:42 GMT</pubDate></item><item><title><![CDATA[python으로 mysql 파이프 라인 만들기]]></title><description><![CDATA[ 
 <br>물론이지! 파이썬으로 데이터를 MySQL에 삽입하는 간단한 예제를 만들어볼게. 이를 위해 pymysql 라이브러리를 사용할 거야. 이 라이브러리를 사용하면 파이썬에서 MySQL 서버와 쉽게 통신할 수 있어.<br>먼저, pymysql 라이브러리를 설치해야 해:<br>pip install pymysql
복사<br>그리고 아래는 파이썬 스크립트의 예제야:<br>import pymysql

# MySQL 연결 설정
connection = pymysql.connect(
    host='localhost',  # MySQL 서버의 호스트 이름 또는 IP 주소
    user='ubuntu',     # MySQL 사용자 이름
    password='1234',   # MySQL 사용자 비밀번호
    database='testdb'  # 사용할 데이터베이스 이름
)

try:
    with connection.cursor() as cursor:
        # 테이블 생성 (존재하지 않는 경우)
        create_table_query = """
        CREATE TABLE IF NOT EXISTS users (
            id INT AUTO_INCREMENT PRIMARY KEY,
            name VARCHAR(100),
            age INT
        )
        """
        cursor.execute(create_table_query)

        # 데이터 삽입
        insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
        data_to_insert = [
            ('Alice', 30),
            ('Bob', 24),
            ('Charlie', 29)
        ]
        cursor.executemany(insert_query, data_to_insert)

    # 변경사항 커밋
    connection.commit()

    # 데이터 확인
    with connection.cursor() as cursor:
        cursor.execute("SELECT * FROM users")
        result = cursor.fetchall()
        for row in result:
            print(row)

finally:
    connection.close()
Run복사<br>이 스크립트는 다음 단계를 수행해:<br>
<br>MySQL 데이터베이스에 연결.
<br>users 테이블을 생성 (테이블이 이미 존재하면 아무 작업도 하지 않음).
<br>users 테이블에 데이터를 삽입.
<br>삽입된 데이터를 조회하고 출력.
<br>MySQL 연결을 닫음.
<br>스크립트를 실행하기 전에 MySQL 서버에 연결할 수 있는지 확인하고, testdb 데이터베이스가 존재하는지 확인해야 해. 데이터베이스가 없다면, MySQL에서 데이터베이스를 생성해:<br>CREATE DATABASE testdb;
Run복사<br>이제 이 스크립트를 실행하면, 데이터가 users 테이블에 삽입되고 조회 결과가 출력될 거야. 이 예제를 기반으로 데이터 파이프라인을 만들어 나갈 수 있을 거야. 더 궁금한 점이 있으면 알려줘!]]></description><link>root\dev\data-pipe-line\데이터-파이프라인-구축\python으로-mysql-파이프-라인-만들기.html</link><guid isPermaLink="false">root/dev/data pipe line/데이터 파이프라인 구축/python으로 mysql 파이프 라인 만들기.md</guid><pubDate>Wed, 24 Jul 2024 16:45:50 GMT</pubDate></item><item><title><![CDATA[shell ssh 여러 개 접속 테스트]]></title><description><![CDATA[ 
 <br>#!/bin/bash

ips=("server1" "server2" "server3")

# SSH 키 파일 (필요하면 경로 지정)
# key_file="~/.ssh/id_rsa"

# 각 IP에 대해 SSH 접속 시도
for ip in "${ips[@]}"; do
    echo "접속 시도 중: $ip"
    ssh -o ConnectTimeout=5 $ip "echo '접속 성공: $ip'"
    if [ $? -eq 0 ]; then
        echo "$ip 에 성공적으로 접속했어"
    else
        echo "$ip 에 접속 실패"
    fi
done
복사]]></description><link>root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html</link><guid isPermaLink="false">root/dev/data pipe line/자동화/shell ssh 여러 개 접속 테스트.md</guid><pubDate>Sun, 28 Jul 2024 17:04:31 GMT</pubDate></item><item><title><![CDATA[shellscript 자기 자신이 실행되는 경로 알아내기]]></title><description><![CDATA[ 
 <br>#!/bin/bash

SCRIPT_DIR=$(dirname "$0")
python "$SCRIPT_DIR/create.py"
복사<br>이렇게 하면 스크립트가 실행되는 위치에 관계없이 create.py 파일을 올바르게 참조하게 돼. 다시 시도해보고 제대로 작동하는지 확인해줘.]]></description><link>root\dev\data-pipe-line\자동화\shellscript-자기-자신이-실행되는-경로-알아내기.html</link><guid isPermaLink="false">root/dev/data pipe line/자동화/shellscript 자기 자신이 실행되는 경로 알아내기.md</guid><pubDate>Sun, 04 Aug 2024 11:07:48 GMT</pubDate></item><item><title><![CDATA[Airflow note]]></title><description><![CDATA[ 
 <br><br><br><a data-href="docker ubuntu 기본 환경#설치" href="docker ubuntu 기본 환경#설치" class="internal-link" target="_self" rel="noopener">docker ubuntu 기본 환경 &gt; 설치</a><br>
<a data-href="docker ubuntu 기본 환경#가상환경" href="docker ubuntu 기본 환경#가상환경" class="internal-link" target="_self" rel="noopener">docker ubuntu 기본 환경 &gt; 가상환경</a><br>sudo apt update
sudo apt install -y pip
sudo apt-get install python3.12-venv
python3 -m venv airflow_venv 
source airflow_venv/bin/activate
복사<br><br>pip install "apache-airflow[celery]==2.9.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.1/constraints-3.12.txt"
복사<br><br><a rel="noopener" class="external-link" href="https://github.com/hjkim-sun/airflow_2.9.1/blob/master/plugins/sensors/seoul_api_date_column_sensor.py" target="_blank">https://github.com/hjkim-sun/airflow_2.9.1/blob/master/plugins/sensors/seoul_api_date_column_sensor.py</a><br>
<a rel="noopener" class="external-link" href="https://github.com/hjkim-sun/airflow_2.9.1/blob/master/dags/dags_custom_sensor_2.py" target="_blank">https://github.com/hjkim-sun/airflow_2.9.1/blob/master/dags/dags_custom_sensor_2.py</a>]]></description><link>root\dev\data-pipe-line\airflow\airflow-note.html</link><guid isPermaLink="false">root/dev/data pipe line/airflow/Airflow note.md</guid><pubDate>Wed, 19 Jun 2024 00:06:25 GMT</pubDate></item><item><title><![CDATA[Elasticsearch Cerebro]]></title><description><![CDATA[ 
 <br><a rel="noopener" class="external-link" href="https://github.com/lmenezes/cerebro/releases" target="_blank">https://github.com/lmenezes/cerebro/releases</a><br><br><br>wget https://github.com/lmenezes/cerebro/releases/download/v0.9.4/cerebro-0.9.4.tgz
복사<br><br>tar -xvzf cerebro-0.9.4.tgz
복사<br><br>ubuntu@ip-172-31-6-227:~/app/cerebro$ cd cerebro-0.9.4/

ubuntu@ip-172-31-6-227:~/app/cerebro/cerebro-0.9.4$ ls
README.md  bin  conf  lib

ubuntu@ip-172-31-6-227:~/app/cerebro/cerebro-0.9.4$ cd bin/

ubuntu@ip-172-31-6-227:~/app/cerebro/cerebro-0.9.4/bin$ ls
cerebro  cerebro.bat
# 리눅스용 # 윈도우용
복사<br><br><br> vim cerebro
복사<br>들여쓰기 주의  JAVA_OPTS="$JAVA_OPTS --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.file=ALL-UNNAMED"

  # Now we check to see if there are any java opts on the environment. These get listed first, with the script able to override them.
  if [[ "$JAVA_OPTS" != "" ]]; then
    java_opts="${JAVA_OPTS}"
  fi

복사
246 번째 줄<br>JAVA_OPTS="$JAVA_OPTS --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/java.util=ALL-UNNAMED --add-opens java.base/sun.net.www.protocol.file=ALL-UNNAMED"
복사<br><br>인바운드 규칙
<br>9000
<br><br>public_ip:9000
복사<br><br>http://마스터_노드:9200

# http://ip-172-31-1-189:9200
복사]]></description><link>root\dev\data-pipe-line\elasticsearch\elasticsearch-cerebro.html</link><guid isPermaLink="false">root/dev/data pipe line/Elasticsearch/Elasticsearch Cerebro.md</guid><pubDate>Fri, 19 Jul 2024 07:39:09 GMT</pubDate></item><item><title><![CDATA[Elasticsearch Install]]></title><description><![CDATA[ 
 <br><a rel="noopener" class="external-link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html</a><br><br>Important서버 3개에 설치<br><br>wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
복사<br><br>sudo apt-get install apt-transport-https
복사<br>echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
복사<br>sudo apt-get update
복사<br>sudo apt-get -y install elasticsearch
복사<br><br>sudo systemctl status elasticsearch
복사<br>elasticsearch.service - Elasticsearch
     Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; preset: enabled)
     Active: inactive (dead)
       Docs: https://www.elastic.co
복사<br><br><br>Attention
<br>3대의 서버 모두 일괄 적용
<br>node.name은 각자 서버에 맞게
<br><br>["ip-172-31-6-227","ip-172-31-1-189","ip-172-31-14-208"]
복사<br>1, 2, 3 node 넣는다.<br><br>sudo -i
복사<br><br>cd /etc/elasticsearch/
복사<br><br>vi elasticsearch.yml
복사<br><br># ---------------------------------- Cluster -----------------------------------
cluster.name: my-elk-stack

# ------------------------------------ Node ------------------------------------
node.name: ip-172-31-6-227 # 각자 노드 이름

# ---------------------------------- Network -----------------------------------
network.host: 0.0.0.0

http.port: 9200

# --------------------------------- Discovery ----------------------------------
discovery.seed_hosts: ["ip-172-31-6-227","ip-172-31-1-189","ip-172-31-14-208"]

----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------
xpack.security.enabled: false
xpack.security.enrollment.enabled: false
xpack.security.http.ssl:
  enabled: false
xpack.security.transport.ssl:
  enabled: false

cluster.initial_master_nodes: ["ip-172-31-6-227","ip-172-31-1-189","ip-172-31-14-208"]
복사<br><br>Attention3대 모두에서 확인 및 실행<br><br>ssh ip-172-31-6-227
ssh ip-172-31-1-189
ssh ip-172-31-14-208
복사<br><br>sudo /bin/systemctl daemon-reload

sudo /bin/systemctl enable elasticsearch.service

sudo systemctl start elasticsearch.service
복사<br><br>sudo systemctl status elasticsearch
복사<br>curl localhost:9200
복사<br>{
  "name" : "ip-172-31-6-227",
  "cluster_name" : "my-elk-stack",
  "cluster_uuid" : "fw7VOLA4S96P24fZmtpRHQ",
  "version" : {
    "number" : "8.14.3",
    "build_flavor" : "default",
    "build_type" : "deb",
    "build_hash" : "d55f984299e0e88dee72ebd8255f7ff130859ad0",
    "build_date" : "2024-07-07T22:04:49.882652950Z",
    "build_snapshot" : false,
    "lucene_version" : "9.10.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
Run복사<br><br>ubuntu@ip-172-31-6-227:~$ curl localhost:9200/_cat/health?v
epoch      timestamp cluster      status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1721373567 07:19:27  my-elk-stack green           3         3      0   0    0    0        0             0                  -                100.0%
ubuntu@ip-172-31-6-227:~$ curl localhost:9200/_cat/nodes?v
ip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role   master name
172.31.1.189             1          73   1    0.00    0.01     0.01 cdfhilmrstw *      ip-172-31-1-189
172.31.14.208            4          73   0    0.00    0.01     0.00 cdfhilmrstw -      ip-172-31-14-208
172.31.6.227             4          73   0    0.00    0.00     0.00 cdfhilmrstw -      ip-172-31-6-227
복사<br>별 붙어있음]]></description><link>root\dev\data-pipe-line\elasticsearch\elasticsearch-install.html</link><guid isPermaLink="false">root/dev/data pipe line/Elasticsearch/Elasticsearch Install.md</guid><pubDate>Fri, 19 Jul 2024 07:22:52 GMT</pubDate></item><item><title><![CDATA[Hadoop HDFS JAVA API]]></title><description><![CDATA[ 
 <br><a data-href="vscode maven 프로젝트 생성" href="root\dev\data-pipe-line\hadoop\hadoop-수업\실습\vscode-maven-프로젝트-생성.html" class="internal-link" target="_self" rel="noopener">vscode maven 프로젝트 생성</a><br><br>.
|-- pom.xml
`-- src
    `-- main
        `-- java
            `-- FileSystemPrint.java
복사<br><br>&lt;dependencies&gt;
	&lt;dependency&gt;
			&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
			&lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
			&lt;version&gt;3.3.6&lt;/version&gt;
	&lt;/dependency&gt;
&lt;/dependencies&gt;
복사<br><br>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;


import java.io.IOException;
import java.io.InputStream;
import java.net.URI;


public class FileSystemPrint {
  public static void main(String[] args) throws IOException {
    String uri = args[0];
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(URI.create(uri), conf);
    try (InputStream in = fs.open(new Path(uri))) {
      IOUtils.copyBytes(in, System.out, 4096, false);
    }
  }
}
Run복사]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-수업\실습\hadoop-hdfs-java-api.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop 수업/실습/Hadoop HDFS JAVA API.md</guid><pubDate>Wed, 26 Jun 2024 04:51:16 GMT</pubDate></item><item><title><![CDATA[Hadoop Mapreduce]]></title><description><![CDATA[ 
 <br><br><a data-href="vscode maven 프로젝트 생성" href="root\dev\data-pipe-line\hadoop\hadoop-수업\실습\vscode-maven-프로젝트-생성.html" class="internal-link" target="_self" rel="noopener">vscode maven 프로젝트 생성</a><br>
<br>프로젝트 이름: mapreduce
<br><br>.
`-- mapreduce
    |-- data
    |   `-- word.txt
    |-- pom.xml
    `-- src
        `-- main
            `-- java
                `-- org
                    `-- example
                        |-- WordCountDriver.java
                        |-- WordCountMapper.java
                        `-- WordCountReducer.java
복사<br><br>&lt;?xml version="1.0" encoding="UTF-8"?&gt;

&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

  &lt;groupId&gt;com.example&lt;/groupId&gt;
  &lt;artifactId&gt;mapreduce&lt;/artifactId&gt;
  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;

  &lt;name&gt;mapreduce&lt;/name&gt;
  &lt;!-- FIXME change it to the project's website --&gt;
  &lt;url&gt;http://www.example.com&lt;/url&gt;

  &lt;properties&gt;
    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;
    &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;
    &lt;hadoop.version&gt;3.3.6&lt;/hadoop.version&gt;
  &lt;/properties&gt;

  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.11&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
        &lt;version&gt;${hadoop.version}&lt;/version&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
        &lt;version&gt;${hadoop.version}&lt;/version&gt;
      &lt;/dependency&gt;
      &lt;dependency&gt;
        &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
        &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt;
        &lt;version&gt;${hadoop.version}&lt;/version&gt;
      &lt;/dependency&gt;
  &lt;/dependencies&gt;

&lt;/project&gt;

복사<br><br>package org.example;

import org.apache.hadoop.io.IntWritable; // Hadoop의 IntWritable 클래스 임포트
import org.apache.hadoop.io.LongWritable; // Hadoop의 LongWritable 클래스 임포트
import org.apache.hadoop.io.Text; // Hadoop의 Text 클래스 임포트
import org.apache.hadoop.mapreduce.Mapper; // Hadoop의 Mapper 클래스 임포트

import java.io.IOException; // 입출력 예외 처리를 위한 IOException 클래스 임포트

public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

	private Text outputKey = new Text(); // 출력 키를 저장할 Text 객체 생성
	private IntWritable outputValue = new IntWritable(1); // 출력 값을 저장할 IntWritable 객체 생성 (항상 1로 설정)

	@Override
	protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
		// 한 줄의 텍스트를 String으로 변환
		String line = value.toString();
		// 공백을 기준으로 단어를 분리하여 배열에 저장
		String[] words = line.split(" ");
		// 분리된 각 단어에 대해
		for (String word : words) {
			// 단어를 outputKey에 설정
			outputKey.set(word);
			// (단어, 1) 쌍을 컨텍스트에 기록
			context.write(outputKey, outputValue);
		}
	}

}
Run복사<br><br>package org.example; // 패키지 선언

import org.apache.hadoop.io.IntWritable; // Hadoop의 IntWritable 클래스 임포트
import org.apache.hadoop.io.Text; // Hadoop의 Text 클래스 임포트
import org.apache.hadoop.mapreduce.Reducer; // Hadoop의 Reducer 클래스 임포트
import java.io.IOException; // 입출력 예외 처리를 위한 IOException 클래스 임포트

public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

    private IntWritable result = new IntWritable(); // 최종 결과 값을 저장할 IntWritable 객체 생성

    @Override
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        // 값들을 합산할 변수 초기화
        int sum = 0;
        // 각 값에 대해
        for (IntWritable value : values) {
            // 현재 값을 sum에 더함
            sum += value.get();
        }
        // 합산된 결과를 result에 설정
        result.set(sum);
        // (단어, 합계) 쌍을 컨텍스트에 기록
        context.write(key, result);
    }

}
Run복사<br><br>package org.example;

import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

public class WordCountDriver extends Configured implements Tool {

    // Tool 인터페이스의 run 메서드를 오버라이드합니다.
    @Override
    public int run(String[] args) throws Exception {
        // 잡 인스턴스를 생성합니다. 현재 설정을 기반으로 "Word Count"라는 이름을 사용합니다.
        Job job = Job.getInstance(getConf(), "Word Count");

        // 잡의 JAR 파일을 이 클래스의 JAR 파일로 설정합니다.
        job.setJarByClass(getClass());

        // 맵퍼 클래스를 설정합니다.
        job.setMapperClass(WordCountMapper.class);

        // 리듀서 클래스를 설정합니다.
        job.setReducerClass(WordCountReducer.class);

        // 컴바이너 클래스를 설정합니다. (여기서는 리듀서와 동일하게 설정)
        job.setCombinerClass(WordCountReducer.class);

        // 출력 키 클래스를 설정합니다.
        job.setOutputKeyClass(Text.class);

        // 출력 값 클래스를 설정합니다.
        job.setOutputValueClass(IntWritable.class);

        // 입력 경로를 설정합니다.
        FileInputFormat.addInputPath(job, new Path(args[0]));

        // 출력 경로를 설정합니다.
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        // 잡이 완료될 때까지 기다리고, 성공하면 0을 반환하고 실패하면 1을 반환합니다.
        return job.waitForCompletion(true) ? 0 : 1;
    }

    public static void main(String[] args) {
        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        try {
            int exitCode = ToolRunner.run(new WordCountDriver(), args);
            System.exit(exitCode);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

}
Run복사<br><br>Deer Bear River
Car Car River
Deer Car Bear
복사<br><br>hadoop fs -mkdir -p /user/ubuntu/input
hadoop fs -put ./word.txt /user/ubuntu/input/
hadoop fs -ls /user/ubuntu/input/
복사<br><br>“Pasted image 20240625155903.png” could not be found.<br>
<br>클릭: 탐색기
<br>우클릭: maven
<br>클릭: Run Maven commands
<br>클릭: install
<br>확인
<br><br>“Pasted image 20240626145053.png” could not be found.<br>yarn jar 1 org.example.WordCountDriver 2 /user/ubuntu/output/
복사<br>Attention이미지 1, 2를 shell의 1, 2 위치로 붙여넣기<br>yarn jar /home/ubuntu/work/hadoop/mapreduce/mapreduce/target/mapreduce-1.0-SNAPSHOT.jar org.example.WordCountDriver /user/ubuntu/input/word.txt /user/ubuntu/output/
복사<br><br>2024-06-26 14:39:02,936 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
2024-06-26 14:39:03,181 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1719361466459_0001
2024-06-26 14:39:03,317 INFO input.FileInputFormat: Total input files to process : 1
2024-06-26 14:39:03,368 INFO mapreduce.JobSubmitter: number of splits:1
2024-06-26 14:39:03,454 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1719361466459_0001
2024-06-26 14:39:03,456 INFO mapreduce.JobSubmitter: Executing with tokens: []
2024-06-26 14:39:03,561 INFO conf.Configuration: resource-types.xml not found
2024-06-26 14:39:03,561 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2024-06-26 14:39:03,705 INFO impl.YarnClientImpl: Submitted application application_1719361466459_0001
2024-06-26 14:39:03,733 INFO mapreduce.Job: The url to track the job: http://66a2aa90a7fa:8088/proxy/application_1719361466459_0001/
2024-06-26 14:39:03,733 INFO mapreduce.Job: Running job: job_1719361466459_0001
2024-06-26 14:39:08,795 INFO mapreduce.Job: Job job_1719361466459_0001 running in uber mode : false
2024-06-26 14:39:08,795 INFO mapreduce.Job:  map 0% reduce 0%
2024-06-26 14:39:11,838 INFO mapreduce.Job:  map 100% reduce 0%
2024-06-26 14:39:15,854 INFO mapreduce.Job:  map 100% reduce 100%
2024-06-26 14:39:15,860 INFO mapreduce.Job: Job job_1719361466459_0001 completed successfully
2024-06-26 14:39:15,925 INFO mapreduce.Job: Counters: 54
        File System Counters
                FILE: Number of bytes read=50
                FILE: Number of bytes written=472597
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=157
                HDFS: Number of bytes written=28
                HDFS: Number of read operations=8
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
                HDFS: Number of bytes read erasure-coded=0
        Job Counters 
                Launched map tasks=1
                Launched reduce tasks=1
                Data-local map tasks=1
                Total time spent by all maps in occupied slots (ms)=1414
                Total time spent by all reduces in occupied slots (ms)=1521
                Total time spent by all map tasks (ms)=1414
                Total time spent by all reduce tasks (ms)=1521
                Total vcore-milliseconds taken by all map tasks=1414
                Total vcore-milliseconds taken by all reduce tasks=1521
                Total megabyte-milliseconds taken by all map tasks=1447936
                Total megabyte-milliseconds taken by all reduce tasks=1557504
        Map-Reduce Framework
                Map input records=3
                Map output records=9
                Map output bytes=80
                Map output materialized bytes=50
                Input split bytes=113
                Combine input records=9
                Combine output records=4
                Reduce input groups=4
                Reduce shuffle bytes=50
                Reduce input records=4
                Reduce output records=4
                Spilled Records=8
                Shuffled Maps =1
                Failed Shuffles=0
                Merged Map outputs=1
                GC time elapsed (ms)=68
                CPU time spent (ms)=590
                Physical memory (bytes) snapshot=495579136
                Virtual memory (bytes) snapshot=5120688128
                Total committed heap usage (bytes)=458752000
                Peak Map Physical memory (bytes)=296083456
                Peak Map Virtual memory (bytes)=2556694528
                Peak Reduce Physical memory (bytes)=199495680
                Peak Reduce Virtual memory (bytes)=2563993600
        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters 
                Bytes Read=44
        File Output Format Counters 
                Bytes Written=28
복사<br><br><br>hadoop dfs -ls /user/ubuntu/output
복사<br>WARNING: Use of this script to execute dfs is deprecated.
WARNING: Attempting to execute replacement "hdfs dfs" instead.

Found 2 items
-rw-r--r--   1 ubuntu supergroup          0 2024-06-26 14:39 /user/ubuntu/output/_SUCCESS
-rw-r--r--   1 ubuntu supergroup         28 2024-06-26 14:39 /user/ubuntu/output/part-r-00000
복사<br><br>hadoop dfs -text /user/ubuntu/output/part-r-00000
복사<br>WARNING: Use of this script to execute dfs is deprecated.
WARNING: Attempting to execute replacement "hdfs dfs" instead.

Bear    2
Car     3
Deer    2
River   2
복사<br><br><br>“Pasted image 20240626150226.png” could not be found.<br>
<br>입력: localhost:8088
<br>확인: FINISHED, SUCCEEDED
<br>클릭: application_1719361466459_0001
<br>“Pasted image 20240626150325.png” could not be found.]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-수업\실습\hadoop-mapreduce.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop 수업/실습/Hadoop Mapreduce.md</guid><pubDate>Wed, 26 Jun 2024 06:03:38 GMT</pubDate></item><item><title><![CDATA[vscode maven 프로젝트 생성]]></title><description><![CDATA[ 
 <br><a rel="noopener" class="external-link" href="https://hyunchang88.tistory.com/309" target="_blank">https://hyunchang88.tistory.com/309</a><br><br>“Pasted image 20240626132635.png” could not be found.<br>
<br>입력: ctrl + shift + p
<br>생성: 디렉토리

<br>규칙: <a data-tooltip-position="top" aria-label="https://heytech.tistory.com/294" rel="noopener" class="external-link" href="https://heytech.tistory.com/294" target="_blank">스네이크 표기법</a>을 쓸 것


<br>선택: java: create java project
<br><br><br>“Pasted image 20240626132344.png” could not be found.<br>
<br>선택: Maven create from archetype
<br><br><br>“Pasted image 20240626132409.png” could not be found.<br>
<br>선택: maven-archetype-quickstart
<br><br><br>“Pasted image 20240626132434.png” could not be found.<br>
<br>선택: 1.4
<br><br><br>“Pasted image 20240626132453.png” could not be found.<br>
<br>입력: com.example(기본값)
<br><br><br>“Pasted image 20240626132656.png” could not be found.<br>
<br>입력: my_project(스네이크 표기법 사용)
<br><br><br>“Pasted image 20240626132819.png” could not be found.<br>
<br><a class="internal-link" data-href="##프로젝트" href="about:blank##프로젝트" target="_self" rel="noopener">프로젝트</a> 에서 만든 디렉토리 사용
<br><br><br>“Pasted image 20240626132932.png” could not be found.<br>
<br>입력: 엔터
<br>입력: Y
<br>입력: 아무 키
<br><br><br><br>“Pasted image 20240626133026.png” could not be found.<br>Attention마지막에 정하라고 하는데 그 때는 프로젝트 이름과 같은<br>
디렉토리를 만들어서 해주자
.
`-- my_project
    `-- my_project
복사<br><br>“Pasted image 20240626133045.png” could not be found.<br>.
`-- my_project
    `-- my_project
        |-- pom.xml
        |-- src
        |   |-- main
        |   |   `-- java
        |   |       `-- com
        |   |           `-- example
        |   |               `-- App.java
        |   `-- test
        |       `-- java
        |           `-- com
        |               `-- example
        |                   `-- AppTest.java
        `-- target
            |-- classes
            |   `-- com
            |       `-- example
            |           `-- App.class
            `-- test-classes
                `-- com
                    `-- example
                        `-- AppTest.class
복사]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-수업\실습\vscode-maven-프로젝트-생성.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop 수업/실습/vscode maven 프로젝트 생성.md</guid><pubDate>Wed, 26 Jun 2024 04:45:51 GMT</pubDate></item><item><title><![CDATA[docker hadoop 개발 환경 구축]]></title><description><![CDATA[ 
 <br><br><a data-href="docker ubuntu 기본 환경" href="docker ubuntu 기본 환경" class="internal-link" target="_self" rel="noopener">docker ubuntu 기본 환경</a><br><br>docker container run -it -p 9870:9870 -p 8088:8088 -p 8042:8042 -v /home/wsl/work:/home/ubuntu/work --name hadoop-study2 -u ubuntu -w /home/ubuntu ubuntu
복사<br><br>sudo apt-get update
sudo apt-get install -y openssh-server
sudo service ssh start
복사<br><br><br><br>ssh-keygen -t rsa
cat &gt;&gt; ~/.ssh/authorized_keys &lt; ~/.ssh/id_rsa.pub

# 설치 가능한 리스트 업데이트
sudo apt-get -y update

# 업데이트한 패키지들을 최신 버전에 맞게 업그레이드
sudo apt-get -y upgrade

# 의존성까지 체크해서 업그레이드
sudo apt-get -y dist-upgrade

# 필요 라이브러리 설치
sudo apt-get install -y vim wget unzip ssh openssh-* net-tools tree

# Ubuntu 20.4 에는 native libray 인 snappy 가 설치되어 있지 않다.
# 아래 snappy 설치를 하지 않으면 하둡 설치 후 snappy 사용 시 에러가 발생한다.
sudo apt install libsnappy-dev -y


# EC2 Ubuntu terminal

# Java 8 설치
sudo apt-get install -y openjdk-8-jdk

# Java 버전 확인
java -version

# Java 경로 확인
sudo find / -name java-8-openjdk-amd64 2&gt;/dev/null
# /usr/lib/jvm/java-8-openjdk-amd64

복사<br><br># EC2 Ubuntu terminal

# Java 시스템 환경변수 등록 및 활성화
sudo vim /etc/environment

# 아래 내용 추가 후 저장
PATH 뒤에 ":/usr/lib/jvm/java-8-openjdk-amd64/bin" 추가
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

# 시스템 환경변수 활성화
source /etc/environment

# 사용자 환경변수 등록
sudo echo 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' &gt;&gt; ~/.bashrc

# 사용자 환경변수 활성화
source ~/.bashrc
복사<br><br><br><br># EC2 Ubuntu terminal

# 설치파일 관리용 다운로드 디렉토리 생성
mkdir ~/downloads &amp;&amp; cd ~/downloads

# Hadoop 3.2.3 설치
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz

# Hadoop 3.2.3 압축 해제
sudo tar -zxvf hadoop-3.2.3.tar.gz -C /usr/local

# 소유권 변경
sudo chown -R $USER:$USER /usr/local/hadoop-3.2.3

# hadoop 심볼릭 링크 생성
cd /usr/local &amp;&amp; sudo ln -s hadoop-3.2.3 hadoop
복사<br><br># EC2 Ubuntu terminal

# Hadoop 시스템 환경변수 설정
sudo vim /etc/environment

# 아래 내용 추가 후 저장
PATH 뒤에 ":/usr/local/hadoop/bin" 추가
PATH 뒤에 ":/usr/local/hadoop/sbin" 추가
HADOOP_HOME="/usr/local/hadoop"

# 시스템 환경변수 활성화
source /etc/environment

# Hadoop환 사용자 환경변수 설정
#sudo echo 'export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_HOME=/usr/local/hadoop' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_COMMON_HOME=$HADOOP_HOME' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_HDFS_HOME=$HADOOP_HOME' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_YARN_HOME=$HADOOP_HOME' &gt;&gt; ~/.bashrc
sudo echo 'export HADOOP_MAPRED_HOME=$HADOOP_HOME' &gt;&gt; ~/.bashrc

# 이거 안하면 hadoop hash 안됨
sudo echo /etc/environment &gt;&gt; ~/.bashrc
# 사용자 환경변수 활성화
source ~/.bashrc
복사<br><br><br>vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
복사<br># Java
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

# Hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native
export HADOOP_OPTS="${HADOOP_OPTS} -Djava.library.path=$HADOOP_HOME/lib/native"

# For PID
# hadoop-USER-datanode.pid (DataNode)
# hadoop-USER-journalnode.pid (JournalNode)
# hadoop-USER-namenode.pid (NameNode)
# hadoop-USER-zkfc.pid (DFSZKFailoverController)
export HADOOP_PID_DIR=${HADOOP_HOME}/pids
export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}
복사<br><br>vim $HADOOP_HOME/etc/hadoop/core-site.xml
복사<br>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;fs.defaultFS&lt;/name&gt;
		&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
		&lt;value&gt;ubuntu&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
복사<br><br>vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml
복사<br>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;1&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
		&lt;value&gt;/usr/local/hadoop/data/namenode&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
		&lt;value&gt;/usr/local/hadoop/data/datanode&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;
		&lt;value&gt;/usr/local/hadoop/data/dfs/journalnode&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
복사<br><br>vim $HADOOP_HOME/etc/hadoop/mapred-site.xml
복사<br>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
		&lt;value&gt;yarn&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;mapreduce.application.classpath&lt;/name&gt;   
		&lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
복사<br><br>vim $HADOOP_HOME/etc/hadoop/yarn-site.xml
복사<br>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
	&lt;/property&gt;
	&lt;property&gt;
		&lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;
		&lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;
복사<br><br>hdfs namenode -format
복사<br><br>sudo service ssh start
cd $HADOOP_HOME/sbin
./start-all.sh
복사<br><br>hadoop version
hdfs version
복사<br>Hadoop 3.2.3
Source code repository https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131
Compiled by ubuntu on 2022-03-20T01:18Z
Compiled with protoc 2.5.0
From source with checksum 39bb14faec14b3aa25388a6d7c345fe8
This command was run using /usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar
복사<br><br><br>
<br>HDFS : http://localhost:9870
<br>YARN : http://localhost:8088
<br>NodeManager : http://localhost:8042
]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-수업\환경-구축\docker-hadoop-개발-환경-구축.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop 수업/환경 구축/docker hadoop 개발 환경 구축.md</guid><pubDate>Wed, 26 Jun 2024 00:35:31 GMT</pubDate></item><item><title><![CDATA[vscode wsl maven 프로젝트 환경 구축]]></title><description><![CDATA[ 
 <br><br><br>sudo apt-get update
sudo apt-get install mavein
복사<br><br><br><a data-tooltip-position="top" aria-label="https://velog.io/@dkdlel102/vscode%EC%97%90%EC%84%9C-maven-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EB%A5%BC-jar-%ED%8C%8C%EC%9D%BC-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0" rel="noopener" class="external-link" href="https://velog.io/@dkdlel102/vscode%EC%97%90%EC%84%9C-maven-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EB%A5%BC-jar-%ED%8C%8C%EC%9D%BC-%EB%B9%8C%EB%93%9C%ED%95%98%EA%B8%B0" target="_blank">https://velog.io/@dkdlel102/vscode에서-maven-프로젝트를-jar-파일-빌드하기</a><br><br>“Pasted image 20240625152734.png” could not be found.<br>maven for java
복사<br><br>“Pasted image 20240625152714.png” could not be found.<br>java configuration maven: user setting
복사<br>/usr/share/maven/conf/settings.xml
복사<br><br>“Pasted image 20240625155903.png” could not be found.]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-수업\환경-구축\vscode-wsl-maven-프로젝트-환경-구축.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop 수업/환경 구축/vscode wsl maven 프로젝트 환경 구축.md</guid><pubDate>Wed, 26 Jun 2024 04:52:34 GMT</pubDate></item><item><title><![CDATA[hadoop wsl port 정상 작동 로그]]></title><description><![CDATA[ 
 <br>Check
<br>port: [ 8042, 8088, 9870 ]<br>
sudo lsof -i :port 했을 때<br>
아래와 같은 결과가 나와야 한다.
<br><br>Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
tcp        0      0 0.0.0.0:9870            0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.54:53           0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:8042            0.0.0.0:*               LISTEN     
tcp        0      0 0.0.0.0:8088            0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN     
tcp        0      0 127.0.0.1:32921         0.0.0.0:*               LISTEN     
tcp6       0      0 :::9870                 :::*                    LISTEN     
tcp6       0      0 :::8042                 :::*                    LISTEN     
tcp6       0      0 :::8088                 :::*                    LISTEN     
udp        0      0 127.0.0.54:53           0.0.0.0:*                          
udp        0      0 127.0.0.53:53           0.0.0.0:*                          
udp        0      0 127.0.0.1:323           0.0.0.0:*                          
udp6       0      0 ::1:323                 :::*                               
복사<br><br><br><br>&gt;  sudo lsof -i :8088

COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
docker-pr 25876 root    4u  IPv4 396225      0t0  TCP *:omniorb (LISTEN)
docker-pr 25885 root    3u  IPv6 443958      0t0  TCP ip6-localhost:omniorb-&gt;ip6-localhost:37532 (ESTABLISHED)
docker-pr 25885 root    4u  IPv6 401527      0t0  TCP *:omniorb (LISTEN)
docker-pr 25885 root    5u  IPv4 443960      0t0  TCP 172.17.0.1:39330-&gt;172.17.0.2:omniorb (ESTABLISHED)
복사<br><br>&gt;  sudo lsof -i :8042

COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
docker-pr 25857 root    4u  IPv4 398819      0t0  TCP *:8042 (LISTEN)
docker-pr 25869 root    3u  IPv6 437943      0t0  TCP ip6-localhost:8042-&gt;ip6-localhost:38704 (ESTABLISHED)
docker-pr 25869 root    4u  IPv6 396220      0t0  TCP *:8042 (LISTEN)
docker-pr 25869 root    5u  IPv4 437945      0t0  TCP 172.17.0.1:55528-&gt;172.17.0.2:8042 (ESTABLISHED)
복사<br><br>&gt;  sudo lsof -i :9870

COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
docker-pr 25892 root    4u  IPv4 400877      0t0  TCP *:9870 (LISTEN)
docker-pr 25899 root    4u  IPv6 391975      0t0  TCP *:9870 (LISTEN)
복사]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-wsl-port-정상-작동-로그.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop wsl port 정상 작동 로그.md</guid><pubDate>Mon, 01 Jul 2024 14:59:37 GMT</pubDate></item><item><title><![CDATA[hadoop-study docker image]]></title><description><![CDATA[ 
 <br>hahoop, zookeeper maridb, hive<br><br>docker pull namugach/hadoop-study:latest
복사<br><br><br>
<br>create volum dir
<br>mkdir ~/work
복사<br>
<br>create run docker container
<br>docker container run -it -d -p 9870:9870 -p 8088:8088 -p 8042:8042 -v /home/$USER/work:/home/ubuntu/work --name my-hadoop-study namugach/hadoop-study

복사<br><br><br>docker container exec -it my-hadoop-study bash
복사<br><br><br>
<br>start

<br>ssh에 관한 게 나오면 yes를 입력
<br>hadoopStart.sh 이것은 컨테이너를 만들고 나서 처음만 하면 됨.


<br>~/hadoopStart.sh
복사<br>
<br>check
<br>jps
복사<br>
<br>output
<br>400 NameNode
3875 NodeManager
3220 SecondaryNameNode
3540 ResourceManager
8454 Jps
542 DataNode
복사<br>
<br>web

<br>HDFS : http://localhost:9870
<br>YARN : http://localhost:8088
<br>NodeManager : http://localhost:8042


<br><br><br>Attention컨테이너를 바로 부팅하고 시작 하면 안되고, 좀 기다렸다 해야함.<br>
<br>connect
<br>hive
복사<br>
<br>show database
<br>show databases;
복사<br>
<br>output
<br>OK
default
test
Time taken: 0.475 seconds, Fetched: 2 row(s)
복사<br><br><br>
<br>start
<br>~/zkServer.sh start
복사<br>
<br>stop
<br>~/zkServer.sh stop
복사]]></description><link>root\dev\data-pipe-line\hadoop\hadoop-study-docker-image.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/hadoop-study docker image.md</guid><pubDate>Mon, 01 Jul 2024 00:24:27 GMT</pubDate></item><item><title><![CDATA[Hive 트러블 슈팅]]></title><description><![CDATA[ 
 <br><br>wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar
cp mysql-connector-java-8.0.28.jar $HIVE_HOME/lib/
vi $HIVE_HOME/conf/hive-env.sh
복사<br>export HIVE_AUX_JARS_PATH=$HIVE_AUX_JARS_PATH:/path/to/mysql-connector-java-8.0.28.jar
복사<br>$HIVE_HOME/bin/schematool -dbType mysql -initSchema
history 
복사]]></description><link>root\dev\data-pipe-line\hadoop\hive-트러블-슈팅.html</link><guid isPermaLink="false">root/dev/data pipe line/hadoop/Hive 트러블 슈팅.md</guid><pubDate>Wed, 26 Jun 2024 11:55:09 GMT</pubDate></item><item><title><![CDATA[Log Cleansing]]></title><description><![CDATA[ 
 <br><br><br>
<br>삭제하기 이전의 sement 파일 확인 
<br>&gt; ls /tmp/kafka-logs/kv_topic-0
00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  00000000000000000003.snapshot  leader-epoch-checkpoint  partition.metadata

&gt; ls /tmp/kafka-logs/kv_topic-1
00000000000000000000.index  00000000000000000000.log  00000000000000000000.timeindex  00000000000000000002.snapshot  leader-epoch-checkpoint  partition.metadata
복사<br>
<br>kv_topic 데이터 삭제 

<br>직접 삭제하는 것이 아니라, 데이터를 보유하는 기간을 0으로 조정하여, 
<br>그 이전 데이터는 broker가 자동으로 삭제


<br>&gt; bin/kafka-configs.sh --bootstrap-server localhost:9092 --topic kv_topic --add-config retention.ms=0 --alter
Completed updating config for topic kv_topic.
복사<br>
<br>약 5분 정도 후에 다시 segment 파일을 조회하면 기존 segment 파일(00000.log)은 삭제되고
<br>partition 0는 6.log 파일이 새롭게 생성되고, 
<br>partition 1은 3.log 파일이 새롭게 생성되었다. 
<br>&gt; /tmp/kafka-logs/kv_topic-0
00000000000000000000.index.deleted  00000000000000000000.timeindex.deleted  00000000000000000006.log       leader-epoch-checkpoint
00000000000000000000.log.deleted    00000000000000000003.snapshot           00000000000000000006.snapshot  partition.metadata

&gt; ls /tmp/kafka-logs/kv_topic-1
00000000000000000000.index.deleted  00000000000000000000.timeindex.deleted  00000000000000000003.log       leader-epoch-checkpoint
00000000000000000000.log.deleted    00000000000000000002.snapshot           00000000000000000003.snapshot  partition.metadata
복사<br>
<br>변경했던 옵션을 삭제
<br>&gt; bin/kafka-configs.sh --bootstrap-server localhost:9092 --topic kv_topic --delete-config retention.ms --alter
Completed updating config for topic kv_topic.

## Config에 retention.ms 옵션이 삭제됨
&gt; bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic kv_topic --describe
Topic: kv_topic	TopicId: JuKdf41ASVKbZXEKru3iLA	PartitionCount: 2	ReplicationFactor: 1	Configs: segment.bytes=1073741824
	Topic: kv_topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
	Topic: kv_topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
복사<br><br>kafka-topics --create --zookeeper zookeeper:2181 --topic latest-product-price --replication-factor 1 --partitions 1 --config "cleanup.policy=compact" --config "delete.retention.ms=100"  --config "segment.ms=100" --config "min.cleanable.dirty.ratio=0.01"

&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic log_compact_topic \
  --config "cleanup.policy=compact" \
  --config "delete.retention.ms=100"  \
  --config "segment.ms=100" \
  --config "min.cleanable.dirty.ratio=0.01"

&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092

## Produce key-value message 
&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic log_compact_topic \
--property "parse.key=true" \
--property "key.separator=:" \
--property "print.key=true"
k1:msg11
k2:msg21
k3:msg31
k1:msg12
k2:msg22
k3:msg32
k1:msg13
k2:msg23


## 약 5분 정도 후에 아래 consumer를 통해서 메세지를 확인해 본다. 
## 5분은 내부적으로 broker의 log cleaner가 동작하는 시간 고려
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic log_compact_topic \
--property print.key=true \
--property key.separator=":" \
--from-beginning
k2:msg22
k3:msg32
k1:msg13
k2:msg23 &lt;-- 아마도 이 메시지는 active segment에 있기 때문에 중복 제거가 되지 않았을 듯
복사]]></description><link>root\dev\data-pipe-line\kafka\group\01.kafka_basic\log_compaction.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/01.kafka_basic/Log_compaction.md</guid><pubDate>Wed, 07 Aug 2024 00:18:22 GMT</pubDate></item><item><title><![CDATA[Apache Kafka 기본 명령어 활용 예시]]></title><description><![CDATA[ 
 <br><br><br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2 --topic kv_topic

&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
__consumer_offsets
kv_topic
mytopic


&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic kv_topic \
--property "parse.key=true" \
--property "key.separator=:" \
--property "print.key=true"
k1:msg1
k2:msg2
k3:msg3
k4:msg4
k5:msg5
k6:msg6
k7:msg7

&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic --from-beginning

msg1
msg2
msg3
msg4
msg5
msg6
msg7

&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic \
--property print.key=true \
--property key.separator="-" \
--group my-group \
--from-beginning

k1-msg1
k2-msg2
k6-msg6
k3-msg3
k4-msg4
k5-msg5
k7-msg7


복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
my-group
console-consumer-64400


&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --describe
GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
my-group        kv_topic        0          4               4               0               console-consumer-00714fae-a38e-4c52-9b7b-83e431210802 /10.178.0.4     console-consumer
my-group        kv_topic        1          3               3               0               console-consumer-00714fae-a38e-4c52-9b7b-83e431210802 /10.178.0.4     console-consumer
복사<br><br>
<br>my-group 내에 2개의 consumer를 실행한다. 
<br>## Consumer 0 (my-group)
&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic \
--property print.key=true \
--property key.separator="-" \
--group my-group \
--from-beginning

## Consumer 1 (my-group)
&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic \
--property print.key=true \
--property key.separator="-" \
--group my-group \
--from-beginning

## Consumer group 확인 
## 2개의 consumer에 각 partition 1개씩 할당되어 처리하고 있음.  
&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-group --describe

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                           HOST            CLIENT-ID
my-group        kv_topic        0          4               4               0               console-consumer-49fe00c1-4f0e-4fe4-b33f-489165b540de /10.178.0.4     console-consumer
my-group        kv_topic        1          3               3               0               console-consumer-49fe00c1-4f0e-4fe4-b33f-489165b540de /10.178.0.4     console-consumer

## Key/Value 데이터 전송 
## 동일한 Key 데이터가 항상 동일한 consumer로 전달 되는지 확인 가능 
&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic kv_topic \
--property "parse.key=true" \
--property "key.separator=:" \
--property "print.key=true"
k1:msg1
k2:msg2
k3:msg3
k4:msg4
k5:msg5
k1:msg1
k2:msg2
k3:msg3
k4:msg4
k5:msg5
k1:msg1
k2:msg2

## Consumer 0의 출력 결과
k3-msg3
k4-msg4
k5-msg5
k3-msg3
k4-msg4
k5-msg5

## Consumer 1의 출력 결과
k1-msg1
k2-msg2
k1-msg1
k2-msg2
k1-msg1
k2-msg2

# Consumer 중지
복사<br><br>
<br>partition 옵션을 통해서 특정 partition만 읽도록 조정 가능 
<br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic \
  --partition 1 \
  --from-beginning

msg1
msg2
msg6
msg1
msg2
msg1
msg2
msg1
msg2

복사<br><br>
<br>offset 0는 from-beginning과 동일한 효과 
<br>&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic \
  --partition 1 \
  --offset 0

msg1
msg2
msg6
msg1
msg2
msg1
msg2
msg1
msg2

복사<br><br>
<br>기본 producer은 데이터만 전달하는거에 비해 멱등성 producer은 데이터를 전달할 때 producer PID와 시퀀스 넘버를 함께 전달하여 중복 데이터 적재를 방지한다. 
<br>시퀀스 넘버는 데이터를 전달할 때마다 시퀀스 넘버를 1씩 증가하여 보내며, broker는 데이터를 전달받을 때 PID와 시퀀스 넘버를 이용해 같은 데이터인지를 확인하고 같은 데이터인 경우 데이터를 저장하지 않는다.
<br>&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; vi producer.conf
enable.idempotence=true
max.in.flight.request.per.connection=5
retries=3
acks=all

&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic kv_topic --producer.config producer.conf
k8:msg8
k9:msg9
k10:msg10
k11:msg11
k12:msg12
복사<br><br>
<br>topic 내의 지정한 partition의 시작점에서 특정 offset 까지의 범위를 삭제한다. 
<br><br>{
    "partitions": [
    {"topic": "kv_topic", "partition":0, "offset": 2}
    ], 
    "version":1
}
복사<br>
<br>중간의 특정 데이터만 삭제할 수는 없음
<br># partition 0번의 offset 0~2 까지의 데이터를 삭제한다. 
&gt; cd ~/apps/kafka_2.12-3.6.2

# 삭제하기 이전의 데이터 확인
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic   --partition 0   --from-beginning
msg3
msg4
msg5
msg7
msg3
msg4
msg5
msg3
msg4
msg5


# 데이터 삭제를 위한 설정 파일 생성
&gt; vi delete_offset.json

&gt; bin/kafka-delete-records.sh --bootstrap-server localhost:9092 --offset-json-file ./delete_offset.json
Executing records delete operation
Records delete operation completed:
partition: kv_topic-0	low_watermark: 2

&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic kv_topic   --partition 0   --from-beginning
## msg3, msg4 데이터가 삭제됨
msg5
msg7
msg3
msg4
msg5
msg3
msg4
msg5
...
복사<br><br><br>&gt; bin/kafka-dump-log.sh --print-data-log --files /tmp/kafka-logs/kv_topic-0/00000000000000000000.log

Dumping /tmp/kafka-logs/kv_topic-0/00000000000000000000.log
Starting offset: 0   &lt;-- offset 시작점

## offset 0 번째 데이터 정보 출력  
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1648305071780 size: 74 magic: 2 compresscodec: none crc: 585719343 isvalid: true
| offset: 0 CreateTime: 1648305071780 keySize: 2 valueSize: 4 sequence: -1 headerKeys: [] key: k3 payload: msg3


## offset 0~2 번째 데이터 정보 출력 
baseOffset: 0 lastOffset: 2 count: 3 baseSequence: 0 lastSequence: 2 producerId: 1000 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1719919955151 size: 100 magic: 2 compresscodec: none crc: 2553173963 isvalid: true
| offset: 0 CreateTime: 1719919955151 keySize: 2 valueSize: 4 sequence: 0 headerKeys: [] key: k3 payload: msg3
| offset: 1 CreateTime: 1719919955151 keySize: 2 valueSize: 4 sequence: 1 headerKeys: [] key: k4 payload: msg4
| offset: 2 CreateTime: 1719919955151 keySize: 2 valueSize: 4 sequence: 2 headerKeys: [] key: k5 payload: msg5

## offset 3 번째 데이터 정보 출력 
baseOffset: 3 lastOffset: 3 count: 1 baseSequence: 3 lastSequence: 3 producerId: 1000 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 100 CreateTime: 1719919956175 size: 74 magic: 2 compresscodec: none crc: 3724116375 isvalid: true
| offset: 3 CreateTime: 1719919956175 keySize: 2 valueSize: 4 sequence: 3 headerKeys: [] key: k7 payload: msg7

## offset 4 ~ 9 번째 데이터 정보 출력 
baseOffset: 4 lastOffset: 9 count: 6 baseSequence: 0 lastSequence: 5 producerId: 1001 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 174 CreateTime: 1719920692528 size: 139 magic: 2 compresscodec: none crc: 1245378216 isvalid: true
| offset: 4 CreateTime: 1719920692527 keySize: 2 valueSize: 4 sequence: 0 headerKeys: [] key: k3 payload: msg3
| offset: 5 CreateTime: 1719920692528 keySize: 2 valueSize: 4 sequence: 1 headerKeys: [] key: k4 payload: msg4
| offset: 6 CreateTime: 1719920692528 keySize: 2 valueSize: 4 sequence: 2 headerKeys: [] key: k5 payload: msg5
| offset: 7 CreateTime: 1719920692528 keySize: 2 valueSize: 4 sequence: 3 headerKeys: [] key: k3 payload: msg3
| offset: 8 CreateTime: 1719920692528 keySize: 2 valueSize: 4 sequence: 4 headerKeys: [] key: k4 payload: msg4
| offset: 9 CreateTime: 1719920692528 keySize: 2 valueSize: 4 sequence: 5 headerKeys: [] key: k5 payload: msg5


## partition 0의 offset이 2번 부터 시작함. (이전에 삭제한 0,1 offet은 제외)
&gt; bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/kv_topic-0/00000000000000000000.timeindex
Dumping /tmp/kafka-logs/kv_topic-0/00000000000000000000.timeindex
timestamp: 0 offset: 0
The following indexed offsets are not found in the log.
Indexed offset: 0, found log offset: 2

복사<br><br>&gt; cat /tmp/kafka-logs/replication-offset-checkpoint
kv_topic 0 10 &lt;-- partition 0은 마지막 commit offset이 10 (현재 9번 offset까지 존재)
kv_topic 1 14 &lt;-- partition 0은 마지막 commit offset이 14 (현재 13번 offset까지 존재)
복사<br><br>&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic kv_topic \
 --property "parse.key=true" \
 --property "key.separator=:" \
 --property "print.key=true"
k20:msg20
k21:msg21

&gt; cat /tmp/kafka-logs/replication-offset-checkpoint
kv_topic 0 12 &lt;-- partition 0에만 위 2건이 추가된 commit 확인 가능
kv_topic 1 14
복사<br><br>
<br>현재 운영중인 Kafka broker에 적용된 모든 설정 값을 확인 가능
<br>&gt; bin/kafka-configs.sh --bootstrap-server localhost:9092 --broker 0 --describe --all
All configs for broker 0 are:
  advertised.listeners=null sensitive=false synonyms={}
  alter.config.policy.class.name=null sensitive=false synonyms={}
  alter.log.dirs.replication.quota.window.num=11 sensitive=false synonyms={DEFAULT_CONFIG:alter.log.dirs.replication.quota.window.num=11}
  alter.log.dirs.replication.quota.window.size.seconds=1 sensitive=false synonyms={DEFAULT_CONFIG:alter.log.dirs.replication.quota.window.size.seconds=1}
  authorizer.class.name= sensitive=false synonyms={DEFAULT_CONFIG:authorizer.class.name=}
  auto.create.topics.enable=true sensitive=false synonyms={DEFAULT_CONFIG:auto.create.topics.enable=true}
  auto.include.jmx.reporter=true sensitive=false synonyms={DEFAULT_CONFIG:auto.include.jmx.reporter=true}
  auto.leader.rebalance.enable=true sensitive=false synonyms={DEFAULT_CONFIG:auto.leader.rebalance.enable=true}
  background.threads=10 sensitive=false synonyms={DEFAULT_CONFIG:background.threads=10}
  broker.heartbeat.interval.ms=2000 sensitive=false synonyms={DEFAULT_CONFIG:broker.heartbeat.interval.ms=2000}

.....
복사<br><br>
<br>
<a rel="noopener" class="external-link" href="https://github.com/onlybooks/kafka2" target="_blank">https://github.com/onlybooks/kafka2</a> 실전 카프카 개발부터 운영까지

<br>
<a rel="noopener" class="external-link" href="https://github.com/bjpublic/apache-kafka-with-java" target="_blank">https://github.com/bjpublic/apache-kafka-with-java</a> 아파치 카프카 애플리케이션 프로그래밍 with 자바

<br>
<a rel="noopener" class="external-link" href="https://github.com/bstashchuk/apache-kafka-course" target="_blank">https://github.com/bstashchuk/apache-kafka-course</a> The Complete Apache Kafka Practical Guide

<br><br>
<br>
<a rel="noopener" class="external-link" href="https://github.com/jingene/fastcampus_kafka_handson" target="_blank">https://github.com/jingene/fastcampus_kafka_handson</a> Part 2. 실무에서 쉽게 써보는 Kafka

<br>
<a rel="noopener" class="external-link" href="https://github.com/fast-campus-lecture" target="_blank">https://github.com/fast-campus-lecture</a> Part 3. Spring for Apache Kafka

<br>
<a rel="noopener" class="external-link" href="https://github.com/freepsw/kafka-metrics-monitoring" target="_blank">https://github.com/freepsw/kafka-metrics-monitoring</a> Part 4. 실시간 모니터링을 위한 Kafka 매트릭 이해

]]></description><link>root\dev\data-pipe-line\kafka\group\01.kafka_basic\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/01.kafka_basic/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Real-time data pipeline using apache kafka]]></title><description><![CDATA[ 
 <br><br><br><br>
<br>kafka_basic/README.md에 명시된 가이드에 따라서
<br>java 설치 및 JAVA_HOME 설정
<br><br>
<br>Elasticsearch를 비즈니에서 활용시 주의사항 (OSS버전 vs Default)

<br>OSS는 elasticsearch를 이용하여 별도의 제품/솔루션으로 판매할 목적인 경우에 활용
<br>Basic은 기업 내부에서는 무료로 사용가능 

<br>즉 OSS 버전을 기반으로 elastic사에서 추가기능(ML, SIEM등)을 무료로 제공하는 것


<br>정리하면, OSS는 누구나 활용 가능한 오픈소스

<br>이를 이용해 별도의 제품을 만들어도 가능함.
<br>elastic사도 OSS를 이용해서 basic 제품을 개발하고, 이를 무료로 제공함. 
<br>하지만, basic 버전의 소유권은 elastic사에 귀속됨(무로지만, 이를 이용해 비즈니스/사업을 하면 안됨)


<br><a rel="noopener" class="external-link" href="http://kimjmin.net/2020/06/2020-06-elastic-devrel/" target="_blank">http://kimjmin.net/2020/06/2020-06-elastic-devrel/</a>


<br>Elastic stack 설치

<br>

<br><a rel="noopener" class="external-link" href="https://www.elastic.co/guide/en/elastic-stack/current/installing-elastic-stack.html" target="_blank">https://www.elastic.co/guide/en/elastic-stack/current/installing-elastic-stack.html</a> 참고




<br><br>
<br>config 설정 

<br>외부 접속 허용(network.host) : server와 client가 다른 ip가 있을 경우, 외부에서 접속할 수 있도록 설정을 추가해야함.


<br>&gt; cd ~/apps
&gt; wget wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.14.1-linux-x86_64.tar.gz
&gt; tar -xzf elasticsearch-8.14.1-linux-x86_64.tar.gz
&gt; cd ~/apps/elasticsearch-8.14.1
&gt; vi config/elasticsearch.yml
# bind ip to connect from client  (lan이 여러개 있을 경우 외부에서 접속할 ip를 지정할 수 있음.)
# bind all ip server have "0.0.0.0"
network.host: 0.0.0.0   #(":" 다음에 스페이스를 추가해야 함.)

# kibana에서 보안정책 없이 접근 가능하도록 "false"로 변경
xpack.security.enabled: false

복사<br><br>
<br>시스템의 nmap count를 증가기켜야 한다.
<br>에러 : [2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
<br><a rel="noopener" class="external-link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html</a>
<br># 0) 현재 설정 값 확인
&gt; cat /proc/sys/vm/max_map_count
65530

# 아래 3가지 방법 중 1가지를 선택하여 적용 가능
# 1-1) 현재 서버상태에서만 적용하는 방식
&gt; sudo sysctl -w vm.max_map_count=262144

# 1-2) 영구적으로 적용 (서버 재부팅시 자동 적용)
&gt; sudo vi /etc/sysctl.conf

# 아래 내용 추가
vm.max_map_count = 262144

# 1-3) 또는 아래 명령어 실행 
&gt; echo vm.max_map_count=262144 | sudo tee -a /etc/sysctl.conf


# 3) 시스템에 적용하여 변경된 값을 확인
&gt; sudo sysctl -p
vm.max_map_count = 262144
복사<br><br>&gt; ./bin/elasticsearch
....................
[2024-07-02T12:41:39,687][INFO ][o.e.l.ClusterStateLicenseService] [instance-20240701-162846] license [2ce86f9d-ae5b-47c0-815a-35e4ad0d9ae2] mode [basic] - valid
[2024-07-02T12:41:46,485][INFO ][o.e.x.s.InitialNodeSecurityAutoConfiguration] [instance-20240701-162846] HTTPS has been configured with automatically generated certificates, and the CA's hex-encoded SHA-256 fingerprint is [6500b8c8df356e4965da2f1692d8569a4e3058f04522f945fd699b76d5c81c64]
[2024-07-02T12:41:46,489][INFO ][o.e.x.s.s.SecurityIndexManager] [instance-20240701-162846] security index does not exist, creating [.security-7] with alias [.security]
[2024-07-02T12:41:46,506][INFO ][o.e.x.s.e.InternalEnrollmentTokenGenerator] [instance-20240701-162846] Will not generate node enrollment token because node is only bound on localhost for transport and cannot connect to nodes from other hosts
[2024-07-02T12:41:46,559][INFO ][o.e.c.m.MetadataCreateIndexService] [instance-20240701-162846] [.security-7] creating index, cause [api], templates [], shards [1]/[0]
[2024-07-02T12:41:46,617][INFO ][o.e.x.s.s.SecurityIndexManager] [instance-20240701-162846] security index does not exist, creating [.security-7] with alias [.security]
[2024-07-02T12:41:46,839][INFO ][o.e.c.r.a.AllocationService] [instance-20240701-162846] current.health="GREEN" message="Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.security-7][0]]])." previous.health="YELLOW" reason="shards started [[.security-7][0]]"


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.

ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  ZG73I6D*OjUTT06oqZ4h

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  6500b8c8df356e4965da2f1692d8569a4e3058f04522f945fd699b76d5c81c64

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjE0LjAiLCJhZHIiOlsiMTAuMTc4LjAuNDo5MjAwIl0sImZnciI6IjY1MDBiOGM4ZGYzNTZlNDk2NWRhMmYxNjkyZDg1NjlhNGUzMDU4ZjA0NTIyZjk0NWZkNjk5Yjc2ZDVjODFjNjQiLCJrZXkiOiJSc1YzYzVBQk5JX1ppY3hPSlk5RjpjdGtUUWcwc1NidWpxX2wxTU5uVlBBIn0=

ℹ️  Configure other nodes to join this cluster:
• On this node:
  ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  ⁃ Restart Elasticsearch.
• On other nodes:
  ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token &lt;token&gt;`, using the enrollment token that you generated.
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

복사<br><br>
<br>Console을 이용한 접근
<br>&gt; export ELASTIC_PASSWORD="ZG73I6D*OjUTT06oqZ4h"

&gt; curl --cacert ~/apps/elasticsearch-8.14.1/config/certs/http_ca.crt -u elastic:$ELASTIC_PASSWORD https://localhost:9200 
{
  "name" : "instance-20240701-162846",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "BjsJfIIxSQySPV90paP3TQ",
  "version" : {
    "number" : "8.14.1",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "93a57a1a76f556d8aee6a90d1a95b06187501310",
    "build_date" : "2024-06-10T23:35:17.114581191Z",
    "build_snapshot" : false,
    "lucene_version" : "9.10.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}

## 아래와 같은 방법으로 접근도 가능함
&gt; curl -k -v -u elastic https://34.64.166.195:9200
Enter host password for user 'elastic': ZG73I6D*OjUTT06oqZ4h
복사<br>
<br>Web Browser를 이용한 접근

<br>Web brower 주소 창에서 "https://Elastic-server-ip:9200/" 입력
<br>팝업 창에서 user/password를 elastid/위에서 저장한 password 로 입력
<br>web에서 elasticsearch 정보 확인


<br><br>&gt; cd ~/apps
&gt; curl -O https://artifacts.elastic.co/downloads/kibana/kibana-8.14.1-linux-x86_64.tar.gz
&gt; tar -xzf kibana-8.14.1-linux-x86_64.tar.gz
&gt; cd ~/apps/kibana-8.14.1/
복사<br><br>
<br>외부 접속 가능하도록 설정 값 변경 

<br>외부의 어떤 IP에서도 접속 가능하도록 0.0.0.0으로 변경 (운영환경에서는 특정 ip대역만 지정하여 보안강화)


<br>elasticsearch 접속을 위한 user/password 설정
<br>&gt;  
# 외부에서 접근 가능하도록 설정 
server.host: "0.0.0.0"

복사<br><br>&gt; cd ~/apps/kibana-8.14.1/ 
&gt; bin/kibana
.....
  log   [10:40:10.296] [info][server][Kibana][http] http server running at http://localhost:5601
  log   [10:40:12.690] [warning][plugins][reporting] Enabling the Chromium sandbox provides an additional layer of protection
복사<br><br>curl -XDELETE http://localhost:9200/.kibana
curl -XDELETE 'http://localhost:9200/.kibana*'
curl -XDELETE http://localhost:9200/.kibana_2
curl -XDELETE http://localhost:9200/.kibana_1
복사<br><br>&gt; cd ~/apps
&gt; wget https://artifacts.elastic.co/downloads/logstash/logstash-8.14.1-linux-x86_64.tar.gz
&gt; tar xvf logstash-8.14.1-linux-x86_64.tar.gz
&gt; cd ~/apps/logstash-8.14.1
복사<br><br>&gt; bin/logstash -e 'input { stdin { } } output { stdout {} }'
# 실행까지 시간이 소요된다. (아래 메세지가 출력되면 정상 실행된 것으로 확인)
.........
[2024-07-02T15:42:37,938][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=&gt;"main"}
The stdin plugin is now waiting for input:
[2024-07-02T15:42:37,953][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}

mytest  &lt;-- 메세지 입력 후 아래와 같이 출력되면 정상적으로 설치된 것
{
    "host" =&gt; {
        "hostname" =&gt; "instance-20240701-162846"
    },
    "event" =&gt; {
        "original" =&gt; "mytest"
    },
    "@version" =&gt; "1",
    "message" =&gt; "mytest",
    "@timestamp" =&gt; 2024-07-02T15:44:03.085998069Z
}
복사<br><br><br>
<br>실습에 사용할 topic을 생성한다. 
<br>&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic realtime

# check created topic "realtime"
&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
__consumer_offsets
kv_topic
mytopic
realtime
복사<br><br>&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic realtime
This is a message
This is another message
복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic realtime --from-beginning

This is a message
This is another message
복사<br><br><br>&gt; cd ~/apps
&gt; wget https://github.com/freepsw/demo-spark-analytics/raw/master/00.stage1/tracks.csv
&gt; touch tracks_live.csv
복사<br><br>
<br>kafka topic을 realtime로 변경
<br>&gt; vi ~/apps/producer.conf
복사<br>
<br>아래 path의 경로를 다운로드 받은 파일의 경로로 변경
<br>input {
  file {
    path =&gt; "/home/freepsw/apps/tracks_live.csv"
  }
}

output {
  stdout {
    codec =&gt; rubydebug{ }
  }

  kafka {
    codec =&gt; plain {
      format =&gt; "%{message}"
    }
    bootstrap_servers =&gt; "localhost:9092"
    topic_id =&gt; "realtime"
  }
}

복사<br>
<br>run logstash 
<br>&gt; cd ~/apps/
&gt; ~/apps/logstash-8.14.1/bin/logstash -f ~/apps/producer.conf
.....
[2024-07-02T16:06:52,012][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}
복사<br><br>&gt; cd ~/apps
&gt; vi data_generator.py
복사<br><br>#-*- coding: utf-8 -*-
import time
import random

r_fname = "tracks.csv"
w_fname = "tracks_live.csv"

rf = open(r_fname)
wf = open(w_fname, "a+")

try:
	num_lines = sum(1 for line in rf)
	print(num_lines)
	#num_lines = 10

	rf.seek(0)
	lines = 0
	while (1):
		line = rf.readline()
		wf.write(line)
		wf.flush()

		# sleep for weighted time period
		stime = random.choice([1, 1, 1, 0.5, 0.5, 0.8, 0.3, 2, 0.1, 3])
		print(stime)
		time.sleep(stime)
		lines += 1

		# exit if read all lines
		if(lines == num_lines):
			break
		# if(lines == num_lines):
		# 	rf.seek(0)
finally:
	rf.close()
	wf.close()
	print("close file")
Run복사<br><br>&gt; cd ~/apps
&gt; python data_generator.py
복사<br><br>
<br>logstash에서 kafka로 정상적으로 메세지가 전송되고 있는지 모니터링
<br>아래의 kafka-console-consumer 명령어를 통해 전송되는 메세지를 확인
<br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic realtime
# logstash에서 정상적으로 메세지를 보내면, 아래와 같은 메세지가 출력될 것임.
0,48,453,"2014-10-23 03:26:20",0,"72132"
1,1081,19,"2014-10-15 18:32:14",1,"17307"
2,532,36,"2014-12-10 15:33:16",1,"66216
복사<br><br>&gt; cd ~/apps
&gt; vi ~/apps/consumer.conf
복사<br>input {
  kafka{
    topics =&gt; ["realtime"]
    bootstrap_servers =&gt; "localhost:9092"
  }
}

filter {
  csv {
    columns =&gt; ["event_id","customer_id","track_id","datetime","ismobile","listening_zip_code"]
    separator =&gt; ","
  }

  date {
    match =&gt; [ "datetime", "YYYY-MM-dd HH:mm:ss"]
    target =&gt; "datetime"
  }

  mutate {
    convert =&gt; { "ismobile" =&gt; "integer" }
  }
}

output {
  stdout { codec =&gt; rubydebug }

  elasticsearch {
    hosts =&gt; "http://localhost:9200"
    index =&gt; "ba_realtime"
  }
} 
복사<br><br>&gt; cd ~/apps
&gt; ~/apps/logstash-8.14.1/bin/logstash --path.data ~/apps/consumer-data -f ~/apps/consumer.conf
복사<br><br><br>
<br>http://vm-instance-ip:5601 
]]></description><link>root\dev\data-pipe-line\kafka\group\02.kafka_pipeline\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/02.kafka_pipeline/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Run Apache kafka &amp; Zookeeper using JMX Port]]></title><description><![CDATA[ 
 <br><br><br><br>&gt; echo "export KAFKA_HOME=~/apps/kafka_2.12-3.0.0" &gt;&gt; ~/.bash_profile
&gt; source ~/.bash_profile
복사<br><br><br>&gt; cd $KAFKA_HOME

# 1) Foreground 실행 (테스트 용으로 zookeeper 로그를 직접 확인)
&gt; bin/zookeeper-server-start.sh config/zookeeper.properties

# 2) Background 실행
&gt; bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
&gt; ps -ef | grep zookeeper
복사<br><br><br>
<br>Kafka는 내부망에서 데이터를 빠르게 송/수신 할 수 있도록
<br>내부망에서 Broker 접근할 수 있는 네트워크 주소와 (listeners)
<br>외부방에서 Broker에 접근할 수 있는 네트워크 주소를 (advertised.listeners)
<br>분리하여 지정한다.
<br>&gt; cd $KAFKA_HOME
## producer 또는 consumer에서 broker에 접속할 때 사용 가능한 hostname 설정 
&gt; vi config/server.properties
advertised.listeners=PLAINTEXT://broker01:9092

## java.rmi.server.hostname은 본인의 외부 IP로 변경
### rmi.server.hostname을 설정해야, 외부의 jconsole과 같은 도구에서 접속 가능
&gt; export KAFKA_JMX_OPTS='-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false 
  -Dcom.sun.management.jmxremote.ssl=false 
  -Dcom.sun.management.jmxremote.port=9999 
  -Dcom.sun.management.jmxremote.rmi.port=9999 
  -Djava.rmi.server.hostname=34.85.125.73'

# 1) Foregroud 
&gt; env JMX_PORT=9999 bin/kafka-server-start.sh config/server.properties

# 2) background 실행
&gt; env JMX_PORT=9999 bin/kafka-server-start.sh -daemon config/server.properties
복사<br><br>
&gt; cd $KAFKA_HOME
&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic kafka-mon \
--partitions 2 --replication-factor 1

# 생성된 topic 확인
&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092 
kafka-mon
복사<br><br>
<br>JDK가 설치된 노트북에 아래 명령어 실행. 
<br>IP:JMX_PORT 입력하여 연결
<br>&gt; jconsole
복사]]></description><link>root\dev\data-pipe-line\kafka\group\03.kafka_monitoring\01.setup_kafka.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/03.kafka_monitoring/01.setup_kafka.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[STEP 0. Connect to vm server using ssh]]></title><description><![CDATA[ 
 <br><br>
<br>"broker-01" 서버로 접속한 후, 아래의 명령어를 순서대로 실행한다. 
<br><br><br>&gt; sudo yum install -y java

# 현재 OS 설정이 한글인지 영어인지 확인한다. 
&gt; alternatives --display java

# 아래와 같이 출력되면 한글임. 
슬레이브 unpack200.1.gz: /usr/share/man/man1/unpack200-java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64.1.gz
현재 '최고' 버전은 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre/bin/java입니다.

### 한글인 경우 
&gt; alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||'
&gt; export JAVA_HOME=$(alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||')

### 영문인 경우
&gt; alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||' | sed 's/^ //g'
&gt; export JAVA_HOME=$(alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||' | sed 's/^ //g')

# 제대로 java 경로가 설정되었는지 확인
# 아래 명령어를 실행하면, "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre"와 유사한 경로가 보여야 함. 
&gt; echo $JAVA_HOME

&gt; echo "export JAVA_HOME=$JAVA_HOME" &gt;&gt; ~/.bash_profile
&gt; source ~/.bash_profile
복사<br><br>&gt; cd ~
&gt; curl -OL https://github.com/jiaqi/jmxterm/releases/download/v1.0.2/jmxterm-1.0.2-uber.jar
복사<br><br>## jmxterm 실행
&gt; java -jar jmxterm-1.0.2-uber.jar
Welcome to JMX terminal. Type "help" for available commands.

## broker jmx port에 접속
$&gt;open localhost:9999

## 조회 가능한 mbeans domain(metric 그룹)을 확인한다. 
$&gt;domains

## 위 domain 중에 조회할 domain을 선택한다. 
$&gt;domain kafka.server

## "kafka.server" domain에서 제공하는 beans 목록을 확인한다. 
$&gt;beans

## bean 중에서 "MessageInPerSec" metric을 선택한다. 
$&gt;bean kafka.server:name=MessagesInPerSec,type=BrokerTopicMetrics

## "MessageInPerSec"에서 조회 가능한 값의 유형을 선택한다.
$&gt;info

## info에서 출력된 목록 중 원하는 값의 유형을 선택한다. 
$&gt;get Count

$&gt;get MeanRate

## JMXTERM 종료
$&gt;bye
#bye
복사<br>
<br>jmxterm에서 제공하는 명령어 유형
<br>$&gt;help
#following commands are available to use:
about    - Display about page
bean     - Display or set current selected MBean.
beans    - List available beans under a domain or all domains
bye      - Terminate console and exit
close    - Close current JMX connection
domain   - Display or set current selected domain.
domains  - List all available domain names
exit     - Terminate console and exit
get      - Get value of MBean attribute(s)
help     - Display available commands or usage of a command
info     - Display detail information about an MBean
jvms     - List all running local JVM processes
open     - Open JMX session or display current connection
option   - Set options for command session
quit     - Terminate console and exit
run      - Invoke an MBean operation
set      - Set value of an MBean attribute
subscribe - Subscribe to the notifications of a bean
unsubscribe - Unsubscribe the notifications of an earlier subscribed bean
watch    - Watch the value of one MBean attribute constantly
복사]]></description><link>root\dev\data-pipe-line\kafka\group\03.kafka_monitoring\02.setup_jmxterm.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/03.kafka_monitoring/02.setup_jmxterm.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[AKHQ를 설치하고 Broker를 연결하여 상태 모니터링]]></title><description><![CDATA[ 
 <br><br>
<br>따라서 java11을 설치할 별도의 서버를 생성한 후, 해당 서버에 ssh로 접속 필요
<br>vm 이름 : "kafka-monitoring"
<br><br>
<br>"kafka-monitoring" 서버로 접속한 후, 아래의 명령어를 순서대로 실행한다. 
<br><br><br>
<br>AKHQ는 java 11 부터 설치가 가능함. 
<br>## 설치 가능한 java version 확인
&gt; yum list java*jdk-devel
Available Packages
java-1.6.0-openjdk-devel.x86_64
java-1.7.0-openjdk-devel.x86_64 
java-1.8.0-openjdk-devel.i686 
java-1.8.0-openjdk-devel.x86_64  
java-11-openjdk-devel.i686  
java-11-openjdk-devel.x86_64
java-latest-openjdk-devel.x86_64 

## jdk 11 설치
&gt; sudo yum install -y java-11-openjdk-devel.x86_64

## 설치된 java version 확인
&gt; java -version
openjdk version "11.0.13" 2021-10-19 LTS
복사<br><br>&gt; cd ~
&gt; curl -LO https://github.com/tchiotludo/akhq/releases/download/0.19.0/akhq.jar
복사<br><br>## 모니터링 할 broker 접속 정보를 config 파일에 추가
&gt; vi ~/akhq_config_simple.yml

akhq:
  connections:
    local:
      properties:
        bootstrap.servers: "broker01:9092"


## AKHQ 실행
&gt; cd ~
&gt; java -Dmicronaut.config.files=akhq_config_simple.yml -jar akhq.jar
복사<br><br>
<br>CMAK에 접속하기 위한 8080 PORT 허용
<br><br>
<br>http://:8080
]]></description><link>root\dev\data-pipe-line\kafka\group\03.kafka_monitoring\03.setup_akhq.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/03.kafka_monitoring/03.setup_akhq.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[04.setup_elastic]]></title><description><![CDATA[ 
 <br><br>
<br>"broker01" 서버로 접속한 후, apache kafka 실행

<br>apache kafka 실행 (zookeeper, broker) : jmx port 적용


<br><br>
<br>"kafka-monitoring" 서버에 접속하여 아래 명령어 실행 
<br><br><br>
<br>[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
<br>file descriptor 갯수를 증가시켜야 한다.
<br>에러 : [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]
<br><a rel="noopener" class="external-link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/setting-system-settings.html#limits.conf" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/setting-system-settings.html#limits.conf</a>
<br>&gt; sudo vi /etc/security/limits.conf
# 아래 내용 추가 
* hard nofile 70000
* soft nofile 70000
root hard nofile 70000
root soft nofile 70000

# 적용을 위해 콘솔을 닫고 다시 연결한다. (console 재접속)
# 적용되었는지 확인
&gt; ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 59450
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 70000  #--&gt; 정상적으로 적용됨을 확인함
복사<br><br>
<br>시스템의 nmap count를 증가기켜야 한다.
<br>에러 : [2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
<br><a rel="noopener" class="external-link" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html</a>
<br># 0) 현재 설정 값 확인
&gt; cat /proc/sys/vm/max_map_count
65530

# 아래 3가지 방법 중 1가지를 선택하여 적용 가능
# 1-1) 현재 서버상태에서만 적용하는 방식
&gt; sudo sysctl -w vm.max_map_count=262144

# 1-2) 영구적으로 적용 (서버 재부팅시 자동 적용)
&gt; sudo vi /etc/sysctl.conf
# 아래 내용 추가
vm.max_map_count = 262144

# 1-3) 또는 아래 명령어 실행 
&gt; echo vm.max_map_count=262144 | sudo tee -a /etc/sysctl.conf

# 3) 시스템에 적용하여 변경된 값을 확인
&gt; sudo sysctl -p
vm.max_map_count = 262144
복사<br><br>&gt; curl -LO https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.10.2-linux-x86_64.tar.gz
&gt; tar -xzf elasticsearch-oss-7.10.2-linux-x86_64.tar.gz
&gt; rm -rf elasticsearch-oss-7.10.2-linux-x86_64.tar.gz
복사<br><br>
<br>conf/elasticsearch.yml
<br>master host 설정 (cluster.initial_master_nodes) : Master Node의 후보를 명시하여, Master Node 다운시 새로운 Master로 선출한다.
<br>설정하지 않을 경우, elasticsearch 실행 시 아래 오류 발생
<br>[3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configured
복사<br>
<br>따라서 아래와 같이 설정 필요
<br>&gt; cd ~/elasticsearch-7.10.2
&gt; vi config/elasticsearch.yml

## Master Node의 후보 서버 목록을 적어준다. (여기서는 1대 이므로 본인의 서버 정보만)
cluster.initial_master_nodes: ["kakfa-monitoring"]

## 위의 설정에서 IP를 입력하면, 아래 오류 발생
    - skipping cluster bootstrapping as local node does not match bootstrap requirements: [34.64.85.xx]
    - master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and [cluster.initial_master_nodes] is empty on this node
복사<br><br>&gt; cd ~/elasticsearch-7.10.2/

## 아래 2가지 방식으로 실행 가능 

### 1) foreground로 실행시 명령어
&gt; ./bin/elasticsearch 

### 2) background(daemon)으로 실행시 명령어 (실행된 프로세스의 pid 값을 elastic_pid 파일에 기록)
&gt; ./bin/elasticsearch -d -p elastic_pid

### daemon으로 실행시 서비스 종료하려면 아래 명령어 실행 (기록된 pid 값을 읽어와서 프로세스 종료)
&gt; pkill -F elastic_pid
복사<br><br>&gt; curl -X GET "localhost:9200/?pretty"
복사<br><br><br>&gt; cd ~
&gt; curl -OL https://artifacts.elastic.co/downloads/kibana/kibana-oss-7.10.2-linux-x86_64.tar.gz
&gt; tar xvf kibana-oss-7.10.2-linux-x86_64.tar.gz 
&gt; rm -rf kibana-oss-7.10.2-linux-x86_64.tar.gz
&gt; cd kibana-7.10.2-linux-x86_64/

## 외부에서 접속 할 수 있도록 설정한다. 
&gt; vi config/kibana.yml

## 아래 내용 추가 (0.0.0.0은 모든 ip에서 접근이 가능한 설정. 운영환경에서는 특정 IP로 제한 필요)
server.host: "0.0.0.0"
복사<br><br>&gt; cd ~/kibana-7.10.2-linux-x86_64

## foreground 실행
&gt; bin/kibana

## background 실행
&gt; nohup bin/kibana &amp;
복사<br><br>
<br>kibana가 현재 실행중. (설정이나 재시작이 필욯지 않다면 현재 실해 중인 kibana 사용)
<br>재시작이 필요하다면, 아래와 같이 process id를 찾아서 해당 process를 종료한 후, kibana 재시작
<br>## 마지막 칼럼의 16998이 process id (process id는 개인별로 다르게 표시됨)
&gt; netstat -anp | grep 5601
tcp        0      0 0.0.0.0:5601            0.0.0.0:*               LISTEN      16998/bin/../node/b

## process 종료
&gt; kill -9 16998
복사<br><br><br>&gt; cd ~
&gt; curl -OL https://artifacts.elastic.co/downloads/logstash/logstash-oss-7.10.2-linux-x86_64.tar.gz
&gt; tar xvf logstash-oss-7.10.2-linux-x86_64.tar.gz
&gt; rm -rf logstash-oss-7.10.2-linux-x86_64.tar.gz
&gt; cd ~/logstash-7.10.2

## install jmx plugin
&gt; bin/logstash-plugin install logstash-input-jmx
복사<br><br>&gt; mkdir ~/jmx_conf
&gt; vi ~/jmx_conf/broker01.conf

{
  "host" : "broker01", 
  "port" : 9999,
  "alias" : "broker01",
  "queries" : [
  {
    "object_name" : "kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec",
    "attributes" : [ "OneMinuteRate" ],
    "object_alias" : "${type}.${name}"
  },
  {
    "object_name" : "kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec",
    "attributes" : [ "OneMinuteRate" ],
    "object_alias" : "${type}.${name}"
  },
  {
    "object_name" : "kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec",
    "attributes" : [ "OneMinuteRate" ],
    "object_alias" : "${type}.${name}"
  },
  {
    "object_name" : "kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions",
    "attributes" : [ "Value" ],
    "object_alias" : "${type}.${name}"
  },
  {
    "object_name" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce",
    "attributes" : [ "Mean" ],
    "object_alias" : "${type}.${name}"
  },
  {
    "object_name" : "kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer",
    "attributes" : [ "Mean" ],
    "object_alias" : "${type}.${name}"
  }
 ]
}
복사<br><br>
<br>broker01.conf에 정의된 metric 들을 수집하여, 
<br>elasticsearch로 전송하는 logstash 설정
<br>&gt; mkdir ~/logstash_conf
&gt; vi ~/logstash_conf/logstash_jmx.conf
# 아래 내용 입력 
input {
 jmx {
  path =&gt; "/home/freepsw.10/jmx_conf"
  polling_frequency =&gt; 2
 }
}

output{
 stdout {
  codec =&gt; rubydebug
 }
 elasticsearch {
   hosts =&gt; "localhost:9200"
   index =&gt; "kafka_mon"
 }
}


&gt;  cd ~/apps/logstash-7.10.1
## install jmx plugin
&gt; bin/logstash-plugin install logstash-input-jmx

&gt; bin/logstash -f ~/logstash_conf/logstash_jmx.conf
복사<br><br>&gt; curl -X GET "localhost:9200/_cat/indices/"
yellow open kafka_mon 6ULkg30FT7aQuxAI7ua7yg 1 1 18 0   32kb   32kb
복사<br><br>
<br>Kibana 접속하기 위한 5601 PORT 허용
<br><br>
<br>http://:5601
]]></description><link>root\dev\data-pipe-line\kafka\group\03.kafka_monitoring\04.setup_elastic.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/03.kafka_monitoring/04.setup_elastic.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Monitoring the apache kafka using jmx metrics]]></title><description><![CDATA[ 
 <br><br><br>
<br>실습에 필요한 라이브러리 설치
<br><br>&gt; sudo yum install -y java

# 현재 OS 설정이 한글인지 영어인지 확인한다. 
&gt; alternatives --display java

# 아래와 같이 출력되면 한글임. 
슬레이브 unpack200.1.gz: /usr/share/man/man1/unpack200-java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64.1.gz
현재 '최고' 버전은 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre/bin/java입니다.

### 한글인 경우 
&gt; alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||'
&gt; export JAVA_HOME=$(alternatives --display java | grep '현재 /'| sed "s/현재 //" | sed 's|/bin/java로 링크되어 있습니다||')

### 영문인 경우
&gt; alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||' | sed 's/^ //g'
&gt; export JAVA_HOME=$(alternatives --display java | grep current | sed 's/link currently points to //' | sed 's|/bin/java||' | sed 's/^ //g')

# 제대로 java 경로가 설정되었는지 확인
&gt; echo $JAVA_HOME
&gt; echo "export JAVA_HOME=$JAVA_HOME" &gt;&gt; ~/.bash_profile
&gt; source ~/.bash_profile
복사]]></description><link>root\dev\data-pipe-line\kafka\group\03.kafka_monitoring\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/03.kafka_monitoring/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Apache Kafka Connect]]></title><description><![CDATA[ 
 <br><br><br>
<br><a rel="noopener" class="external-link" href="https://kafka.apache.org/quickstart" target="_blank">https://kafka.apache.org/quickstart</a>
<br><br>
<br>Kafka Connect에서 사용할 library를 추가한다. 
<br>&gt; cd #KAFKA_HOME 
&gt; vi config/connect-standalone.properties

# 아래 항목 추가 
plugin.path=lib/connect-file-3.2.0.jar
복사<br><br>&gt; cd $KAFKA_HOME
&gt; echo -e "foo\nbar" &gt; test.txt
복사<br><br><br>&gt; cd $KAFKA_HOME 
&gt; vi config/connect-file-source.properties
복사<br>
<br>connect-file-source.properties
<br>name=local-file-source
connector.class=FileStreamSource
tasks.max=1
file=test.txt
topic=connect-test
복사<br><br>&gt; cd $KAFKA_HOME 
&gt; vi config/connect-file-sink.properties
복사<br><br>&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
복사<br><br>&gt; cd $KAFKA_HOME
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
{"schema":{"type":"string","optional":false},"payload":"foo"}
{"schema":{"type":"string","optional":false},"payload":"bar"}
복사]]></description><link>root\dev\data-pipe-line\kafka\group\04.kafka_connect\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/04.kafka_connect/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Apache Kafka Producer &amp; Consumer 구현 (Java)]]></title><description><![CDATA[ 
 <br><br>
<br><a rel="noopener" class="external-link" href="https://developer.confluent.io/tutorials/kafka-producer-callback-application/confluent.html" target="_blank">https://developer.confluent.io/tutorials/kafka-producer-callback-application/confluent.html</a>
<br><br>&gt; sudo yum install -y git maven

&gt; cd ~
&gt; git clone https://github.com/freepsw/RTS_Practice
&gt; cd ~/RTS_Practice/02.kaka_practice/02.producer_consumer_code/java/
&gt; cd my-kafka-java/
복사<br><br><br>        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
            &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
            &lt;version&gt;3.0.0&lt;/version&gt;
        &lt;/dependency&gt;
복사<br><br>&gt; mvn clean package
복사<br><br>## topic을 생성하지 않았다면, 아래와 같이 생성한다. 
&gt; cd $KAFKA_HOME
&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2 --topic my_topic


복사<br><br><br>
<br>확인을 위한 consumer 먼저 실행
<br>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my_topic \
--property print.key=true \
--property key.separator="-" \
--group my-group \
--from-beginning
복사<br>
<br>producer 실행 테스트
<br>&gt; cd ~/RTS_Practice/02.kaka_practice/02.producer_consumer_code/java/my-kafka-java/

&gt; mvn exec:java -Dexec.mainClass="Producer_Simple"
ProducerRecord(topic=my_topic, partition=null, headers=RecordHeaders(headers = [], isReadOnly = true), key=null, value=simple producer message, timestamp=null)


&gt; mvn exec:java -Dexec.mainClass="Producer_Callback_Sync"
Record written to offset 25 timestamp 1648373316114

&gt; mvn exec:java -Dexec.mainClass="Producer_Callback_Async"
Record written to offset 28 timestamp 1648373544238

복사<br><br>
<br>Consumer로 데이터를 전송하기 위한 producer 실행 
<br>&gt; cd $KAFKA_HOME
&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my_topic \
--property "parse.key=true" \
--property "key.separator=:" \
--property "print.key=true"

k1:simple msg
k2:commit_auto
k3:commit_sync
k4:commit_sync_offset
k5:commit_async
복사<br>&gt; cd ~/RTS_Practice/02.kaka_practice/02.producer_consumer_code/java/my-kafka-java/

&gt; mvn exec:java -Dexec.mainClass="Consumer_Simple"

ConsumerRecord(topic = my_topic, partition = 1, leaderEpoch = 0, offset = 25, CreateTime = 1648373795837, serialized key size = 7, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = sgarcia, value = gift card)

&gt; mvn exec:java -Dexec.mainClass="Consumer_Commit_Auto"

ConsumerRecord(topic = my_topic, partition = 0, leaderEpoch = 0, offset = 32, CreateTime = 1648373867522, serialized key size = 8, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = jbernard, value = batteries)


&gt; mvn exec:java -Dexec.mainClass="Consumer_Commit_Sync"

ConsumerRecord(topic = my_topic, partition = 0, leaderEpoch = 0, offset = 46, CreateTime = 1648373922585, serialized key size = 8, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = jbernard, value = batteries)"


&gt; mvn exec:java -Dexec.mainClass="Consumer_Commit_Sync_Offset"


&gt; mvn exec:java -Dexec.mainClass="Consumer_Commit_Async"

ConsumerRecord(topic = my_topic, partition = 0, leaderEpoch = 0, offset = 52, CreateTime = 1648374010211, serialized key size = 6, serialized value size = 9, headers = RecordHeaders(headers = [], isReadOnly = false), key = eabara, value = batteries)
Commit succeeded
복사<br><br><br>
<br><a rel="noopener" class="external-link" href="https://kafka.apache.org/25/javadoc/overview-summary.html" target="_blank">https://kafka.apache.org/25/javadoc/overview-summary.html</a>
<br><br>
<br><a rel="noopener" class="external-link" href="https://developer.confluent.io/tutorials/kafka-producer-callback-application/confluent.html#initialize-the-project" target="_blank">https://developer.confluent.io/tutorials/kafka-producer-callback-application/confluent.html#initialize-the-project</a>
]]></description><link>root\dev\data-pipe-line\kafka\group\05.producer_consumer_code\java\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/05.producer_consumer_code/java/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Apache Kafka Producer &amp; Consumer 구현 (Python)]]></title><description><![CDATA[ 
 <br><br>
<br>Python library (confluent-kafka)를 활용한 구현
<br><a rel="noopener" class="external-link" href="https://github.com/confluentinc/confluent-kafka-python" target="_blank">https://github.com/confluentinc/confluent-kafka-python</a>
<br><a rel="noopener" class="external-link" href="https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent-kafka-api" target="_blank">https://docs.confluent.io/platform/current/clients/confluent-kafka-python/html/index.html#confluent-kafka-api</a>
<br><br>&gt; mkdir ~/python_kafka
&gt; cd ~/python_kafka 

# python 3 가상환경 생성
&gt; sudo yum install -y python3 
&gt; python3 -m venv kafka_virtualenv

## 가상환경 실행하기 
&gt; source kafka_virtualenv/bin/activate
&gt; python -V

# kafka library 설치 
&gt; pip install --upgrade pip
&gt; pip install confluent-kafka==1.8.2  
복사<br><br><br>&gt; cd $KAFKA_HOME
&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2 --topic my_topic
복사<br><br><br><br>&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my_topic --from-beginning
복사<br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

## python code 작성
&gt; vi producer.py

## 메시지 전송 후 consumer에서 정상 수신하는지 확인 
&gt; python producer.py
Message delivered to my_topic [1]
복사<br><br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

## python code 작성
&gt; vi consumer.py

## 메시지 전송 후 consumer에서 정상 수신하는지 확인 
&gt; python consumer.py
Received message: test-msg
복사<br><br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

&gt; vi admin.py 

&gt; python admin.py 
Topic my_topic_new1 created
Topic my_topic_new2 created

&gt; cd $KAFKA_HOME
&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
my_topic_new1
my_topic_new2

복사<br><br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

&gt; vi consumer_detail.py 

&gt; python consumer_detail.py localhost my_group my_topic

Assignment: [TopicPartition{topic=my_topic,partition=0,offset=-1001,error=None}, TopicPartition{topic=my_topic,partition=1,offset=-1001,error=None}]
% my_topic [1] at offset 0 with key None:
b'test-msg'
% my_topic [1] at offset 1 with key None:
b'test-msg'
% my_topic [1] at offset 2 with key None:
b'test-msg'
% my_topic [1] at offset 3 with key None:
b'test-msg'
% my_topic [0] at offset 0 with key None:
b'test-msg'
복사<br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

&gt; vi producer_detail.py 

&gt; python producer_detail.py localhost my_topic
new msg
my msg
% Message delivered to my_topic [0] @ 1
복사<br>
<br>consumer에서 정상적으로 수신하는지 확인한다. 
<br><br>&gt; cd ~/python_kafka 
&gt; source kafka_virtualenv/bin/activate

&gt; vi producer_key.py 

&gt; python producer_key.py 

&gt; python producer_key.py
Message delivered to my_topic [0]
Message delivered to my_topic [0]
Message delivered to my_topic [0]
.....
복사<br><br>&gt; deactivate
복사<br><br><br>
<br><a rel="noopener" class="external-link" href="https://deeplearning.lipingyang.org/2018/12/23/anaconda-vs-miniconda-vs-virtualenv/" target="_blank">https://deeplearning.lipingyang.org/2018/12/23/anaconda-vs-miniconda-vs-virtualenv/</a>
<br><a rel="noopener" class="external-link" href="https://stackoverflow.com/questions/38217545/what-is-the-difference-between-pyenv-virtualenv-anaconda" target="_blank">https://stackoverflow.com/questions/38217545/what-is-the-difference-between-pyenv-virtualenv-anaconda</a>
<br><br>
<br>python과 관련된 모든 어플리케이션, 패키지 등을 자동으로 설치
<br>약 720개 이상의 package들을 자동으로 설치해 줌. 
<br>최소 3G 이상의 디스크 공간 필요  
<br><br>
<br>python package를 관리하기 위한 용도로 최적화
<br>Anaconda에서 제공하는 다양한 어플리케이션은 설치되지 않음
<br>단순히 python 가상환경을 구성하여 프로그래밍하는 목적으로 최적
<br>디스크 사용량 최소화 (다만 필요한 패키지는 직접 설치)
<br><br>
<br>python 버전별로 별도의 python 가상환경을 제공하는 용도

<br>pyenv는 python 버전 별로 환경을 제공했다면, (동일 package의 다른 버전 설치시 관리 어려움)
<br>virtualenv는 동일한 python 버전이라도,
<br>프로젝트에 따라 동일한 package의 다른 버전을 설치할 수 있도록 지원


<br>sudo(관리자 권한)이 있는 경우에만 사용 가능함. 
<br>conda 보다 복잡한 명령어 이해와 관리가 필요
<br><br>
<br>python 버전을 쉽게 설치 및 변경할 수 있게 지원
<br>동일한 버전의 python에 서로 다른 버전의 package(numpy 등)을 설치하면
<br>버전 호환등의 문제로 실행시 오류가 발생하는 이슈 (사용자가 직접 관리해야 함)

<br>즉, 완벽한 독립적인 가상환경을 제공하지는 못함. 


<br>windows는 지원하지 않음
<br><br>&gt; pip install virtualenv
&gt; virtualenv venv_ --python=python3.8
&gt; source venv/bin/activate
&gt; python -m pip install --upgrade pip
&gt; 
Run복사]]></description><link>root\dev\data-pipe-line\kafka\group\05.producer_consumer_code\python\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/05.producer_consumer_code/python/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[Apache Kafka : Data Message Queue]]></title><description><![CDATA[ 
 <br><br><br>
<br>CentOS 9 
<br>실습에 필요한 라이브러리 설치
<br><br>
<br>참고 링크 : <a rel="noopener" class="external-link" href="https://www.linkedin.com/pulse/how-install-kafka-centos-aimasterlive-xp0xc/" target="_blank">https://www.linkedin.com/pulse/how-install-kafka-centos-aimasterlive-xp0xc/</a>
<br>&gt; sudo dnf install java-17-openjdk java-17-openjdk-devel

&gt; java --version
openjdk version "17.0.6" 2023-01-17 LTS
OpenJDK Runtime Environment (Red_Hat-17.0.6.0.10-3.el9) (build 17.0.6+10-LTS)
OpenJDK 64-Bit Server VM (Red_Hat-17.0.6.0.10-3.el9) (build 17.0.6+10-LTS, mixed mode, sharing)

# java home 경로 확인
&gt; ls /usr/lib/jvm/jre-17-openjdk
bin  conf  include  legal  lib  release  tapset

# JAVA_HOME 설정
&gt; echo 'export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))' | sudo tee -a /etc/profile

&gt; source /etc/profile

&gt; echo $JAVA_HOME
/usr/lib/jvm/java-17-openjdk-17.0.6.0.10-3.el9.x86_64

&gt; ls $JAVA_HOME
bin  conf  include  legal  lib  release  tapset

복사<br><br><br>&gt; sudo dnf install -y  wget
&gt; mkdir ~/apps
&gt; cd ~/apps/
&gt; wget https://downloads.apache.org/kafka/3.6.2/kafka_2.12-3.6.2.tgz
&gt; tar -xzvf kafka_2.12-3.6.2.tgz
복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2

# 1) Foreground 실행 (테스트 용으로 zookeeper 로그를 직접 확인)
&gt; bin/zookeeper-server-start.sh config/zookeeper.properties

# 2) Background 실행
&gt; bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
&gt; ps -ef | grep zookeeper
복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2

# 1) Foregroud 
&gt; bin/kafka-server-start.sh config/server.properties

# 2) background 실행
&gt; bin/kafka-server-start.sh -daemon config/server.properties

복사<br><br>
<br>config/server.properties의 log.dir에 정의된 파일

<br><a rel="noopener" class="external-link" href="https://github.com/apache/kafka/blob/3.0/config/server.properties" target="_blank">https://github.com/apache/kafka/blob/3.0/config/server.properties</a>


<br># A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs
복사<br>
<br>아직 topic이 생성되지 않아서, 메타정보 관련된 파일만 생성됨. 
<br>&gt; ls /tmp/kafka-logs/
cleaner-offset-checkpoint    meta.properties                   replication-offset-checkpoint
log-start-offset-checkpoint  recovery-point-offset-checkpoint
복사<br>
<br>메타 정보 출력
<br>## Broker 관련 메타 정보를 출력 
&gt; cat /tmp/kafka-logs/meta.properties
#
#Sun Mar 27 00:10:12 UTC 2022
cluster.id=OX9OhkJmRZWJ9xFCE8CIgw
version=0
broker.id=0
복사<br><br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test

&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
복사<br><br>
<br>__consumer_offsets 파티션정보 (첫번째 topic 생성시 자동으로 생성됨)
<br>&gt; ls /tmp/kafka-logs/
cleaner-offset-checkpoint    meta.properties                   replication-offset-checkpoint
log-start-offset-checkpoint  recovery-point-offset-checkpoint  test-0
.....
복사<br>
<br>.log 파일에 데이터가 없음. (전송된 메지가 없음)
<br>&gt; ls -al /tmp/kafka-logs/test-0total 16
drwxr-xr-x.  2 freepsw18 freepsw18      167 Jul  1 16:59 .
drwxr-xr-x. 53 freepsw18 freepsw18     4096 Jul  1 17:01 ..
-rw-r--r--.  1 freepsw18 freepsw18 10485760 Jul  1 16:59 00000000000000000000.index
-rw-r--r--.  1 freepsw18 freepsw18      160 Jul  1 17:00 00000000000000000000.log
-rw-r--r--.  1 freepsw18 freepsw18 10485756 Jul  1 16:59 00000000000000000000.timeindex
-rw-r--r--.  1 freepsw18 freepsw18        8 Jul  1 16:59 leader-epoch-checkpoint
-rw-r--r--.  1 freepsw18 freepsw18       43 Jul  1 16:59 partition.metadata

복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
my message
second message
복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
my message
second message

복사<br><br>
<br>__consumer_offsets 파티션 생성 (첫번째 topic 생성시 자동으로 생성됨)
<br>&gt; ls /tmp/kafka-logs/
__consumer_offsets-0   __consumer_offsets-22  __consumer_offsets-36  __consumer_offsets-5
.....
복사<br>
<br>.log 파일에 실제 데이터가 저장됨 (기본 7일간 데이터 보관, 이후에 자동 삭제)
<br>&gt; ls -al /tmp/kafka-logs/test-0
drwxr-xr-x.  2 freepsw18 freepsw18      167 Jul  1 16:59 .
drwxr-xr-x. 53 freepsw18 freepsw18     4096 Jul  1 17:02 ..
-rw-r--r--.  1 freepsw18 freepsw18 10485760 Jul  1 16:59 00000000000000000000.index
-rw-r--r--.  1 freepsw18 freepsw18      160 Jul  1 17:00 00000000000000000000.log
-rw-r--r--.  1 freepsw18 freepsw18 10485756 Jul  1 16:59 00000000000000000000.timeindex
-rw-r--r--.  1 freepsw18 freepsw18        8 Jul  1 16:59 leader-epoch-checkpoint
-rw-r--r--.  1 freepsw18 freepsw18       43 Jul  1 16:59 partition.metadata


복사<br>
<br>log 파일의 내용 확인 
<br>&gt; cd ~/apps/kafka_2.12-3.6.2

## log 파일 확인 
&gt; bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/test-0/00000000000000000000.log
Dumping /tmp/kafka-logs/test-0/00000000000000000000.log
Log starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: 0 lastSequence: 0 producerId: 0 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 0 CreateTime: 1719853215613 size: 78 magic: 2 compresscodec: none crc: 3210426395 isvalid: true
| offset: 0 CreateTime: 1719853215613 keySize: -1 valueSize: 10 sequence: 0 headerKeys: [] payload: my message
baseOffset: 1 lastOffset: 1 count: 1 baseSequence: 1 lastSequence: 1 producerId: 0 producerEpoch: 0 partitionLeaderEpoch: 0 isTransactional: false isControl: false deleteHorizonMs: OptionalLong.empty position: 78 CreateTime: 1719853219329 size: 82 magic: 2 compresscodec: none crc: 2827246318 isvalid: true
| offset: 1 CreateTime: 1719853219329 keySize: -1 valueSize: 14 sequence: 1 headerKeys: [] payload: second message


## index 파일 확인
&gt; bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/test-0/00000000000000000000.index
Dumping /tmp/kafka-logs/test-0/00000000000000000000.index
offset: 0 position: 0

## timestamp index 파일 확인 
&gt; bin/kafka-run-class.sh kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files /tmp/kafka-logs/test-0/00000000000000000000.timeindex
Dumping /tmp/kafka-logs/test-0/00000000000000000000.timeindex
timestamp: 1719853219329 offset: 1


복사<br>
<br>Topic의 partition 별로 마지막 commit 된 위치 확인 

<br>40번 partition에 commit 2로 저장됨


<br>&gt; cat /tmp/kafka-logs/replication-offset-checkpoint
0
51
test 0 2
__consumer_offsets 29 0
__consumer_offsets 43 0
__consumer_offsets 0 0
__consumer_offsets 40 2

복사<br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-topics.sh --describe  --bootstrap-server localhost:9092 --topic test
Topic: test	TopicId: w1XGVh69Q32nB6-TK4yV9A	PartitionCount: 1	ReplicationFactor: 1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
복사<br><br>
<br><a rel="noopener" class="external-link" href="https://kafka.apache.org/documentation/#basic_ops_consumer_lag" target="_blank">https://kafka.apache.org/documentation/#basic_ops_consumer_lag</a>
<br># find consumer group list
&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list
console-consumer-29099

# view offset of group
&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group console-consumer-29099

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                     HOST            CLIENT-ID
test            0          2               2               0               consumer-1-ff412a90-fec0-45e4-89d9-06179c7bd8e3 /10.146.0.6     consumer-1
복사<br><br>
<br>실습에서는 삭제하지 말고, 명령어 참고만 할 것
<br>&gt; bin/kafka-topics.sh --delete --bootstrap-server localhost:9092  --topic test

# 삭제 후 topic이 삭제되었는지 확인
&gt; bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

복사<br><br>
<br>접속할 zookeeper 정보를 입력하여, zookeeper 내부로 접속
<br>&gt; cd ~/apps/kafka_2.12-3.6.2

&gt; bin/zookeeper-shell.sh localhost:2181
Connecting to localhost:2181
Connecting to localhost:2181
Welcome to ZooKeeper!
JLine support is disabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

## Zookeeper에서 제공하는 정보 조회
ls /
[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]


## Broker 관련 정보 조회 
ls /brokers
[ids, seqid, topics]


## 현재 broker 목록을 조회 (broker 0번만 존재)
ls /brokers/ids
[0]

## 0번 broker에 대한 정보 확인 
get /brokers/ids/0
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://instance-20240701-162846.asia-northeast3-b.c.beaming-grid-428115-d6.internal:9092"],"jmx_port":-1,"features":{},"host":"instance-20240701-162846.asia-northeast3-b.c.beaming-grid-428115-d6.internal","timestamp":"1719917952131","port":9092,"version":5}


## topic 정보 조회 
ls /brokers/topics
[__consumer_offsets]


## topic 상세 정보 조회
get /brokers/topics/test
Node does not exist: /brokers/topics/test

# test topic을 삭제하지 않은 경우에는 아래와 같은 topic 정보가 출력됨
{"removing_replicas":{},"partitions":{"0":[0]},"topic_id":"ZOLr9CLNQjSW70hyIHx6BQ","adding_replicas":{},"version":3}


## controller broker 조회
get /controller
{"version":2,"brokerid":0,"timestamp":"1719917952299","kraftControllerEpoch":-1}
복사<br><br><br><br>&gt; cd ~/apps/kafka_2.12-3.6.2
&gt; bin/kafka-topics.sh --create  --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic mytopic

&gt; bin/kafka-topics.sh --list --bootstrap-server localhost:9092
복사<br><br>
<br>아래 링크를 이용하여 Apache flume을 설치 및 실행 테스트
<br><a rel="noopener" class="external-link" href="https://github.com/freepsw/RTS_Practice/blob/master/01.Apache%20Flume.md#1-collectionapache-flume" target="_blank">https://github.com/freepsw/RTS_Practice/blob/master/01.Apache%20Flume.md#1-collectionapache-flume</a>
<br><br>&gt; cd ~/apps/apache-flume-1.8.0-bin/conf
&gt; vi nc-kafka.conf

a1.sources = r1
a1.channels = c1
a1.sinks = s1

a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 56565
a1.sources.r1.channels = c1

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

a1.sinks.s1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.s1.topic = mytopic
a1.sinks.s1.brokerList = localhost:9092
a1.sinks.s1.requiredAcks = 1
a1.sinks.s1.batchSize = 20
a1.sinks.s1.channel = c1
복사<br><br>&gt; cd ~/apps/apache-flume-1.8.0-bin/
&gt; ./bin/flume-ng agent -c ./conf -f ./conf/nc-kafka.conf -name a1 -Dflume.root.logger=INFO,console
복사<br><br><br>&gt; cd ~/apps/logstash-6.4.2
&gt; vi kafka-input.conf

input {
  kafka{
  	topics =&gt; ["토픽명"]
  	bootstrap_servers =&gt; "kafka브로커ip:9092"
  }
}

output {
  stdout { codec =&gt; rubydebug }
}
복사<br>
<br>logstash 실행
<br><br>&gt; curl telnet://localhost:56565
hi message
OK
복사<br><br>
<br>아래와 같이 necat으로 보낸 메세지를 받는지 확인한다.
<br>{
       "message" =&gt; "hi message",
      "@version" =&gt; "1",
    "@timestamp" =&gt; 2018-10-12T14:18:36.953Z
}
복사<br><br>
<br>Kafka의 주요 topic, consumer group, offset 값을 모니터링 한다.
<br><a rel="noopener" class="external-link" href="https://github.com/Morningstar/kafka-offset-monitor" target="_blank">https://github.com/Morningstar/kafka-offset-monitor</a> 참고
<br>&gt; cd ~/apps/apache-flume-1.8.0-bin/
&gt; wget https://github.com/Morningstar/kafka-offset-monitor/releases/download/0.4.6/KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar
&gt; java -cp KafkaOffsetMonitor-assembly-0.4.6-SNAPSHOT.jar \
      com.quantifind.kafka.offsetapp.OffsetGetterWeb \
    --offsetStorage kafka \
    --kafkaBrokers localhost:9092 \
    --zk localhost:2181 \
    --port 8081 \
    --refresh 10.seconds \
    --retain 2.days
복사<br><br>
<br>별도의 수집용 code를 만들지 않고, kafka connect를 이용하여 데이터 import 및 export 할 수 있다.
<br>Secenario : file을 import하고, file로 export 한다.
<br>#  creating some seed data to test with
&gt; echo -e "foo\nbar" &gt; test.txt

# start two connectors running in standalone mode
# 3개의 config 파일을 파라미터로 넘긴다.
# 1. Kafka connect process용 config (broker info, data format ...)
# 2. source에 대한 config (test.txt 지정)
# 3. sink에 대한 config (test.sink.txt 지정)
&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties

# check test.sink.txt file
&gt; cat test.sink.txt
foo
bar

# send another message
&gt; echo "Another line" &gt;&gt; test.txt

# check test.sink.txt file again


# check kafka topic message
&gt; bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic connect-test --from-beginning

복사<br><br># create message and publish to topic "streams-file-input"
&gt; cd $KAFKA_HOME
&gt; echo -e "all streams lead to kafka\nhello kafka streams\njoin kafka summit" &gt; file-input.txt
&gt; cat file-input.txt | ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-file-input

# run kafka stream
&gt; ./bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo

# check result of kafka stream
&gt; ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 \
            --topic streams-wordcount-output \
            --from-beginning \
            --formatter kafka.tools.DefaultMessageFormatter \
            --property print.key=true \
            --property print.key=true \
            --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
            --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer

all     1
streams 1
lead    1
to      1
kafka   1
hello   1
kafka   2
streams 2
join    1
kafka   3
summit  1

복사]]></description><link>root\dev\data-pipe-line\kafka\group\readme.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/group/README.md</guid><pubDate>Tue, 02 Jul 2024 00:14:40 GMT</pubDate></item><item><title><![CDATA[kafka 서버 설정 자동화]]></title><description><![CDATA[ 
 <br><br>mkdir ~/run/kafka
vi ~/run/kafka/update_cluster_configs.sh
복사<br><br><br>#!/bin/bash

# 서버 목록과 각 서버에 해당하는 ID와 IP 주소 설정
declare -A servers
servers=(
	["server1"]="1 172.17.0.2"
	["server2"]="2 172.17.0.3"
	["server3"]="3 172.17.0.4"
)

# 파일 경로 설정
MYID_PATH="/home/ubuntu/zkdata/myid"
SERVER_PROPERTIES_PATH="~/app/kafka/kafka_2.13-3.6.2/config/server.properties"

for server in "${!servers[@]}"; do
	id_ip=(${servers[$server]})
	id=${id_ip[0]}
	ip=${id_ip[1]}

	echo "Updating $server with ID $id and IP $ip"

	# myid 파일 업데이트
	ssh "$server" "echo $id &gt; $MYID_PATH"

	# server.properties 파일 업데이트
	ssh "$server" "sed -i 's/^broker.id=.*/broker.id=$id/' $SERVER_PROPERTIES_PATH"
	ssh "$server" "sed -i 's|^listeners=PLAINTEXT://.*|listeners=PLAINTEXT://$ip:9092|' $SERVER_PROPERTIES_PATH"
	ssh "$server" "sed -i 's|^advertised.listeners=PLAINTEXT://.*|advertised.listeners=PLAINTEXT://$ip:9092|' $SERVER_PROPERTIES_PATH"

	echo "Updated $server"
done

echo "All servers updated successfully."
복사<br><br>chmod +x update_kafka_configs.sh
복사<br><br>./update_kafka_configs.sh
복사]]></description><link>root\dev\data-pipe-line\kafka\shellscript\kafka-서버-설정-자동화.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/shellscript/kafka 서버 설정 자동화.md</guid><pubDate>Sat, 03 Aug 2024 15:50:14 GMT</pubDate></item><item><title><![CDATA[kafka 클러스터 스크립트]]></title><description><![CDATA[ 
 <br><br>mkdir ~/run/kafka
vi ~/run/kafka/start_cluster.sh
복사<br><br><br><br>#!/bin/bash

ips=("server1" "server2" "server3")

start_service() {
	local service_name=$1
	local script_path=$2
	local sleep_time=${3:-1} # 기본값을 1로 설정

	for ip in "${ips[@]}"; do
		echo ""
		echo "============== $service_name 시작 시도 중: $ip =============="
		echo ""
		ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 $ip 'bash -s' &lt; "$script_path"
		sleep $sleep_time
		if [ $? -eq 0 ]; then
			echo "$ip 에서 $service_name 가 성공적으로 시작되었어"
		else
			echo "$ip 에서 $service_name 시작 실패"
			exit 1
		fi
	done
}

# Zookeeper 시작 및 상태 확인
start_service "Zookeeper" "./run/kafka/start_zookeeper.sh"

# 대기
sleep 3

# Kafka 서버 시작
start_service "Kafka 서버" "./run/kafka/start_server.sh" 2

# Kafka check
start_service "Kafka check" "./run/kafka/check_conn.sh" 2
복사<br><br>#!/bin/bash

ips=("server1" "server2" "server3")

# Zookeeper 시작 및 상태 확인
for ip in "${ips[@]}"; do
	echo "Zookeeper 시작 시도 중: $ip"
	ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 $ip 'bash -s' &lt; ./run/kafka_zookeeper_start.sh
	sleep 1
	if [ $? -eq 0 ]; then
			echo "$ip 에서 Zookeeper가 성공적으로 시작되었어"
	else
			echo "$ip 에서 Zookeeper 시작 실패"
			exit 1
	fi
done

# 대기
sleep 3

# Kafka 서버 시작
for ip in "${ips[@]}"; do
	echo "Kafka 서버 시작 시도 중: $ip"
	ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 $ip 'bash -s' &lt; ./run/kafka_server_start.sh
	sleep 1
	if [ $? -eq 0 ]; then
			echo "$ip 에서 Kafka 서버가 성공적으로 시작되었어"
	else
			echo "$ip 에서 Kafka 서버 시작 실패"
			exit 1
	fi
done
복사<br><br><br>mkdir ./run/kafka
vi ./run/kafka/stop_cluster.sh
복사<br>#!/bin/bash

ips=("server1" "server2" "server3")

# Kafka 서버 종료
for ip in "${ips[@]}"; do
	echo "Kafka 서버 종료 시도 중: $ip"
	ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 $ip 'bash -s' &lt;&lt;-'EOF'
			cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/
			./bin/kafka-server-stop.sh
EOF
	if [ $? -eq 0 ]; then
			echo "$ip 에서 Kafka 서버가 성공적으로 종료되었어"
	else
			echo "$ip 에서 Kafka 서버 종료 실패"
			exit 1
	fi
done

# Zookeeper 서버 종료
for ip in "${ips[@]}"; do
	echo "Zookeeper 서버 종료 시도 중: $ip"
	ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 $ip 'bash -s' &lt;&lt;-'EOF'
			cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/
			./bin/zookeeper-server-stop.sh
EOF
	if [ $? -eq 0 ]; then
			echo "$ip 에서 Zookeeper 서버가 성공적으로 종료되었어"
	else
			echo "$ip 에서 Zookeeper 서버 종료 실패"
			exit 1
	fi
done
복사<br><br><br>
<br>종료 스크립트 사용 방법:
<br>
<br>스크립트에 실행 권한을 부여해:
<br>chmod +x stop_kafka_cluster.sh
복사<br>
<br>스크립트를 실행해:
<br>./stop_kafka_cluster.sh
복사<br>
<br>시작 스크립트 사용 방법:
<br>
<br>이미 실행 권한을 부여한 상태라면:
<br>./start_kafka_cluster.sh
복사<br><br>
<br>stop_kafka_cluster.sh: 각 서버에 SSH로 접속하여 Kafka 서버를 먼저 종료한 후, Zookeeper 서버를 종료해. 순차적으로 종료되도록 설계했어.
<br>start_kafka_cluster.sh: 각 서버에 SSH로 접속하여 Zookeeper 서버를 먼저 시작하고, 상태를 확인한 후 Kafka 서버를 시작해. Zookeeper가 제대로 시작되었는지 확인하는 과정이 포함되어 있어.
<br>이 스크립트들을 사용하면 안전하게 Kafka 클러스터를 종료하고 시작할 수 있어. 문제가 발생할 경우 스크립트가 중단되기 때문에 안정성을 높일 수 있어.]]></description><link>root\dev\data-pipe-line\kafka\shellscript\kafka-클러스터-스크립트.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/shellscript/kafka 클러스터 스크립트.md</guid><pubDate>Sat, 03 Aug 2024 15:51:02 GMT</pubDate></item><item><title><![CDATA[shellscirpt kafka zookeeper server]]></title><description><![CDATA[ 
 <br><br>mkdir ~/run/kafka
vi ~/run/kafka/start_zookeeper.sh
복사<br><br><br>#!/bin/bash

cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/

nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt;/dev/null 2&gt;&amp;1
복사]]></description><link>root\dev\data-pipe-line\kafka\shellscript\shellscirpt-kafka-zookeeper-server.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/shellscript/shellscirpt kafka zookeeper server.md</guid><pubDate>Mon, 05 Aug 2024 15:55:14 GMT</pubDate></item><item><title><![CDATA[shellscript kafka server conn check]]></title><description><![CDATA[ 
 <br><br>mkdir ~/run/kafka
vi ~/run/kafka/check_conn.sh
복사<br><br><br>#!/bin/bash
netstat -ntlp | grep 2181

echo ""
echo "======================="
echo ""

netstat -ntlp | grep 9092
복사]]></description><link>root\dev\data-pipe-line\kafka\shellscript\shellscript-kafka-server-conn-check.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/shellscript/shellscript kafka server conn check.md</guid><pubDate>Sat, 03 Aug 2024 15:50:49 GMT</pubDate></item><item><title><![CDATA[shellscript kafka server start]]></title><description><![CDATA[ 
 <br><br>mkdir ~/run/kafka
vi ~/run/kafka/start_server.sh
복사<br><br><br>#!/bin/bash

cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/

nohup ./bin/kafka-server-start.sh config/server.properties &gt;/dev/null 2&gt;&amp;1
복사]]></description><link>root\dev\data-pipe-line\kafka\shellscript\shellscript-kafka-server-start.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/shellscript/shellscript kafka server start.md</guid><pubDate>Sat, 03 Aug 2024 15:50:43 GMT</pubDate></item><item><title><![CDATA[카프카에 대한 고찰]]></title><description><![CDATA[ 
 <br>Kafka를 사용했을 때와 그렇지 않았을 때의 차이를 극명하게 느끼려면, Kafka의 핵심 기능인 실시간 데이터 스트리밍과 확장성의 이점을 실제로 체험해보는 것이 중요합니다. 이를 위해 다음과 같은 시나리오를 비교해 볼 수 있습니다:<br><br><br>이 경우, 데이터는 MySQL 데이터베이스에 직접 삽입되고, 데이터베이스에서 실시간 데이터 분석이나 다른 시스템으로의 데이터 전송은 별도의 작업으로 처리됩니다.<br><br>
<br>시스템이 단순해지고, 데이터베이스 관리와 유지보수가 쉬워집니다.
<br><br>
<br>실시간 데이터 처리가 어렵고, 데이터 처리 속도가 데이터베이스 성능에 제한됩니다.
<br>확장성이 낮아, 높은 데이터 트래픽이나 다양한 데이터 소비 요구에 대응하기 어렵습니다.
<br>여러 시스템 간 데이터 통합이나 데이터 동기화가 복잡해질 수 있습니다.
<br><br>Kafka를 사용하면 데이터는 Kafka 토픽에 게시되고, MySQL 데이터베이스는 Kafka의 컨슈머 중 하나로서 데이터를 소비하게 됩니다. 이 경우 Kafka는 데이터 브로커로서 다양한 시스템에 데이터를 스트리밍합니다.<br><br>
<br>실시간 데이터 스트리밍: Kafka는 데이터를 실시간으로 여러 컨슈머에게 스트리밍할 수 있어, 데이터베이스 저장 외에도 실시간 분석, 모니터링, 알림 시스템 등 다양한 사용 사례에 데이터를 즉시 사용할 수 있습니다.
<br>확장성: Kafka 클러스터를 확장하여 더 많은 데이터를 처리할 수 있으며, 새로운 컨슈머를 쉽게 추가하여 다양한 애플리케이션이 데이터를 소비할 수 있습니다.
<br>내구성: Kafka는 데이터를 디스크에 저장하고 복제하므로, 시스템 장애 시에도 데이터 손실이 최소화됩니다.
<br>분리된 시스템: 데이터 생산자와 소비자 시스템 간의 결합이 느슨해져, 각각 독립적으로 확장하거나 업데이트할 수 있습니다.
<br><br>
<br>시스템 복잡도가 증가하고, Kafka 클러스터 운영과 관리에 추가적인 노력이 필요합니다.
<br>초기 설정 및 학습 곡선이 있습니다.
<br><br>실제로 Kafka를 사용했을 때와 사용하지 않았을 때의 차이를 체험하려면 다음과 같은 실험을 해볼 수 있습니다:<br>
<br>
고속 데이터 인제스트 시나리오:

<br>대량의 데이터를 짧은 시간에 인제스트할 때, Kafka가 이를 효과적으로 버퍼링하고 처리하는 반면, MySQL은 성능 병목 현상이 발생할 수 있습니다.


<br>
실시간 분석 시나리오:

<br>Kafka를 사용하여 데이터가 발생하는 즉시 스트리밍 데이터를 분석하는 컨슈머를 연결하고, MySQL에서는 데이터베이스 쿼리로 실시간 데이터를 분석해보십시오. Kafka의 스트리밍 처리가 얼마나 신속하고 효율적인지 알 수 있습니다.


<br>
다양한 소비자 연결 시나리오:

<br>같은 데이터를 여러 시스템이 소비해야 하는 경우, Kafka의 pub-sub 모델을 사용하여 이를 간단하게 구현할 수 있습니다. MySQL을 사용하면 데이터베이스에서 직접 데이터 복사를 설정하거나 별도의 통합 솔루션이 필요할 수 있습니다.


<br>이러한 실험을 통해 Kafka의 장점을 직접 경험할 수 있으며, 이를 통해 Kafka 사용의 실질적인 이점을 명확하게 느낄 수 있습니다.<br><br><br>CSV 데이터를 사용하여 Kafka를 통해 데이터를 스트리밍하고, MySQL에 저장하는 예제를 제공하겠습니다. 이 예제에서는 CSV 파일에서 데이터를 읽어 Kafka에 게시하고, Kafka 컨슈머가 이 데이터를 소비하여 MySQL에 저장하는 방식으로 진행됩니다.<br><br>
<br>CSV 파일: 예시로 사용될 데이터를 포함한 파일. 예를 들어, users.csv라는 파일을 사용할 수 있습니다.
<br>MySQL 데이터베이스 서버
<br>Kafka 브로커 및 Zookeeper
<br>Kafka Python 클라이언트 (kafka-python 패키지)
<br>MySQL Python 클라이언트 (pymysql 패키지)
<br>Pandas (CSV 데이터를 읽기 위해 사용)
<br><br>name,age
Alice,30
Bob,24
Charlie,29
복사<br><br>먼저, MySQL에 users 테이블을 생성합니다.<br>import pymysql

# MySQL 연결 설정
connection = pymysql.connect(
    host='localhost',
    user='root',
    password='password',
    database='my_data'
)

# 테이블 생성 쿼리
create_table_query = """
CREATE TABLE IF NOT EXISTS users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100),
    age INT
)
"""

try:
    with connection.cursor() as cursor:
        cursor.execute(create_table_query)
    connection.commit()
finally:
    connection.close()
Run복사<br><br>CSV 파일의 데이터를 읽어 MySQL에 직접 삽입하는 코드입니다.<br>import pandas as pd
import pymysql

def insert_csv_into_mysql(file_path):
    # CSV 데이터 읽기
    data = pd.read_csv(file_path)

    # MySQL 연결 설정
    connection = pymysql.connect(
        host='localhost',
        user='root',
        password='password',
        database='my_data'
    )

    insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
    try:
        with connection.cursor() as cursor:
            # CSV 데이터를 MySQL에 삽입
            for _, row in data.iterrows():
                cursor.execute(insert_query, (row['name'], row['age']))
        connection.commit()
    finally:
        connection.close()

# CSV 파일 경로
csv_file_path = 'users.csv'

insert_csv_into_mysql(csv_file_path)
print("MySQL에 CSV 데이터 삽입 완료.")
Run복사<br><br><br>CSV 파일의 데이터를 Kafka 토픽에 게시하는 코드입니다.<br>from kafka import KafkaProducer
import pandas as pd
import json

def produce_csv_to_kafka(file_path, topic):
    # CSV 데이터 읽기
    data = pd.read_csv(file_path)

    producer = KafkaProducer(
        bootstrap_servers='localhost:9092',
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    # 각 행을 Kafka 메시지로 게시
    for _, row in data.iterrows():
        message = row.to_dict()
        producer.send(topic, value=message)

    producer.flush()
    print("Kafka에 CSV 데이터 게시 완료.")

produce_csv_to_kafka(csv_file_path, 'my_topic')
Run복사<br><br>Kafka에서 데이터를 소비하고 MySQL에 저장하는 코드입니다.<br>from kafka import KafkaConsumer
import pymysql
import json

def consume_from_kafka_and_insert_into_mysql(topic):
    consumer = KafkaConsumer(
        topic,
        bootstrap_servers='localhost:9092',
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    def insert_into_mysql(data):
        connection = pymysql.connect(
            host='localhost',
            user='root',
            password='password',
            database='my_data'
        )

        insert_query = "INSERT INTO users (name, age) VALUES (%s, %s)"
        try:
            with connection.cursor() as cursor:
                cursor.executemany(insert_query, [(d['name'], d['age']) for d in data])
            connection.commit()
        finally:
            connection.close()

    buffer = []
    for message in consumer:
        data = message.value
        buffer.append(data)
        if len(buffer) &gt;= 10:  # 예시로 10개씩 버퍼링하여 MySQL에 삽입
            insert_into_mysql(buffer)
            buffer = []

    # 남은 데이터 삽입
    if buffer:
        insert_into_mysql(buffer)

    print("Kafka에서 데이터 소비 및 MySQL에 저장 완료.")

consume_from_kafka_and_insert_into_mysql('my_topic')
Run복사<br><br>
<br>시나리오 1:

<br>insert_csv_into_mysql 함수를 호출하여 CSV 데이터를 MySQL에 직접 삽입합니다.


<br>시나리오 2:

<br>produce_csv_to_kafka 함수를 호출하여 CSV 데이터를 Kafka에 게시한 후, consume_from_kafka_and_insert_into_mysql 함수를 호출하여 Kafka에서 데이터를 소비하고 MySQL에 저장합니다.


<br>이 실습을 통해 CSV 데이터를 사용하여 Kafka의 사용 여부에 따른 데이터 처리 방식을 비교하고, Kafka의 장점(실시간 처리, 확장성 등)을 직접 체험할 수 있습니다.]]></description><link>root\dev\data-pipe-line\kafka\카프카에-대한-고찰.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/카프카에 대한 고찰.md</guid><pubDate>Sat, 27 Jul 2024 05:57:01 GMT</pubDate></item><item><title><![CDATA[카프카에 csv 파일 넣어보기]]></title><description><![CDATA[ 
 <br>pip install kafka-python
복사<br>from kafka import KafkaProducer
import pandas as pd
import json

def produce_csv_to_kafka(file_path, topic):
    # CSV 데이터 읽기
    data = pd.read_csv(file_path)

    # Kafka 프로듀서 설정 (클러스터 환경)
    producer = KafkaProducer(
        bootstrap_servers=['172.17.0.2:9092', '172.17.0.3:9092', '172.17.0.4:9092'],
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    # 각 행을 Kafka 메시지로 게시
    for _, row in data.iterrows():
        message = row.to_dict()
        producer.send(topic, value=message)

    producer.flush()
    print("Kafka에 CSV 데이터 게시 완료.")

# CSV 파일 경로와 Kafka 토픽 설정
csv_file_path = 'data.csv'
produce_csv_to_kafka(csv_file_path, 'my_topic')
Run복사]]></description><link>root\dev\data-pipe-line\kafka\카프카에-csv-파일-넣어보기.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/카프카에 csv 파일 넣어보기.md</guid><pubDate>Mon, 29 Jul 2024 15:23:20 GMT</pubDate></item><item><title><![CDATA[Apache Kafka 실시간 Data Pipeline 구축하기]]></title><description><![CDATA[ 
 <br>“Pasted image 20240708175351.png” could not be found.<br><a rel="noopener" class="external-link" href="https://github.com/freepsw/kafka_basic/tree/main/02.kafka_pipeline" target="_blank">https://github.com/freepsw/kafka_basic/tree/main/02.kafka_pipeline</a><br><a rel="noopener" class="external-link" href="https://blog.naver.com/freepsw/222227393582" target="_blank">https://blog.naver.com/freepsw/222227393582</a><br><a data-tooltip-position="top" aria-label="https://docs.confluent.io/platform/current/kafka/monitoring.html" rel="noopener" class="external-link" href="https://docs.confluent.io/platform/current/kafka/monitoring.html" target="_blank">Monitoring Kafka with JMX | Confluent Documentation</a>]]></description><link>root\dev\data-pipe-line\kafka\apache-kafka-실시간-data-pipeline-구축하기.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/Apache Kafka 실시간 Data Pipeline 구축하기.md</guid><pubDate>Mon, 08 Jul 2024 08:53:52 GMT</pubDate></item><item><title><![CDATA[kafak에 mysql 연결]]></title><description><![CDATA[ 
 <br>카프카를 데이터 파이프라인의 중심에 두고, 데이터 생산자는 카프카에 데이터를 전송하고, 카프카 커넥트를 사용해 데이터를 다양한 싱크(데이터베이스 등)로 전달하는 방식은 매우 일반적이고 안정적인 데이터 처리 방법이야.<br>카프카와 데이터베이스를 연결하는 것은 여러 가지 장점이 있어:<br>
<br>확장성: 카프카는 높은 처리량을 처리할 수 있어서 대규모 데이터 전송에도 적합해.
<br>신뢰성: 카프카는 데이터의 내구성을 보장해, 데이터 손실 없이 안정적으로 전송할 수 있어.
<br>유연성: 다양한 소스와 싱크를 연결할 수 있어 데이터 파이프라인을 유연하게 구성할 수 있어.
<br>비동기 처리: 데이터 생산자와 소비자가 비동기적으로 동작할 수 있어서 시스템의 복잡도를 줄일 수 있어.
<br>다시 한번 정리하면, 카프카와 데이터베이스를 연결하는 시나리오는 다음과 같아:<br>
<br>데이터 생산자 (Python 코드): CSV 파일 데이터를 읽고, 카프카로 전송.
<br>카프카: 데이터를 토픽으로 받음.
<br>카프카 커넥트: 카프카 토픽 데이터를 MySQL 데이터베이스로 전송.
<br>이 시나리오를 실제로 구현하는 방법을 구체적으로 설명할게.<br><br>Kafka Connect는 카프카의 생태계에 포함되어 있는 도구야. 여기서는 io.confluent.connect.jdbc.JdbcSinkConnector를 사용해 MySQL과 연결할 거야.<br><br>카프카를 이미 설치했다면 Kafka Connect도 함께 설치되어 있어.<br><br>MySQL JDBC 드라이버를 다운로드하고 Kafka Connect의 plugins 디렉토리에 복사해줘.<br><br>MySQL 싱크 커넥터를 설정하는 JSON 파일을 작성해.<br><br>{
  "name": "mysql-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
    "tasks.max": "1",
    "topics": "my_topic",
    "connection.url": "jdbc:mysql://localhost:3306/my_data",
    "connection.user": "ubuntu",
    "connection.password": "1234",
    "auto.create": "true",
    "insert.mode": "insert",
    "pk.mode": "none",
    "table.name.format": "test_a"
  }
}
복사<br><br>Kafka Connect를 실행하고, MySQL 싱크 커넥터를 등록해.<br><br>Kafka Connect를 배포 모드로 실행해.<br>bin/connect-distributed.sh config/connect-distributed.properties
Run복사<br><br>MySQL 싱크 커넥터를 Kafka Connect에 등록해.<br>curl -X POST -H "Content-Type: application/json" --data @mysql-sink-connector.json http://localhost:8083/connectors
Run복사<br><br>Python 코드를 작성해서 CSV 파일 데이터를 카프카 토픽으로 전송해.<br>from confluent_kafka import Producer
import pandas as pd
import json

# 카프카 프로듀서 설정
KAFKA_SERVERS = '172.17.0.2:9092,172.17.0.3:9092,172.17.0.4:9092'
KAFKA_TOPIC = 'my_topic'

# CSV 파일에서 데이터 추출
data = pd.read_csv('data.csv')

# 카프카 프로듀서 설정
producer_conf = {
    'bootstrap.servers': KAFKA_SERVERS,
}
producer = Producer(producer_conf)

# 데이터를 카프카 토픽으로 전송
def delivery_report(err, msg):
    if err is not None:
        print(f"Message delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

for index, row in data.iterrows():
    message = json.dumps({
        'column1': row['column1'],
        'column2': row['column2'],
        'column3': row['column3']  # assuming column3 is in a date format already
    })
    producer.produce(KAFKA_TOPIC, key=str(index), value=message, callback=delivery_report)

producer.flush()

print("데이터가 카프카로 성공적으로 전송되었습니다.")
Run복사<br>이렇게 하면, 데이터는 다음과 같은 순서로 흐르게 돼:<br>
<br>Python 코드: CSV 파일 데이터를 카프카 토픽으로 전송.
<br>카프카: 데이터를 토픽으로 받음.
<br>Kafka Connect: 카프카 토픽 데이터를 MySQL 데이터베이스에 적재.
<br>이 시나리오는 데이터 파이프라인의 확장성과 신뢰성을 모두 제공해, 데이터가 손실 없이 안정적으로 처리될 수 있어. 이 방식은 많은 대규모 데이터 처리 시스템에서 사용되는 검증된 방법이야.]]></description><link>root\dev\data-pipe-line\kafka\kafak에-mysql-연결.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafak에 mysql 연결.md</guid><pubDate>Sun, 28 Jul 2024 18:12:21 GMT</pubDate></item><item><title><![CDATA[kafka 설정 및 실행]]></title><description><![CDATA[ 
 <br><a data-href="kafka 설치" href="root\dev\data-pipe-line\kafka\kafka-설치.html" class="internal-link" target="_self" rel="noopener">kafka 설치</a><br>Attentionserver 1,2,3 모두 적용<br>그리고<br>정말 중요!docker 컨테이너 안에서 실행 할 때는 굳이<br>
-p 옵션을 줘서 포트포워딩 할 필요가 없다.<br>
그러니 이 부분에 있어서는 신경쓰지 말고 구현하자<br><br><br>서버에 맞게 설정!
<br>server1

cd
mkdir zkdata
cd zkdata
echo 1 &gt; ./myid
복사

<br>server2

cd
mkdir zkdata
cd zkdata
echo 2 &gt; ./myid
복사

<br>server3

cd
mkdir zkdata
cd zkdata
echo 3 &gt; ./myid
복사<br><br>cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/config
복사<br><br>Importantdocker image를 만들기 위해서 <a data-href="kafka server.properties" href="root\dev\data-pipe-line\kafka\kafka-server.properties.html" class="internal-link" target="_self" rel="noopener">kafka server.properties</a>를 적용 시키고<br>
서버에 맞게 설정 하는 것은 적용된 값에서 컨테이너마다 설정 해주자<br>서버에 맞게 설정!broker.id
listeners
advertised.listeners
복사
위의 프로퍼티는 각 서버에 맞게 수정해서 적용해야 함

<br>server1

broker.id = 1
listeners=PLAINTEXT://172.17.0.2:9092
advertised.listeners=PLAINTEXT://172.17.0.2:9092
복사

<br>server2

broker.id = 2
listeners=PLAINTEXT://172.17.0.3:9092
advertised.listeners=PLAINTEXT://172.17.0.3:9092
복사

<br>server3

broker.id = 3
listeners=PLAINTEXT://172.17.0.4:9092
advertised.listeners=PLAINTEXT://172.17.0.4:9092
복사<br><br># Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# the directory where the snapshot is stored.
dataDir=/home/ubuntu/zkdata
# the port at which the clients will connect
clientPort=2181
tickTime=2000
initLimit=10
syncLimit=5
server.1=172.17.0.2:2888:3888
server.2=172.17.0.3:2888:3888
server.3=172.17.0.4:2888:3888
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080
복사<br><br><br><br><a data-href="shellscirpt kafka zookeeper server" href="root\dev\data-pipe-line\kafka\shellscript\shellscirpt-kafka-zookeeper-server.html" class="internal-link" target="_self" rel="noopener">shellscirpt kafka zookeeper server</a><br><br><a data-href="shellscript kafka server start" href="root\dev\data-pipe-line\kafka\shellscript\shellscript-kafka-server-start.html" class="internal-link" target="_self" rel="noopener">shellscript kafka server start</a><br><br><a data-href="shellscript kafka server conn check" href="root\dev\data-pipe-line\kafka\shellscript\shellscript-kafka-server-conn-check.html" class="internal-link" target="_self" rel="noopener">shellscript kafka server conn check</a><br><br>sudo chmod +x start_zookeeper.sh start_server.sh check_conn.sh
복사<br><br><br><br>cd /home/ubuntu/app/kafka/kafka_2.13-3.6.2/
복사<br>Info기본값은 백그라운드지만, 포그라운드를 만들어 놓은 이슈는<br>
프로그램 동작 확인과 종료를 쉽게 하기 위함이다.<br><br>nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt;/dev/null 2&gt;&amp;1 &amp;
복사<br>nohup ./bin/kafka-server-start.sh config/server.properties &gt;/dev/null 2&gt;&amp;1 &amp;
복사<br><br>nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt;/dev/null 2&gt;&amp;1
복사<br>nohup ./bin/kafka-server-start.sh config/server.properties &gt;/dev/null 2&gt;&amp;1
복사<br>netstat -ntlp | grep 2181
복사<br>netstat -ntlp | grep 9092
복사<br><br><a data-href="kafka error handling" href="root\dev\data-pipe-line\kafka\kafka-error-handling.html" class="internal-link" target="_self" rel="noopener">kafka error handling</a><br><br><br>nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt;/dev/null 2&gt;&amp;1 &amp;
복사<br>
<br>
nohup: 

<br>목적: 터미널을 닫아도 명령어가 계속 실행되게 해주는 명령어야.
<br>상세: 'no hang up'의 줄임말로, 보통 터미널을 닫으면 그 터미널에서 실행 중인 프로세스도 종료되는데, nohup을 사용하면 백그라운드에서 계속 실행돼.


<br>
./bin/zookeeper-server-start.sh config/zookeeper.properties:

<br>목적: 주키퍼 서버를 시작하는 스크립트야.
<br>상세: zookeeper-server-start.sh는 주키퍼 서버를 시작하는 스크립트 파일이고, config/zookeeper.properties는 주키퍼의 설정 파일이야. 이 파일에 주키퍼 서버 설정들이 담겨 있어.


<br>
&gt;/dev/null 2&gt;&amp;1:

<br>목적: 출력과 에러 메시지를 무시하고 버려.
<br>상세: 

<br>&gt;/dev/null: 표준 출력을 /dev/null로 보내서 출력 결과를 버려.
<br>2&gt;&amp;1: 표준 에러를 표준 출력으로 리다이렉트해. 따라서 표준 출력과 표준 에러 모두 /dev/null로 가서 결과적으로 아무 출력도 보이지 않게 돼.




<br>
&amp;:

<br>목적: 명령어를 백그라운드에서 실행해.
<br>상세: 이걸 붙이면 명령어가 백그라운드에서 실행돼서 터미널을 계속 사용할 수 있게 해줘.


<br>종합해보면, 이 명령어는 주키퍼 서버를 설정 파일을 기반으로 백그라운드에서 실행하면서 출력 결과와 에러 메시지를 모두 무시하는 거야.<br><br><br><br>cd ~/app/kafka/kafka_2.13-3.6.2

./bin/kafka-topics.sh --create --bootstrap-server 172.17.0.2:9092 --replication-factor 3 --partitions 3 --topic testtopic
복사<br>cd ~/app/kafka/kafka_2.13-3.6.2

./bin/kafka-console-producer.sh --bootstrap-server 172.17.0.2:9092 --topic testtopic
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2

./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.3:9092 --topic testtopic --from-beginning
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2

./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.4:9092 --topic testtopic --from-beginning
복사<br><br><br>pkill -f kafka
복사]]></description><link>root\dev\data-pipe-line\kafka\kafka-설정-및-실행.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka 설정 및 실행.md</guid><pubDate>Mon, 05 Aug 2024 15:55:28 GMT</pubDate></item><item><title><![CDATA[kafka 설치]]></title><description><![CDATA[ 
 <br><br><br>docker 설치시멈추면 터미널 모두 끄고 설정 해주면 됨<br>sudo apt-get update
sudo apt-get install -y openjdk-17-jre-headless
복사<br><br>java -version
# openjdk version "17.0.11" 2024-04-16
복사<br><br><br>
<br>amd
<br>export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64
복사<br>
<br>arm (mac)
<br>export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-arm64
복사<br><br>source .bashrc
exec $SHELL
복사<br><br><br><br>cd

mkdir app
cd app

mkdir kafka
cd kafka
복사<br><br>Attention버전이 안 맞으면<br>
<a rel="noopener" class="external-link" href="https://downloads.apache.org/kafka" target="_blank">https://downloads.apache.org/kafka</a><br>
여기서 다운<br>wget https://downloads.apache.org/kafka/3.6.2/kafka_2.13-3.6.2.tgz
복사<br><br>tar -xzf kafka_2.13-3.6.2.tgz
복사<br><a data-href="kafka 설정 및 실행" href="root\dev\data-pipe-line\kafka\kafka-설정-및-실행.html" class="internal-link" target="_self" rel="noopener">kafka 설정 및 실행</a>]]></description><link>root\dev\data-pipe-line\kafka\kafka-설치.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka 설치.md</guid><pubDate>Wed, 31 Jul 2024 05:21:00 GMT</pubDate></item><item><title><![CDATA[kafka 클러스터 서버 시동]]></title><description><![CDATA[ 
 <br><br>확인
<br>server1 
<br>server2 
<br>server3
<br>~/run/ssh_check.sh
복사<br><br><br>시동
<br>server1 
<br>server2 
<br>server3
<br>~/run/kafka_zookeeper_start.sh
복사<br><br><br>시동
<br>server1 
<br>server2 
<br>server3
<br>~/run/kafka_server_start.sh
복사<br><br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-topics.sh --create --bootstrap-server 172.17.0.2:9092 --replication-factor 3 --partitions 3 --topic testtopic
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-topics.sh --delete --topic testtopic --bootstrap-server 172.17.0.2:9092
복사<br><br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-topics.sh --bootstrap-server 172.17.0.2:9092 --describe --topic testtopic
복사<br><br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-producer.sh --bootstrap-server 172.17.0.2:9092 --topic testtopic
복사<br><br><br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.2:9092 --topic testtopic
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.3:9092 --topic testtopic
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.4:9092 --topic testtopic
복사<br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-topics.sh --bootstrap-server 172.18.0.10:9092 --describe --topic testtopic

cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-producer.sh --bootstrap-server 172.18.0.10:9092 --topic testtopic


cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.10:9092 --topic testtopic

cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.11:9092 --topic testtopic

cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.12:9092 --topic testtopic
복사<br>ssh server1
cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.20.0.10:9092 --topic my_data

ssh server2
cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.20.0.11:9092 --topic my_data

ssh server3
cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.20.0.12:9092 --topic my_data
복사]]></description><link>root\dev\data-pipe-line\kafka\kafka-클러스터-서버-시동.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka 클러스터 서버 시동.md</guid><pubDate>Fri, 23 Aug 2024 06:04:49 GMT</pubDate></item><item><title><![CDATA[Kafka Consumer와 MySQL에 데이터 적재하기]]></title><description><![CDATA[ 
 <br><br>Infocsv -&gt; python -&gt; kafka -&gt; mysql<br><br><br>pip install kafka-python pandas sqlalchemy pymysql
복사<br><br><br>vi kafka_consumer_mysql.py
복사<br>from kafka import KafkaProducer, KafkaConsumer
import pandas as pd
import json
from sqlalchemy import create_engine, Table, Column, MetaData, inspect
from sqlalchemy.types import String, BigInteger, Float, DateTime
import pymysql

CSV_DATA = 'data_mini.csv'

# 데이터베이스 연결 설정
DATABASE_USER = 'ubuntu'
DATABASE_PASSWORD = '1234'
DATABASE_HOST = 'localhost'
DATABASE_NAME = 'my_data'

# Kafka 설정
KAFKA_BOOTSTRAP_SERVERS = ['172.17.0.2:9092', '172.17.0.3:9092', '172.17.0.4:9092']
KAFKA_TOPIC = 'testtopic'
KAFKA_GROUP_ID = 'my-group'  # group_id 설정

# 데이터베이스 연결 함수
def create_database_if_not_exists():
	connection = pymysql.connect(
		host=DATABASE_HOST,
		user=DATABASE_USER,
		password=DATABASE_PASSWORD
	)
	try:
		with connection.cursor() as cursor:
			cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}")
	finally:
		connection.close()

# SQLAlchemy 데이터 타입 매핑 함수
def get_sqlalchemy_type(dtype):
	if pd.api.types.is_integer_dtype(dtype):
		return BigInteger
	elif pd.api.types.is_float_dtype(dtype):
		return Float
	elif pd.api.types.is_datetime64_any_dtype(dtype):
		return DateTime
	else:
		return String(length=255)

# 데이터베이스 및 테이블 설정
def setup_database_and_table(csv_path):
	engine = create_engine(f'mysql+pymysql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}/{DATABASE_NAME}')
	
	df = pd.read_csv(csv_path)
	
	metadata = MetaData()
	
	columns = [Column(col, get_sqlalchemy_type(df[col].dtype)) for col in df.columns]
	table = Table('test_a', metadata, *columns)
	
	inspector = inspect(engine)
	if not inspector.has_table('test_a'):
		metadata.create_all(engine)
		print(f"Table 'test_a' created successfully from CSV file.")
	else:
		print(f"Table 'test_a' already exists.")

def produce_csv_to_kafka(file_path, topic):
	data = pd.read_csv(file_path)
	producer = KafkaProducer(
		bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
		value_serializer=lambda v: json.dumps(v).encode('utf-8')
	)
	
	for _, row in data.iterrows():
		message = row.to_dict()
		producer.send(topic, value=message)
	producer.flush()
	producer.close()
	print("CSV 데이터를 Kafka에 게시 완료.")

def load_data_to_mysql(data):
	engine = create_engine(f'mysql+pymysql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}/{DATABASE_NAME}')
	df = pd.DataFrame(data)
	df.to_sql('test_a', engine, if_exists='append', index=False)

consumer = KafkaConsumer(
	KAFKA_TOPIC,
	bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
	auto_offset_reset='earliest',
	enable_auto_commit=False,
	group_id=KAFKA_GROUP_ID,  # group_id 추가
	value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

create_database_if_not_exists()
setup_database_and_table(CSV_DATA)

produce_csv_to_kafka(CSV_DATA, KAFKA_TOPIC)

data_buffer = []
buffer_limit = 100
timeout_ms = 500  # 5초 동안 새로운 메시지가 없으면 타임아웃

while True:
	message_pack = consumer.poll(timeout_ms=timeout_ms)
	
	if not message_pack:
		break
	
	for tp, messages in message_pack.items():
		for message in messages:
			data_buffer.append(message.value)
			
			if len(data_buffer) &gt;= buffer_limit:
				load_data_to_mysql(data_buffer)
				data_buffer = []
				consumer.commit()

if data_buffer:
	load_data_to_mysql(data_buffer)
	consumer.commit()

consumer.close()

print("Kafka에서 MySQL로 데이터 적재 완료.")

Run복사<br><br><br>
<br>
Kafka Consumer 설정:

<br>KafkaConsumer를 사용하여 지정된 토픽(my_topic)에서 메시지를 소비합니다.
<br>bootstrap_servers에는 Kafka 클러스터의 브로커 주소를 지정합니다.


<br>
MySQL 데이터베이스와 테이블 설정:

<br>데이터베이스와 테이블이 존재하지 않으면 새로 생성합니다.


<br>
데이터 적재 함수 (load_data_to_mysql):

<br>Kafka에서 받은 메시지를 pandas 데이터프레임으로 변환하고, to_sql 메서드를 사용하여 MySQL에 데이터를 적재합니다.
<br>if_exists='append' 옵션을 사용하여 기존 테이블에 데이터를 추가합니다.


<br>
Kafka에서 메시지 소비 및 MySQL에 적재:

<br>Kafka에서 메시지를 소비하여 data_buffer에 저장합니다.
<br>버퍼의 데이터 수가 일정 한도(buffer_limit)를 넘으면, load_data_to_mysql 함수를 호출하여 MySQL에 데이터를 적재합니다.
<br>버퍼가 비워진 후에도 데이터가 남아 있으면, 마지막으로 MySQL에 적재합니다.


<br><br>
<br>
데이터 변환: Kafka에서 받은 데이터는 JSON 형식으로 변환되므로, MySQL의 테이블 구조에 맞게 데이터 변환이 필요할 수 있습니다. 실제 환경에서는 데이터를 필터링하거나 변환할 필요가 있을 수 있습니다.

<br>
에러 처리: 실제 사용 시에는 예외 처리 및 로그 기록을 추가하여 오류를 모니터링하고 대응할 수 있습니다.

<br>
성능 고려: 데이터 적재 성능을 높이기 위해, 데이터 배치 처리(batch processing)나 병렬 처리를 고려할 수 있습니다.

<br>이 코드는 실시간으로 Kafka에서 데이터를 소비하고 MySQL에 적재할 수 있도록 설계되었으며, 데이터가 지속적으로 들어오는 환경에서 유용합니다.<br><br><br>Info데이터를 MySQL에 적재할 때, Kafka를 거치는 것과 직접 MySQL에 적재하는 것 사이에는 여러 가지 차이점과 고려할 사항이 있습니다. 아래에 각각의 방법이 데이터베이스에 미치는 영향과 안정성에 대해 비교하여 설명하겠습니다.<br><br>장점:<br>
<br>
내결함성 (Fault Tolerance):

<br>Kafka는 메시지의 내구성을 보장하고 데이터 복제를 통해 장애에 대비할 수 있습니다. Kafka에 데이터가 게시되면, Kafka는 이를 복제하고 유지하여 MySQL 데이터베이스가 다운되더라도 데이터 손실을 방지할 수 있습니다.


<br>
성능 향상 (Performance):

<br>Kafka는 데이터를 버퍼링하고, 배치로 처리하여 대량의 데이터를 효율적으로 처리합니다. 이로 인해 MySQL 데이터베이스에 직접적인 부하를 줄일 수 있습니다.


<br>
비동기 처리 (Asynchronous Processing):

<br>Kafka는 비동기 처리 모델을 사용하여 데이터 전송과 처리를 분리합니다. 이를 통해 데이터 수집과 처리의 병목 현상을 줄일 수 있습니다.


<br>
확장성 (Scalability):

<br>Kafka는 클러스터 기반으로 설계되어 데이터의 양이 증가하더라도 확장 가능성을 제공합니다. Kafka의 파티셔닝 및 복제 기능을 활용하여 데이터 처리량을 높일 수 있습니다.


<br>단점:<br>
<br>
복잡성 증가 (Increased Complexity):

<br>Kafka를 추가하면 시스템의 복잡성이 증가합니다. Kafka 브로커의 설정, 모니터링, 유지 관리가 필요합니다.


<br>
지연 (Latency):

<br>Kafka를 경유하면 데이터 전송에 약간의 지연이 있을 수 있습니다. 하지만 이는 일반적으로 성능 향상과 데이터 안정성을 고려할 때 감수할 수 있는 부분입니다.


<br><br>장점:<br>
<br>
간단한 아키텍처 (Simpler Architecture):

<br>Kafka를 사용하지 않으면 데이터 전송 경로가 간단해지며 시스템이 간결해집니다. 설정 및 유지 관리가 단순해질 수 있습니다.


<br>
즉각적인 데이터 적재 (Immediate Data Insertion):

<br>데이터가 직접적으로 MySQL에 적재되므로 지연이 적을 수 있습니다.


<br>단점:<br>
<br>
부하 (Load):

<br>MySQL에 직접적으로 데이터를 적재할 경우, 데이터베이스에 큰 부하를 줄 수 있습니다. 특히 대량의 데이터가 동시에 들어오면 MySQL의 성능이 저하될 수 있습니다.


<br>
내결함성 부족 (Lack of Fault Tolerance):

<br>MySQL에 직접 데이터를 적재하는 경우, 데이터베이스의 장애나 다운타임에 대한 대비책이 부족할 수 있습니다. 데이터 손실이나 서비스 중단의 위험이 존재합니다.


<br>
확장성 문제 (Scalability Issues):

<br>데이터량이 증가할수록 MySQL의 성능이 저하될 수 있으며, 클러스터를 사용하지 않는 경우 데이터베이스의 확장에 한계가 있을 수 있습니다.


<br>
동기 처리 (Synchronous Processing):

<br>데이터 적재가 동기식으로 처리되면 데이터베이스의 응답 시간과 부하에 영향을 받을 수 있습니다.


<br><br><br>
<br>
Kafka를 사용하는 경우: Kafka는 데이터의 내결함성과 확장성을 제공하며, 대량의 데이터를 처리할 때 안정성을 높입니다. MySQL에 대한 부하를 줄이고, 장애 발생 시 데이터 손실을 방지할 수 있습니다. Kafka는 비동기식 데이터 전송을 통해 성능을 향상시키지만, 시스템의 복잡성이 증가할 수 있습니다.

<br>
Kafka 없이 직접 MySQL에 적재하는 경우: 시스템이 간단해지고 즉각적인 데이터 적재가 가능하지만, 데이터베이스에 부하가 증가하고 내결함성과 확장성에 한계가 있을 수 있습니다. 특히 대량의 데이터가 있을 때 안정성 문제를 겪을 수 있습니다.

<br>따라서, 데이터의 양과 시스템의 요구 사항에 따라 적절한 접근 방식을 선택하는 것이 중요합니다. 대량의 데이터 처리와 높은 안정성이 필요하다면 Kafka를 사용하는 것이 좋습니다. 반면, 데이터량이 상대적으로 적고 시스템의 복잡성을 줄이려는 경우 직접 MySQL에 적재하는 것도 고려할 수 있습니다.]]></description><link>root\dev\data-pipe-line\kafka\kafka-consumer와-mysql에-데이터-적재하기.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/Kafka Consumer와 MySQL에 데이터 적재하기.md</guid><pubDate>Fri, 16 Aug 2024 13:48:24 GMT</pubDate></item><item><title><![CDATA[kafka error handling]]></title><description><![CDATA[ 
 <br><br><br>rm -rf /tmp/kafka-logs
복사<br><br>로그를 분석해보면, Kafka 서버가 시작 중에 InconsistentClusterIdException 오류를 발생시키며 종료되고 있습니다. 이 오류는 Kafka 클러스터의 ID가 meta.properties 파일에 저장된 클러스터 ID와 일치하지 않음을 나타냅니다. 구체적인 내용은 다음과 같습니다:<br>
<br>
오류 원인:

<br>InconsistentClusterIdException: 이 예외는 Kafka 브로커가 현재 연결된 ZooKeeper의 클러스터 ID와 meta.properties 파일에 저장된 클러스터 ID가 일치하지 않을 때 발생합니다. 현재의 클러스터 ID는 FGvt6oxDTaScq61Hj0FieQ인데, meta.properties에는 6NLjhRndTQW9WpESn_EiEw가 저장되어 있습니다.


<br>
문제 해결 방법:

<br>클러스터 ID 확인: ZooKeeper에 저장된 클러스터 ID와 meta.properties 파일의 클러스터 ID가 일치하는지 확인합니다. meta.properties 파일은 Kafka 데이터 디렉터리 (/tmp/kafka-logs 등)에 위치합니다.
<br>클러스터 재설정: 클러스터 ID 불일치 문제를 해결하기 위해, meta.properties 파일을 삭제하고 Kafka 브로커를 재시작하여 새로운 클러스터 ID를 생성할 수 있습니다. 주의: 이 작업은 클러스터의 상태를 초기화하게 되므로, 데이터 손실이 발생할 수 있습니다.
<br>ZooKeeper 설정 확인: zookeeper.connect 설정이 올바른지 확인합니다. 잘못된 ZooKeeper 주소가 설정되어 있을 경우 클러스터 ID 불일치 문제가 발생할 수 있습니다.


<br>
조치 사항:

<br>Kafka가 실행되고 있는 디렉토리에서 meta.properties 파일을 확인하거나 삭제합니다. Kafka가 올바르게 시작될 수 있도록, 필요한 경우 kafka-server-start.sh를 다시 실행합니다.
<br>ZooKeeper 설정이 올바른지 확인한 후, 브로커를 재시작하여 문제를 해결합니다.


<br>이러한 단계를 통해 문제를 해결할 수 있으며, 상황에 따라 클러스터 설정을 재검토하거나 Kafka 및 ZooKeeper를 재설치해야 할 수도 있습니다.]]></description><link>root\dev\data-pipe-line\kafka\kafka-error-handling.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka error handling.md</guid><pubDate>Sat, 27 Jul 2024 09:06:31 GMT</pubDate></item><item><title><![CDATA[kafka note]]></title><description><![CDATA[ 
 <br>bin/kafka-broker-api-versions.sh --bootstrap-server 172.31.6.227:9092
복사<br><br><br>./bin/kafka-topics.sh --create --bootstrap-server 172.31.6.227:9092 --replication-factor 3 --partitions 3 --topic testtopic
복사<br>./bin/kafka-topics.sh --bootstrap-server 172.31.6.227:9092 --describe --topic testtopic
복사<br><br><br>./bin/kafka-console-producer.sh --bootstrap-server 172.31.6.227:9092 --topic testtopic
복사<br><br>./bin/kafkaconsole-consumer.sh --bootstrap-server 172.31.14.208:9092 --topic testtopic --from-beginning
복사<br><br>./bin/kafkaconsole-consumer.sh --bootstrap-server 172.31.1.189:9092 --topic testtopic --from-beginning
복사<br><br><br><br>Kafka의 명령줄 도구를 사용하여 특정 토픽의 데이터를 확인할 수 있습니다. <br><br>kafka-console-consumer는 Kafka 토픽에서 메시지를 소비하고 출력하는 명령줄 도구입니다. 이 도구를 사용하여 실시간으로 데이터가 올라와 있는지 확인할 수 있습니다.<br>kafka-console-consumer --bootstrap-server localhost:9092 --topic my_topic --from-beginning
복사<br>./bin/kafka-console-consumer.sh --bootstrap-server 172.17.0.4:9092 --topic my_topi --from-beginning
복사<br>
<br>--bootstrap-server: Kafka 클러스터의 브로커 주소를 지정합니다.
<br>--topic: 데이터를 확인할 토픽의 이름을 지정합니다.
<br>--from-beginning: 토픽의 시작부터 메시지를 소비합니다. 이 옵션을 생략하면, 현재로부터 새로운 메시지만 소비합니다.
<br>이 명령을 실행하면 해당 토픽에 올라와 있는 메시지가 콘솔에 출력됩니다.<br><br>토픽의 메타데이터나 메시지 수를 확인할 수 있습니다.<br>kafka-topics --bootstrap-server.sh localhost:9092 --describe --topic my_topic
복사<br>./bin/kafka-topics --bootstrap-server.sh 172.17.0.4:9092 --describe --topic my_topic
복사<br>이 명령은 지정한 토픽의 파티션과 리더 상태, 오프셋 정보를 출력합니다. 이 정보를 통해 토픽의 상태를 확인할 수 있습니다.<br><br>Kafka 클라이언트 라이브러리를 사용하여 프로그래밍적으로 토픽의 데이터를 조회할 수도 있습니다. 다음은 Python을 사용한 예제입니다.<br><br>from kafka import KafkaConsumer

# Kafka Consumer 설정
consumer = KafkaConsumer(
    'my_topic',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    value_deserializer=lambda x: x.decode('utf-8')
)

# 메시지 출력
for message in consumer:
    print(f"Key: {message.key}, Value: {message.value}")
    # 특정 조건에서 루프를 종료하려면 추가 로직을 구현
    break  # 이 예제에서는 첫 번째 메시지 하나만 출력 후 종료
Run복사<br>이 코드는 my_topic 토픽에서 메시지를 소비하고 출력합니다. auto_offset_reset='earliest'를 설정하면 토픽의 처음부터 메시지를 읽습니다.<br><br>Kafka의 웹 기반 UI 도구를 사용하여 토픽의 데이터와 상태를 시각적으로 확인할 수 있습니다. 이러한 도구로는 다음과 같은 것들이 있습니다:<br>
<br><a data-tooltip-position="top" aria-label="https://docs.confluent.io/platform/current/control-center/index.html" rel="noopener" class="external-link" href="https://docs.confluent.io/platform/current/control-center/index.html" target="_blank">Confluent Control Center</a>: Confluent Platform의 일부로 제공되며, Kafka 클러스터를 모니터링하고 관리할 수 있습니다.
<br><a data-tooltip-position="top" aria-label="https://github.com/obsidiandynamics/kafdrop" rel="noopener" class="external-link" href="https://github.com/obsidiandynamics/kafdrop" target="_blank">Kafdrop</a>: Kafka 브로커와 토픽을 시각적으로 확인할 수 있는 오픈소스 웹 UI 도구입니다.
<br>이 도구들은 Kafka 클러스터의 상태를 모니터링하고, 토픽의 메시지를 조회하는 데 유용합니다.<br><br>
<br>명령줄 도구: kafka-console-consumer를 사용하여 실시간으로 토픽의 데이터를 확인할 수 있습니다.
<br>클라이언트 라이브러리: Python 등에서 Kafka 클라이언트를 사용하여 프로그램적으로 데이터를 조회할 수 있습니다.
<br>웹 UI 도구: Confluent Control Center나 Kafdrop 등의 도구를 사용하여 시각적으로 Kafka 클러스터와 토픽의 상태를 확인할 수 있습니다.
<br>이 방법들을 사용하여 Kafka에 데이터가 제대로 올라와 있는지 확인할 수 있습니다.<br><br><br>
<br>
토픽의 데이터 유지:

<br>Kafka는 메시지를 토픽에 저장하고, 이 메시지는 기본적으로 삭제되지 않고 계속 유지됩니다. --from-beginning 옵션을 사용하면 토픽의 시작부터 모든 메시지를 읽게 됩니다.


<br>
오프셋 관리:

<br>Kafka는 메시지의 위치를 오프셋(offset)이라는 숫자로 관리합니다. --from-beginning을 사용하면, 소비자가 현재 오프셋이 아닌, 토픽의 시작부터 모든 메시지를 읽기 시작합니다. 
<br>만약 소비자가 처음으로 메시지를 읽는 것이라면, 메시지의 시작부터 읽게 되며, 이전에 작성된 메시지도 모두 포함됩니다.


<br>
소비자 그룹 상태:

<br>만약 특정 소비자 그룹이 이미 존재하고 그 그룹이 저장된 오프셋이 있으면, --from-beginning 옵션을 사용하더라도 해당 소비자 그룹의 저장된 오프셋이 무시되고 시작부터 모든 메시지를 읽게 됩니다.
<br>새로운 소비자 그룹을 사용하거나, 기존 소비자 그룹의 오프셋이 초기화된 경우에도 동일한 결과를 얻을 수 있습니다.


<br>예시:<br>
<br>kafka-console-consumer.sh --bootstrap-server 172.17.0.2:9092 --topic my_topic --from-beginning 명령어를 실행하면, my_topic에서 생성된 모든 메시지를 토픽의 시작부터 현재까지 읽어오게 됩니다.
<br>이 옵션을 사용하지 않으면, 소비자는 마지막으로 읽은 지점부터 새로운 메시지만 읽어오게 됩니다. 만약 이전 메시지들만 읽고 싶지 않다면 --from-beginning 옵션을 제거하고 명령어를 실행하세요:<br>kafka-console-consumer.sh --bootstrap-server 172.17.0.2:9092 --topic my_topic
복사<br>이렇게 하면, 해당 토픽에서 새로 생성된 메시지만을 실시간으로 소비하게 됩니다.]]></description><link>root\dev\data-pipe-line\kafka\kafka-note.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka note.md</guid><pubDate>Sat, 03 Aug 2024 15:37:04 GMT</pubDate></item><item><title><![CDATA[kafka server.properties]]></title><description><![CDATA[ 
 <br># Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#
# This configuration file is intended for use in ZK-based mode, where Apache ZooKeeper is required.
# See kafka.server.KafkaConfig for additional details and defaults
#

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

############################# Socket Server Settings #############################

# The address the socket server listens on. If not configured, the host name will be equal to the value of
# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
listeners=PLAINTEXT://172.17.0.2:9092
# ubuntu@ip-172-31-6-227:~/app/kafka/kafka_2.13-3.6.1/config$

# Listener name, hostname and port the broker will advertise to clients.
# If not set, it uses the value for "listeners".
advertised.listeners=PLAINTEXT://172.17.0.2:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
# num.partitions=1
num.partitions=3

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
#log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
# zookeeper.connect=localhost:2181
zookeeper.connect=172.17.0.2:2181,172.17.0.3:2181,172.17.0.4:2181
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
복사]]></description><link>root\dev\data-pipe-line\kafka\kafka-server.properties.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka server.properties.md</guid><pubDate>Sun, 28 Jul 2024 12:21:49 GMT</pubDate></item><item><title><![CDATA[kafka에 csv로 데이터 흘리기]]></title><description><![CDATA[ 
 <br><br>Infocsv -&gt; python -&gt; kafka<br><br><br>pip install kafka-python pandas
복사<br><br><br>vi csv_to_kafka.py
복사<br>from kafka import KafkaProducer
import pandas as pd
import json

def produce_csv_to_kafka(file_path, topic):
	# CSV 데이터 읽기
	data = pd.read_csv(file_path)

	# Kafka 프로듀서 설정 (클러스터 환경)
	producer = KafkaProducer(
		bootstrap_servers=['172.17.0.2:9092', '172.17.0.3:9092', '172.17.0.4:9092'],
		value_serializer=lambda v: json.dumps(v).encode('utf-8')
	)

	# 각 행을 Kafka 메시지로 게시
	for _, row in data.iterrows():
		message = row.to_dict()
		producer.send(topic, value=message)

	producer.flush()
	print("Kafka에 CSV 데이터 게시 완료.")

# CSV 파일 경로와 Kafka 토픽 설정
csv_file_path = 'data.csv'
produce_csv_to_kafka(csv_file_path, 'my_topic')
Run복사<br><br><br>
<br>CSV 데이터 읽기: pandas 라이브러리를 사용하여 CSV 파일을 읽습니다.
<br>Kafka 프로듀서 설정: 

<br>KafkaProducer를 사용하여 Kafka 클러스터에 데이터를 게시합니다.
<br>bootstrap_servers에는 Kafka 클러스터의 브로커 주소를 지정합니다.
<br>value_serializer를 사용하여 메시지 값을 JSON 형식으로 직렬화합니다.


<br>Kafka에 데이터 게시: 

<br>CSV 데이터프레임의 각 행을 row.to_dict()를 사용하여 딕셔너리 형태로 변환합니다.
<br>각 딕셔너리를 Kafka 메시지로 게시합니다.
<br>producer.flush()를 호출하여 모든 메시지가 Kafka에 전송되도록 합니다.


<br><br>
<br>데이터 형식: CSV 파일의 데이터 형식이 Kafka의 데이터 형식과 일치해야 합니다. 필요에 따라 데이터 변환이 필요할 수 있습니다.
<br>에러 처리: 실제 사용 시에는 예외 처리 및 로그 기록을 추가하여 오류를 모니터링하고 대응할 수 있습니다.
<br>성능 고려: 대량의 데이터를 처리하는 경우, 데이터를 배치로 처리하거나 병렬 처리를 고려하여 성능을 향상시킬 수 있습니다.
<br><br>이 코드는 CSV 파일을 읽어 Kafka에 데이터를 게시하는 기본적인 예시입니다. 실제 환경에서는 다음과 같은 추가 기능을 고려해야 할 수 있습니다.<br>
<br>데이터 필터링: 특정 조건을 만족하는 데이터만 Kafka에 게시합니다.
<br>데이터 변환: CSV 파일의 데이터 형식을 Kafka에 적합하게 변환합니다.
<br>데이터 압축: 데이터 크기를 줄이기 위해 데이터를 압축합니다.
<br>메시지 키 설정: Kafka의 파티셔닝을 위해 메시지 키를 설정합니다.
<br>메시지 타임스탬프 설정: 메시지의 타임스탬프를 설정하여 시간순으로 데이터를 처리합니다.
<br>데이터 유효성 검사: 데이터의 유효성을 검사하여 오류를 방지합니다.
<br>이 코드를 기반으로 필요한 추가 기능을 구현하여 CSV 파일에서 Kafka로 데이터를 효율적으로 전송할 수 있습니다. ]]></description><link>root\dev\data-pipe-line\kafka\kafka에-csv로-데이터-흘리기.html</link><guid isPermaLink="false">root/dev/data pipe line/kafka/kafka에 csv로 데이터 흘리기.md</guid><pubDate>Wed, 31 Jul 2024 06:16:05 GMT</pubDate></item><item><title><![CDATA[aws ec2에 docker로 kafka 클러스터 만들 때]]></title><description><![CDATA[ 
 <br>import sys, time
from pathlib import Path

base_dir = Path('/home/ubuntu/run')
sys.path.insert(0, str(base_dir))
from config.config import server_infos
from util.util import send_ssh_comm

ssh_key_dir = f"{base_dir}/config/keygen"
id_rsa_path = Path(ssh_key_dir) / "id_rsa"
id_rsa_pub_path = Path(ssh_key_dir) / "id_rsa.pub"

# ~/.ssh/config

ssh_config = '''
Host server1
	HostName 172.31.14.186
	Port 2222
	User ubuntu

Host server2
	HostName 172.31.10.99
	Port 2222
	User ubuntu

Host server3
	HostName 172.31.1.229
	Port 2222
	User ubuntu
'''

def get_create_container_comm(id, name, port=2222, server_infos=server_infos):
	with open(id_rsa_path, 'r') as id_rsa_file:
		id_rsa = id_rsa_file.read()
	
	with open(id_rsa_pub_path, 'r') as id_rsa_pub_file:
		id_rsa_pub = id_rsa_pub_file.read()
	print(id)
	return f'''
	docker network create --subnet=172.18.0.0/16 pipeline
	docker container run -itd \\
		--name {name} \\
		--hostname {name} \\
		--add-host {server_infos[0].name}:{server_infos[0].ip} \\
		--add-host {server_infos[1].name}:{server_infos[1].ip} \\
		--add-host {server_infos[2].name}:{server_infos[2].ip} \\
		-p {port}:{port} \\
		-p 2181:2181 \\
		-p 2888:2888 \\
		-p 3888:3888 \\
		-p 9092:9092 \\
		--net pipeline \\
		--ip 172.18.0.{id+10} \\
		namugach/ubuntu-pipeline:24.04-kafka-test \\
		/bin/bash -c "echo '{id_rsa}' &gt; /home/ubuntu/.ssh/id_rsa &amp;&amp; \\
				echo '{id_rsa_pub}' &gt; /home/ubuntu/.ssh/id_rsa.pub &amp;&amp; \\
				echo '{id_rsa_pub}' &gt; /home/ubuntu/.ssh/authorized_keys &amp;&amp; \\
				echo '{ssh_config}' &gt;&gt; /home/ubuntu/.ssh/config &amp;&amp; \\
				chmod 600 /home/ubuntu/.ssh/id_rsa &amp;&amp; \\
				chmod 644 /home/ubuntu/.ssh/id_rsa.pub &amp;&amp; \\
				sudo service ssh start &amp;&amp; \\
				sudo service mysql start &amp;&amp; \\
				tail -f /dev/null"
	'''

for info in server_infos:
	send_ssh_comm(info.name, info.name, get_create_container_comm(info.id, info.name))
	time.sleep(1)
Run복사<br><br>docker network create --subnet=172.18.0.0/16 pipeline
--ip 172.18.0.{id+10} \\
Run복사<br>이거 알아내려고.. 그랬다..]]></description><link>root\dev\data-pipe-line\server\ec2\aws-ec2에-docker로-kafka-클러스터-만들-때.html</link><guid isPermaLink="false">root/dev/data pipe line/server/ec2/aws ec2에 docker로 kafka 클러스터 만들 때.md</guid><pubDate>Sun, 04 Aug 2024 19:25:10 GMT</pubDate></item><item><title><![CDATA[ec2 docker ssh 터널링]]></title><description><![CDATA[ 
 <br>Attention보안에서 2222 포트 열어주면 됨<br># docker -&gt; ec2
ssh server1 -p 2222 -i ./ssh/id_rsa

# ec2 -&gt; docker
ssh server2 localhost -p 2222 -i ./ssh/id_rsa
복사<br>172.31.14.186 server1
172.31.10.99 server2
172.31.1.229 server3

docker run -d -p 22:22 --name my_ssh_container ubuntu:24.04


docker run -itd \
  --name docker_server \
  -p 22:22 \
  namugach/ubuntu-pipeline:24.04-kafka-test

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.31.14.186 \
  --add-host server2:172.31.10.99 \
  --add-host server3:172.31.1.229 \
  -p 2222:2222 \
  namugach/ubuntu-pipeline:24.04-kafka-test

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.31.14.186 \
  --add-host server2:172.31.10.99 \
  --add-host server3:172.31.1.229 \
  namugach/ubuntu-pipeline:24.04-kafka-test
복사]]></description><link>root\dev\data-pipe-line\server\ec2\ec2-docker-ssh-터널링.html</link><guid isPermaLink="false">root/dev/data pipe line/server/ec2/ec2 docker ssh 터널링.md</guid><pubDate>Sun, 04 Aug 2024 14:14:38 GMT</pubDate></item><item><title><![CDATA[ssh 포트 포함 이동]]></title><description><![CDATA[ 
 <br>/etc/hosts 파일에는 호스트 이름과 IP 주소의 매핑만을 설정할 수 있습니다. 포트 번호를 직접 지정할 수 있는 기능은 없습니다. ssh 명령어에서 포트 번호를 지정하려면 ssh 구성 파일을 사용하는 것이 일반적인 방법입니다.<br><br>SSH 구성 파일을 사용하여 특정 호스트에 대해 기본 포트 번호를 설정할 수 있습니다. ~/.ssh/config 파일에 다음과 같이 설정을 추가하면 됩니다:<br># ~/.ssh/config

Host server1
	HostName 172.31.14.186
	Port 2222
	User ubuntu

Host server2
	HostName 172.31.1.229
	Port 2222
	User ubuntu

Host server3
	HostName 172.31.10.99
	Port 2222
	User ubuntu
Run복사<br>이렇게 설정하면 ssh server1 명령어를 실행할 때 자동으로 -p 2222 옵션이 적용됩니다.<br><br>구성 파일을 설정한 후, send_ssh_comm 함수를 다음과 같이 사용할 수 있습니다:<br>import subprocess

def send_ssh_comm(host, name, comm):
	ssh_command = [
			'ssh',
			'-o', 'StrictHostKeyChecking=no',
			host, comm
	]
	print(f"=========={name} 실행...========")
	run = subprocess.run(ssh_command, capture_output=True, text=True)
	print(run.stdout)

# 사용 예시
send_ssh_comm('server1', '테스트 명령', 'echo Hello, World!')
Run복사<br>이렇게 하면 server1에 대해 ~/.ssh/config 파일에 정의된 설정이 자동으로 적용됩니다.<br><br>/etc/hosts 파일은 그대로 유지합니다:<br>172.31.14.186   server1
172.31.1.229    server2
172.31.10.99    server3
Run복사<br>이렇게 하면 server1, server2, server3에 대해 정의된 IP 주소가 호스트 이름으로 매핑됩니다.<br><br>import subprocess

def send_ssh_comm(host, name, comm):
	ssh_command = [
			'ssh',
			'-o', 'StrictHostKeyChecking=no',
			host, comm
	]
	print(f"=========={name} 실행...========")
	run = subprocess.run(ssh_command, capture_output=True, text=True)
	print(run.stdout)

# 사용 예시
send_ssh_comm('server1', '테스트 명령', 'echo Hello, World!')
send_ssh_comm('server2', '테스트 명령', 'echo Hello, World!')
send_ssh_comm('server3', '테스트 명령', 'echo Hello, World!')
Run복사<br>이렇게 하면 server1, server2, server3에 대해 정의된 포트 번호가 자동으로 적용됩니다.]]></description><link>root\dev\data-pipe-line\server\ec2\ssh-포트-포함-이동.html</link><guid isPermaLink="false">root/dev/data pipe line/server/ec2/ssh 포트 포함 이동.md</guid><pubDate>Sun, 04 Aug 2024 13:41:28 GMT</pubDate></item><item><title><![CDATA[ssh server 터널링]]></title><description><![CDATA[ 
 <br><br>nc -zv localhost 2181
nc -zv localhost 9092
복사<br><br><br># ZooKeeper 포트 포워딩
ssh -L 2181:localhost:2181 -N -f serverNumber

# Kafka 포트 포워딩
ssh -L 9092:localhost:9092 -N -f serverNumber
복사<br><br>ssh -L 2181:localhost:2181 -L 9092:localhost:9092 -N -f ubuntu@172.31.14.186

# ZooKeeper 포트 포워딩
ssh -L 2181:localhost:2181 -N -f server1

# Kafka 포트 포워딩
ssh -L 9092:localhost:9092 -N -f server1
복사<br><br>
ssh -L 2181:localhost:2181 -L 9092:localhost:9092 -N -f ubuntu@172.31.10.99 


# ZooKeeper 포트 포워딩
ssh -L 2181:localhost:2181 -N -f server2

# Kafka 포트 포워딩
ssh -L 9092:localhost:9092 -N -f server2
복사<br><br>ssh -L 2181:localhost:2181 -L 9092:localhost:9092 -N -f ubuntu@172.31.1.229


# ZooKeeper 포트 포워딩
ssh -L 2181:localhost:2181 -N -f server3

# Kafka 포트 포워딩
ssh -L 9092:localhost:9092 -N -f server3
복사]]></description><link>root\dev\data-pipe-line\server\ec2\ssh-server-터널링.html</link><guid isPermaLink="false">root/dev/data pipe line/server/ec2/ssh server 터널링.md</guid><pubDate>Sun, 04 Aug 2024 17:06:42 GMT</pubDate></item><item><title><![CDATA[우분투 hosts를 설정하여 ssh 편하게 이동하기]]></title><description><![CDATA[ 
 <br><br>sudo vi /etc/hosts
복사<br>ip_주소 사용하고_싶은_server_이름
복사<br>172.17.0.2 server1
172.17.0.3 server2
172.17.0.4 server3
복사<br><br><br><br>ssh-keygen -t rsa
복사<br>경고야!비번을 입력하지 않고 조건 없이 엔터<br><br>cd ~/.ssh/id_rsa.pub
복사<br>접속자의 pc에서 생성된 id_rsa.pub 를<br>vi ~/.ssh/authorized_keys
복사<br>touch ~/.ssh/authorized_keys
복사<br>접속할 pc의 authorized_keys안에 입력 한다.]]></description><link>root\dev\data-pipe-line\server\local\우분투-hosts를-설정하여-ssh-편하게-이동하기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/우분투 hosts를 설정하여 ssh 편하게 이동하기.md</guid><pubDate>Sat, 27 Jul 2024 07:21:52 GMT</pubDate></item><item><title><![CDATA[우분투 sudo 비번 생략하기]]></title><description><![CDATA[ 
 <br>Attentionroot 계정으로 작업 해야함<br>echo '유저_이름 ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers
복사<br>echo 'ubuntu ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers
복사]]></description><link>root\dev\data-pipe-line\server\local\우분투-sudo-비번-생략하기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/우분투 sudo 비번 생략하기.md</guid><pubDate>Sun, 28 Jul 2024 14:01:02 GMT</pubDate></item><item><title><![CDATA[docker container 시작 시 명령어 시작]]></title><description><![CDATA[ 
 <br>sudo 권한으로 실행하는 것들은 앞에 sudo를 붙여 줘야 함.<br>
그리고 계정이 sudo passwd 생략 할 수 있는 권한이 있어야 함
<a data-href="우분투 sudo 비번 생략하기" href="root\dev\data-pipe-line\server\local\우분투-sudo-비번-생략하기.html" class="internal-link" target="_self" rel="noopener">우분투 sudo 비번 생략하기</a><br>docker run -itd \
--name my_ssh_container \
root-ubuntu:24.01-mysql \
/bin/bash -c "sudo service ssh start &amp;&amp; tail -f /dev/null"
복사<br>docker run -itd --name my_ssh_container \
root-ubuntu:24.01-mysql \
/bin/bash -c "sudo service ssh start &amp;&amp; sudo service mysql start &amp;&amp; tail -f /dev/null"
복사<br>컨테이너가 바로 종료되는 이유는 -c "service ssh start &amp;&amp; bash" 명령어가 제대로 전달되지 않아서 bash가 실행되지 않기 때문이야. Docker에서는 CMD 명령어를 통해 전달되는 쉘 명령어를 실행할 때, -c 옵션을 사용하지 않아도 돼.<br>올바르게 실행하려면 다음과 같이 해봐:<br>docker run -itd --name my_ssh_container root-ubuntu:24.04-ssh-key /bin/bash -c "service ssh start &amp;&amp; tail -f /dev/null"
복사<br>여기서 tail -f /dev/null은 컨테이너가 백그라운드에서 계속 실행되도록 해주는 역할을 해. <br><br>
<br>docker run -itd: 백그라운드에서 인터랙티브 모드로 컨테이너 실행.
<br>--name my_ssh_container: 컨테이너 이름을 my_ssh_container로 설정.
<br>root-ubuntu:24.04-ssh-key: 사용할 Docker 이미지 이름.
<br>/bin/bash -c "service ssh start &amp;&amp; tail -f /dev/null": SSH 서비스를 시작하고, tail -f /dev/null 명령어를 실행해서 컨테이너가 계속 실행 상태를 유지하도록 함.
<br>이 명령어를 실행하면 SSH 서비스가 시작된 상태로 컨테이너가 계속 실행될 거야. 이제 컨테이너가 종료되지 않고, SSH 서버도 실행된 상태를 유지할 거야.]]></description><link>root\dev\data-pipe-line\server\local\docker-container-시작-시-명령어-시작.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/docker container 시작 시 명령어 시작.md</guid><pubDate>Sun, 28 Jul 2024 15:39:58 GMT</pubDate></item><item><title><![CDATA[Kafdrop kafka 클러스터 모니터링]]></title><description><![CDATA[ 
 <br>여러 브로커 서버가 있는 경우, KAFKA_BROKERCONNECT 환경 변수를 여러 브로커를 포함하도록 설정할 수 있습니다. Kafdrop은 이 변수를 통해 Kafka 클러스터의 브로커와 연결합니다. <br>브로커 서버가 server1, server2, server3에 위치하고 각각의 IP 주소가 172.17.0.2, 172.17.0.3, 172.17.0.4라면, 이를 다음과 같이 설정할 수 있습니다.<br><br>여러 브로커를 설정하려면 브로커 주소를 쉼표로 구분하여 KAFKA_BROKERCONNECT 환경 변수에 지정합니다.<br>docker run -d -p 9000:9000 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  -e KAFKA_BROKERCONNECT=server1:9092,server2:9092,server3:9092 \
  obsidiandynamics/kafdrop
복사<br><br>컨테이너가 실행된 후 브라우저를 열고 다음 URL로 접속합니다.<br>http://localhost:9000
복사<br>http://172.17.0.2:9000
복사<br>이제 Kafdrop UI를 통해 Kafka 클러스터의 여러 브로커를 모니터링하고 관리할 수 있습니다.<br><br><br>브로커 서버가 Docker 컨테이너 내에서 실행되고 있다면, Docker 네트워크 설정이 올바르게 되어 있는지 확인해야 합니다. 브로커 컨테이너와 Kafdrop 컨테이너가 동일한 네트워크에 있어야 통신이 가능합니다.<br><br>위 예시에서는 브로커 호스트가 server1, server2, server3이고, 포트가 9092로 동일하다고 가정했습니다. 실제 환경에서는 호스트명과 포트 번호가 다를 수 있으니 환경에 맞게 수정하세요.<br><br>여러 Kafka 브로커를 사용하는 경우, Docker 명령어에서 --add-host 옵션을 사용해 브로커 호스트를 추가하고, KAFKA_BROKERCONNECT 환경 변수에 여러 브로커를 쉼표로 구분하여 설정합니다. 이를 통해 Kafdrop을 사용해 여러 브로커가 있는 Kafka 클러스터를 편리하게 관리할 수 있습니다.]]></description><link>root\dev\data-pipe-line\server\local\kafdrop-kafka-클러스터-모니터링.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/Kafdrop kafka 클러스터 모니터링.md</guid><pubDate>Sat, 03 Aug 2024 13:19:25 GMT</pubDate></item><item><title><![CDATA[mysql 계정 생성시 %의 의미]]></title><description><![CDATA[ 
 <br><br>
<br>%: MySQL에서 %는 "모든 호스트"를 의미해. 즉, %를 사용하면 어떤 IP 주소에서든지 이 사용자 계정으로 MySQL 서버에 접근할 수 있도록 허용하는 거야.
<br><br>
<br>
모든 호스트에서 접근 가능:
CREATE USER 'ubuntu'@'%' IDENTIFIED BY '1234';
GRANT ALL PRIVILEGES ON *.* TO 'ubuntu'@'%' WITH GRANT OPTION;
Run복사
이 명령어는 ubuntu라는 사용자 계정이 어떤 IP 주소에서든지 접속할 수 있도록 설정해. 즉, server3에서뿐만 아니라 다른 모든 서버에서도 이 계정을 사용해 MySQL 서버에 접근할 수 있어.

<br>
특정 IP 주소에서만 접근 가능:
만약 특정 IP 주소에서만 접근을 허용하려면 % 대신 그 IP 주소를 사용하면 돼. 예를 들어 172.17.0.3에서만 접근을 허용하려면:
CREATE USER 'ubuntu'@'172.17.0.3' IDENTIFIED BY '1234';
GRANT ALL PRIVILEGES ON *.* TO 'ubuntu'@'172.17.0.3' WITH GRANT OPTION;
Run복사
이 경우, ubuntu 사용자는 172.17.0.3에서만 MySQL 서버에 접근할 수 있어.

<br>
특정 호스트 이름에서 접근 가능:
특정 호스트 이름을 사용해 접근을 제한할 수도 있어. 예를 들어 server1.example.com에서만 접근을 허용하려면:
CREATE USER 'ubuntu'@'server1.example.com' IDENTIFIED BY '1234';
GRANT ALL PRIVILEGES ON *.* TO 'ubuntu'@'server1.example.com' WITH GRANT OPTION;
Run복사
이 경우, ubuntu 사용자는 server1.example.com이라는 호스트 이름으로만 MySQL 서버에 접근할 수 있어.

<br><br>
<br>%: 모든 호스트에서 접근 허용.
<br>특정 IP 주소: 특정 IP 주소에서만 접근 허용.
<br>호스트 이름: 특정 호스트 이름에서만 접근 허용.
<br>이렇게 설정을 통해 MySQL 서버에 접근할 수 있는 사용자 계정과 그 계정이 접속할 수 있는 호스트를 세밀하게 제어할 수 있어. 추가로 궁금한 점이 있으면 언제든지 물어봐!<br>
이렇게 만드는데, '계정명'@'% 이 부분에서 % 부분에 대해서 알려줘]]></description><link>root\dev\data-pipe-line\server\local\mysql-계정-생성시-의-의미.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/mysql 계정 생성시 %의 의미.md</guid><pubDate>Tue, 30 Jul 2024 15:36:37 GMT</pubDate></item><item><title><![CDATA[ssh 접속시 yes 입력 생략하기]]></title><description><![CDATA[ 
 <br><br>SSH 서버에 처음 접속할 때 "yes"를 입력하는 과정은 서버의 호스트 키를 확인하고 저장하는 과정이야. 이 과정을 자동으로 생략하려면 StrictHostKeyChecking 옵션을 사용하면 돼.<br>다음과 같은 방법을 사용해봐:<br>
<br>명령어에 직접 추가하기:
<br>ssh -o StrictHostKeyChecking=no user@hostname
복사<br>
<br>SSH 설정 파일 수정하기:<br>
SSH 설정 파일인 ~/.ssh/config에 설정을 추가하면 매번 옵션을 입력하지 않아도 돼.
<br>
<br>설정 파일을 열고 (없으면 새로 만들면 돼):
<br>vi ~/.ssh/config
복사<br>
<br>다음 내용을 추가해:
<br>Host 172.*.*.*
 StrictHostKeyChecking no
 UserKnownHostsFile=/dev/null
복사<br>이렇게 하면 모든 호스트에 대해 호스트 키 확인을 하지 않게 돼.<br>주의: StrictHostKeyChecking=no 옵션을 사용하면 보안적으로 위험할 수 있어. 신뢰할 수 없는 서버에 접속할 때는 주의가 필요해. 가능한 경우, 초기 연결 시만 "yes"를 입력하고 나중에 설정 파일을 통해 관리하는 방법을 추천해.<br><br><br>맞아, SSH 설정 파일에 다음 내용을 추가하면 모든 호스트에 대해 StrictHostKeyChecking을 비활성화하고, 호스트 키를 known_hosts 파일에 저장하지 않게 설정할 수 있어. 이렇게 하면 SSH 접속 시 "yes"를 입력할 필요가 없어져.<br><br>
<br>
SSH 설정 파일 열기:
nano ~/.ssh/config
복사

<br>
다음 내용을 추가:
Host *
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사
위 설정은 모든 SSH 접속에 대해 적용되며, 특정 IP 범위에 대해서만 적용하고 싶다면 Host * 대신 특정 IP 패턴을 사용할 수 있어.

<br><br>만약 특정 IP 범위(예: 172.*.*.*)에만 적용하고 싶다면 다음과 같이 설정할 수 있어:<br>Host 172.*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br><br>Host server*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br>이렇게 하면 <br>ssh server1
ssh server2
ssh server3
복사<br>이렇게 모두 생략할 수 있다.]]></description><link>root\dev\data-pipe-line\server\local\ssh-접속시-yes-입력-생략하기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/local/ssh 접속시 yes 입력 생략하기.md</guid><pubDate>Mon, 05 Aug 2024 13:05:46 GMT</pubDate></item><item><title><![CDATA[우분투 root 계정일 때 bash 일반 계정처럼 만들기]]></title><description><![CDATA[ 
 <br># ~/.bashrc: executed by bash(1) for non-login shells.
# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)
# for examples

# If not running interactively, don't do anything
case $- in
    *i*) ;;
      *) return;;
esac

# don't put duplicate lines or lines starting with space in the history.
# See bash(1) for more options
HISTCONTROL=ignoreboth

# append to the history file, don't overwrite it
shopt -s histappend

# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)
HISTSIZE=1000
HISTFILESIZE=2000

# check the window size after each command and, if necessary,
# update the values of LINES and COLUMNS.
shopt -s checkwinsize

# If set, the pattern "**" used in a pathname expansion context will
# match all files and zero or more directories and subdirectories.
#shopt -s globstar

# make less more friendly for non-text input files, see lesspipe(1)
[ -x /usr/bin/lesspipe ] &amp;&amp; eval "$(SHELL=/bin/sh lesspipe)"

# set variable identifying the chroot you work in (used in the prompt below)
if [ -z "${debian_chroot:-}" ] &amp;&amp; [ -r /etc/debian_chroot ]; then
    debian_chroot=$(cat /etc/debian_chroot)
fi

# set a fancy prompt (non-color, unless we know we "want" color)
case "$TERM" in
    xterm-color|*-256color) color_prompt=yes;;
esac

# uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes

if [ -n "$force_color_prompt" ]; then
    if [ -x /usr/bin/tput ] &amp;&amp; tput setaf 1 &gt;&amp;/dev/null; then
	# We have color support; assume it's compliant with Ecma-48
	# (ISO/IEC-6429). (Lack of such support is extremely rare, and such
	# a case would tend to support setf rather than setaf.)
	color_prompt=yes
    else
	color_prompt=
    fi
fi

if [ "$color_prompt" = yes ]; then
    PS1='${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '
else
    PS1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '
fi
unset color_prompt force_color_prompt

# If this is an xterm set the title to user@host:dir
case "$TERM" in
xterm*|rxvt*)
    PS1="\[\e]0;${debian_chroot:+($debian_chroot)}\u@\h: \w\a\]$PS1"
    ;;
*)
    ;;
esac

# enable color support of ls and also add handy aliases
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors &amp;&amp; eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)"
    alias ls='ls --color=auto'
    #alias dir='dir --color=auto'
    #alias vdir='vdir --color=auto'

    alias grep='grep --color=auto'
    alias fgrep='fgrep --color=auto'
    alias egrep='egrep --color=auto'
fi

# colored GCC warnings and errors
#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'

# some more ls aliases
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'

# Add an "alert" alias for long running commands.  Use like so:
#   sleep 10; alert
alias alert='notify-send --urgency=low -i "$([ $? = 0 ] &amp;&amp; echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&amp;|]\s*alert$//'\'')"'

# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.

if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi

복사]]></description><link>root\dev\data-pipe-line\server\우분투-root-계정일-때-bash-일반-계정처럼-만들기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/우분투 root 계정일 때 bash 일반 계정처럼 만들기.md</guid><pubDate>Sun, 11 Aug 2024 16:37:46 GMT</pubDate></item><item><title><![CDATA[웹 브라우저에서 Kafka 클러스터를 모니터링]]></title><description><![CDATA[ 
 <br><br>
# Prometheus 실행
cd ~/app/prometheus-2.32.1.linux-amd64
./prometheus --config.file=prometheus.yml

# Kafka Exporter 실행
cd ~/app/kafka_exporter-1.4.2.linux-amd64
./kafka_exporter --kafka.server=172.17.0.2:9092

# grafana 실행
sudo grafana-server --homepath=/usr/share/grafana &amp;

복사<br><br><br>웹 브라우저에서 Kafka 클러스터를 모니터링하려면 Prometheus와 Grafana를 설치하고 설정한 다음, 웹 브라우저를 통해 Grafana 대시보드를 확인하면 돼. 여기서 그 과정을 단계별로 설명할게.<br><br>1.1. Prometheus 설치<br># Prometheus 다운로드
wget https://github.com/prometheus/prometheus/releases/download/v2.32.1/prometheus-2.32.1.linux-amd64.tar.gz

# 압축 해제
tar -xvf prometheus-2.32.1.linux-amd64.tar.gz
cd ~/app/prometheus-2.32.1.linux-amd64

# Prometheus 실행
./prometheus --config.file=prometheus.yml
복사<br>1.2. Prometheus 설정 파일(prometheus.yml)<br>prometheus.yml 파일을 다음과 같이 설정해:<br>global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['localhost:9308'] # Kafka Exporter가 실행되는 호스트와 포트
복사<br><br>2.1. Kafka Exporter 설치<br># Kafka Exporter 다운로드
wget https://github.com/danielqsj/kafka_exporter/releases/download/v1.4.2/kafka_exporter-1.4.2.linux-amd64.tar.gz

# 압축 해제
tar -xvf kafka_exporter-1.4.2.linux-amd64.tar.gz
cd ~/app/kafka_exporter-1.4.2.linux-amd64

# Kafka Exporter 실행
./kafka_exporter --kafka.server=172.17.0.2:9092
복사<br><br>3.1. Grafana 설치<br># Grafana 다운로드 및 설치 (Ubuntu 예시)
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
sudo apt-get update
sudo apt-get install -y grafana

# Grafana 실행
sudo systemctl start grafana-server
sudo systemctl enable grafana-server
복사<br><br>
<br>웹 브라우저에서 http://localhost:3000으로 이동해 Grafana에 로그인해. (기본 로그인: admin/admin)
<br>로그인을 완료한 후, 왼쪽 사이드바에서 "Add data source"를 클릭해.
<br>Prometheus를 선택해.
<br>Prometheus 설정 페이지에서 URL을 http://localhost:9090으로 설정하고 "Save &amp; Test"를 클릭해 연결을 확인해.
<br><br>
<br>Grafana 홈 화면에서 "Create" -&gt; "Import"를 클릭해.
<br>Import 페이지에서, Grafana Labs의 Kafka Exporter 대시보드 ID인 7589를 입력하고 "Load"를 클릭해.
<br>데이터를 입력하라는 창이 나타나면, "Prometheus" 데이터 소스를 선택해.
<br>"Import"를 클릭해 대시보드를 가져와.
<br><br>이제 웹 브라우저에서 http://localhost:3000으로 Grafana 웹 UI에 접속하면, Kafka 클러스터에 대한 다양한 모니터링 지표를 확인할 수 있는 대시보드를 볼 수 있을 거야.<br>이 과정에서 문제가 발생하면, 로그 파일을 확인하거나 설정 파일을 다시 검토해서 올바르게 설정되었는지 확인해봐. Prometheus와 Kafka Exporter, 그리고 Grafana가 제대로 설치되고 실행되어야만 모든 모니터링이 정상적으로 이루어질 거야.<br><br><br>네, 맞아. 컨테이너 내부에서 실행되는 서비스를 호스트 시스템에서 접근하려면 포트 포워딩이 필요해. 이걸 통해 컨테이너 내부의 포트를 호스트 시스템의 포트에 매핑할 수 있어. 그래서 호스트 시스템에서 localhost:3000으로 접근할 수 있게 돼.<br>포트 포워딩을 설정하지 않으면 컨테이너 내부의 서비스에 호스트 시스템에서 직접 접근할 수 없어. 예를 들어, Docker 컨테이너에서 Grafana를 실행하고 호스트 시스템에서 접근하려면 컨테이너를 시작할 때 -p 옵션을 사용해 포트를 매핑해야 해.<br>다시 정리해서 설명할게.<br><br>컨테이너를 시작할 때 포트 포워딩을 설정해서 다시 실행해 보자:<br><br>먼저, Ubuntu 컨테이너를 실행할 때 포트를 매핑해:<br>docker run -d -it --name monitoring-ubuntu -p 3000:3000 -p 9090:9090 -p 9308:9308 ubuntu
복사<br>이제 컨테이너에 접속해:<br>docker exec -it monitoring-ubuntu bash
복사<br><br>컨테이너 내부에서 필요한 패키지를 설치해:<br>apt-get update
apt-get install -y wget gnupg2 curl
복사<br><br># Prometheus 다운로드 및 압축 해제
wget https://github.com/prometheus/prometheus/releases/download/v2.32.1/prometheus-2.32.1.linux-amd64.tar.gz
tar -xvf prometheus-2.32.1.linux-amd64.tar.gz
mv prometheus-2.32.1.linux-amd64 /opt/prometheus

# Prometheus 설정 파일 작성
mkdir /opt/prometheus/config

cat &lt;&lt;EOF &gt; /opt/prometheus/config/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['localhost:9308']
EOF

# Prometheus 실행
/opt/prometheus/prometheus --config.file=/opt/prometheus/config/prometheus.yml &amp;
복사<br><br># Kafka Exporter 다운로드 및 압축 해제
wget https://github.com/danielqsj/kafka_exporter/releases/download/v1.4.2/kafka_exporter-1.4.2.linux-amd64.tar.gz
tar -xvf kafka_exporter-1.4.2.linux-amd64.tar.gz
mv kafka_exporter-1.4.2.linux-amd64 /opt/kafka_exporter

# Kafka Exporter 실행
/opt/kafka_exporter/kafka_exporter --kafka.server=localhost:9092 &amp;
복사<br><br>설치할 때!su root
복사
해서 root로 접속해서 설치해야함!<br>Grafana의 GPG 키를 추가하고 APT 소스 리스트에 Grafana 저장소를 추가해:<br># Grafana의 GPG 키 추가
wget -q -O - https://packages.grafana.com/gpg.key | apt-key add -

# Grafana 저장소 추가
add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"

# 패키지 리스트 업데이트 및 Grafana 설치
apt-get update
apt-get install -y grafana
복사<br>Grafana를 설치하고 서비스로 실행해:<br># Grafana 실행
grafana-server --homepath=/usr/share/grafana &amp;
복사<br><br>컨테이너 내부에서 Grafana 서버가 실행되면, 호스트 시스템의 웹 브라우저에서 http://localhost:3000으로 접속할 수 있어. 이후 절차는 다음과 같아:<br>
<br>웹 브라우저에서 http://localhost:3000으로 이동해 Grafana에 로그인해. (기본 로그인: admin/admin)
<br>로그인을 완료한 후, 왼쪽 사이드바에서 "Add data source"를 클릭해.
<br>Prometheus를 선택해.
<br>Prometheus 설정 페이지에서 URL을 http://localhost:9090으로 설정하고 "Save &amp; Test"를 클릭해 연결을 확인해.
<br>Grafana 홈 화면에서 "Create" -&gt; "Import"를 클릭해.
<br>Import 페이지에서, Grafana Labs의 Kafka Exporter 대시보드 ID인 7589를 입력하고 "Load"를 클릭해.
<br>데이터를 입력하라는 창이 나타나면, "Prometheus" 데이터 소스를 선택해.
<br>"Import"를 클릭해 대시보드를 가져와.
<br>이제 호스트 시스템의 웹 브라우저에서 http://localhost:3000으로 Grafana 웹 UI에 접속하면, Kafka 클러스터에 대한 다양한 모니터링 지표를 확인할 수 있어.]]></description><link>root\dev\data-pipe-line\server\웹-브라우저에서-kafka-클러스터를-모니터링.html</link><guid isPermaLink="false">root/dev/data pipe line/server/웹 브라우저에서 Kafka 클러스터를 모니터링.md</guid><pubDate>Sat, 03 Aug 2024 08:44:25 GMT</pubDate></item><item><title><![CDATA[docker 우분투 mysql 설치]]></title><description><![CDATA[ 
 <br><a rel="noopener" class="external-link" href="https://jongsky.tistory.com/79" target="_blank">https://jongsky.tistory.com/79</a><br><br>sudo apt-get update
sudo apt-get -y install mysql-server
복사<br>음...?ubuntu 계정의 sudo 패스워드를 패스 하는 권한을 줘서 그런지<br>
설치하고 나면 shell로 나와야 하는데
done!
update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode
Setting up libhtml-parser-perl:amd64 (3.81-1build3) ...
Setting up libhttp-message-perl (6.45-1ubuntu1) ...
Setting up mysql-server-8.0 (8.0.37-0ubuntu0.24.04.1) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 79.)
debconf: falling back to frontend: Readline
invoke-rc.d: could not determine current runlevel
invoke-rc.d: policy-rc.d denied execution of stop.
update-alternatives: using /etc/mysql/mysql.cnf to provide /etc/mysql/my.cnf (my.cnf) in auto mode
Renaming removed key_buffer and myisam-recover options (if present)
mysqld will log errors to /var/log/mysql/error.log
mysqld is running as pid 2171
복사
이렇게 뜨고, shell이 멈춤<br>
ctrl + c 로 캔슬로 안되고..
나갔다 들어오면 잘 되긴 함.<br><br><br>sudo service mysql start
복사<br><br><br><a data-href="mysql 계정 생성시 %의 의미" href="root\dev\data-pipe-line\server\local\mysql-계정-생성시-의-의미.html" class="internal-link" target="_self" rel="noopener">mysql 계정 생성시 %의 의미</a><br><br>sudo mysql -u root
복사<br><br>CREATE USER 'ubuntu'@'%' IDENTIFIED BY '1234';
복사<br>계정이 존재 하면 아래의 것으로 비번을 초기화 함ALTER USER 'ubuntu'@'%' IDENTIFIED BY '1234';
Run복사<br><br>GRANT ALL PRIVILEGES ON *.* TO 'ubuntu'@'%' WITH GRANT OPTION;
복사<br><br>FLUSH PRIVILEGES;
복사<br><br>exit
복사<br><br><br><br>sudo chown -R mysql:mysql /var/run/mysqld
sudo chmod 755 /var/run/mysqld
복사<br><br><br>
<br>열기
<br>sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf
복사<br>
<br>편집
<br>port                    = 3306
bind-address            = 0.0.0.0
복사<br><br>sudo service mysql restart
복사<br><br>
<br>입력
<br>sudo service --status-all
복사<br>
<br>출력
<br> [ - ]  dbus
 [ + ]  mysql
 [ - ]  procps
 [ - ]  ssh
 [ - ]  x11-common
복사<br><br>mysql -u ubuntu -p -h 자신의_아이피_주소
복사<br>
<br>ex
<br>mysql -u ubuntu -p -h 172.17.0.3
복사<br>Important테스트를 할 때, 설정을 한, 본 서버에서 접속이 된다면<br>
타 서버에서도 접속 가능.]]></description><link>root\dev\data-pipe-line\server\docker-우분투-mysql-설치.html</link><guid isPermaLink="false">root/dev/data pipe line/server/docker 우분투 mysql 설치.md</guid><pubDate>Wed, 31 Jul 2024 05:11:41 GMT</pubDate></item><item><title><![CDATA[Docker 태그 없는 이미지 삭제하기]]></title><description><![CDATA[ 
 <br><br>Docker 명령어를 사용해서 태그가 없는 이미지(즉, &lt;none&gt;으로 표시된 이미지)를 제거할 수 있어. 주로 사용되는 명령어는 docker image prune이야. 이 명령어는 사용하지 않는(dangling) 이미지를 삭제해줘.<br>먼저, 모든 사용하지 않는 이미지를 정리하고 싶으면 다음 명령어를 사용할 수 있어:<br>docker image prune -a
복사<br>하지만 이 명령어는 태그가 있는 사용되지 않는 이미지도 삭제하니까 주의해야 해.<br>만약 태그가 없는 이미지들만 정리하고 싶다면, 다음 명령어를 사용하면 돼:<br>docker images purge $(docker images -f "dangling=true" -q)
복사<br>위 명령어들은 아래와 같은 상황에서 실행될 수 있어:<br>
<br>모든 사용하지 않는 이미지(태그 있는 것과 없는 것 모두)를 삭제하고 싶을 때:
<br>docker image prune -a
복사<br>
<br>태그가 없는 이미지들만 삭제하고 싶을 때:
<br>docker image prune
복사<br>위 명령어들을 실행하면 이미지를 정리할 수 있어. 정리한 후 다시 이미지를 확인하고 싶으면 docker image ls -a를 다시 실행하면 돼.<br><br><br>태그 없는 이미지들을 삭제할 때, 그 이미지들이 다른 이미지의 기반이 되는 경우 영향을 미칠 수 있어. 다만, Docker는 기본적으로 실제로 사용되고 있는 이미지를 삭제하지 않기 때문에, 태그가 없는 이미지 중에서도 다른 이미지들이 의존하는 경우에는 삭제되지 않아.<br>안전하게 태그 없는 이미지를 삭제하기 위해 docker image prune 명령어를 사용하면 돼. 이 명령어는 사용되지 않는(dangling) 이미지들만 삭제하기 때문에, 다른 이미지에 영향을 주지 않아.<br>docker image prune
복사<br>이 명령어는 태그가 없는 사용되지 않는 이미지들만 삭제해.<br>만약 정말로 태그가 없는 이미지들이 사용 중인지 확인하고 싶다면, 아래 명령어로 확인할 수 있어:<br>docker images -f "dangling=true"
복사<br>이 명령어는 태그가 없는 이미지들만 표시해줘. 그리고 나서 안전하게 삭제할 수 있어.<br>마지막으로, 이미지 간의 의존성을 파악하기 위해선 docker image inspect 명령어를 사용할 수 있어:<br>docker image inspect &lt;IMAGE_ID&gt;
복사<br>이 명령어로 특정 이미지의 상세 정보를 확인할 수 있어. 이를 통해 이미지 간의 관계를 이해하고 삭제 여부를 결정할 수 있어.<br><br>의존성이 있는 이미지를 다룰 때는 신중해야 해. 이미지 간의 의존성을 확인하고 적절하게 처리하기 위한 몇 가지 방법이 있어.<br>
<br>의존성 확인:<br>
docker image inspect 명령어를 사용해서 이미지의 상세 정보를 확인하고, 이미지 간의 의존성을 파악해. 예를 들어, 특정 이미지의 부모 이미지가 무엇인지 확인할 수 있어.
<br>docker image inspect &lt;IMAGE_ID&gt;
복사<br>
<br>이미지 태그 변경:<br>
만약 특정 이미지가 다른 이미지에 의존하고 있고 태그가 없는 경우, 의존성 문제를 피하기 위해 태그를 추가해. 
<br>docker tag &lt;IMAGE_ID&gt; &lt;new_tag&gt;
복사<br>
<br>필요 없는 이미지 삭제:<br>
의존성이 없는 태그가 없는 이미지들을 안전하게 삭제하기 위해 docker image prune 명령어를 사용해.
<br>docker image prune
복사<br>이 명령어는 다른 이미지에 의존하지 않는 사용되지 않는(dangling) 이미지만 삭제해.<br>
<br>필요한 이미지 백업:<br>
의존성이 있는 이미지라도 삭제가 필요한 경우, 이미지를 백업한 후 삭제할 수 있어. 이미지를 백업하려면 docker save 명령어를 사용해.
<br>docker save -o &lt;output_file.tar&gt; &lt;IMAGE_ID&gt;
복사<br>백업 후 필요 시 docker load 명령어로 이미지를 복원할 수 있어.<br>docker load -i &lt;output_file.tar&gt;
복사<br>
<br>이미지 정리 후 확인:<br>
이미지를 삭제하거나 정리한 후, 다시 이미지 리스트를 확인하고 의도치 않은 이미지 삭제가 없는지 확인해.
<br>docker image ls -a
복사<br>요약하자면, 의존성이 있는 이미지를 처리하기 전에 반드시 의존성을 확인하고, 필요 시 태그를 변경하거나 백업을 진행한 후 안전하게 삭제해.]]></description><link>root\dev\data-pipe-line\server\docker-태그-없는-이미지-삭제하기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/Docker 태그 없는 이미지 삭제하기.md</guid><pubDate>Sat, 03 Aug 2024 09:03:29 GMT</pubDate></item><item><title><![CDATA[pyenv 설치]]></title><description><![CDATA[ 
 <br><br><br>sudo apt-get update
sudo apt-get install locales
sudo locale-gen en_US.UTF-8
export LANG=en_US.UTF-8
export LC_ALL=en_US.UTF-8
echo 'export LANG=en_US.UTF-8' &gt;&gt; ~/.bashrc
echo 'export LC_ALL=en_US.UTF-8' &gt;&gt; ~/.bashrc
source ~/.bashrc
복사<br>너무나 중요!!초기 설정을 모두 끝낸 후 터미널 재시작!!<br><br>exit
복사<br><br><br>sudo apt-get update; sudo apt-get -y install make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
복사<br><br><br>curl https://pyenv.run | bash
복사<br><br><br>vi ~/.bashrc
복사<br>export PYENV_ROOT="$HOME/.pyenv"
[[ -d $PYENV_ROOT/bin ]] &amp;&amp; export PATH="$PYENV_ROOT/bin:$PATH"
eval "$(pyenv init -)"
eval "$(pyenv virtualenv-init -)"
복사<br>Info.bashrc 하단에 추가<br>. ~/.bashrc
복사<br><br><br>pyenv install --list
복사<br><br><br>pyenv install 3.11.9
복사<br><br><br><br>pyenv versions
복사<br><br>pyenv virtualenv 3.11.9 py3_11_9
복사<br><br>pyenv activate py3_11_9
복사<br><br><br>. ~/.bashrc
복사<br>위와 아래는 같음<br>source ~/.bashrc
복사]]></description><link>root\dev\data-pipe-line\server\pyenv-설치.html</link><guid isPermaLink="false">root/dev/data pipe line/server/pyenv 설치.md</guid><pubDate>Sun, 28 Jul 2024 15:18:35 GMT</pubDate></item><item><title><![CDATA[python pip venv 가상환경 사용하지 않기]]></title><description><![CDATA[ 
 <br>sudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED
복사<br>이 방법이 제일 깔금함<br><br>이 에러 메시지는 우분투에서 Python 패키지 설치를 시스템 전역으로 하는 대신, 가상환경을 사용하도록 권장하는 PEP 668에 따른 거야. 도커 컨테이너에서는 가상환경 없이 설치하는 게 더 편할 수 있어, 그래서 이 경고 메시지를 비활성화하고 패키지를 설치할 수 있는 방법을 알려줄게.<br><br>가장 간단한 방법은 --break-system-packages 옵션을 추가해서 경고 메시지를 무시하고 패키지를 설치하는 거야. 예를 들어 pip을 사용할 때 이렇게 하면 돼:<br>pip install &lt;패키지명&gt; --break-system-packages
Run복사<br>이 방법은 도커 컨테이너 내부에서 패키지를 설치할 때 유용해.<br><br><br>PEP 668 기능을 완전히 비활성화하려면 externally-managed 파일을 삭제해도 돼. 이 파일이 /usr/lib/python&lt;버전&gt;/EXTERNALLY-MANAGED 경로에 있어. <br>파일을 삭제해서 비활성화하려면 아래 명령어를 사용하면 돼:<br>sudo rm /usr/lib/python3.11/EXTERNALLY-MANAGED
Run복사<br>이렇게 하면 PEP 668의 제한 없이 패키지를 설치할 수 있어. 이 방법은 도커 환경처럼 특정한 컨테이너 내부에서만 사용할 것을 권장해, 왜냐하면 시스템에 설치된 파이썬 환경을 깨뜨릴 수 있기 때문이야.<br><br><br>만약 특정 패키지만 독립적으로 설치하고 싶다면,pipx를 사용하는 것도 방법이야. pipx는 각각의 패키지를 자체적인 가상환경에 설치해서 충돌을 피할 수 있게 해줘.<br>pipx install &lt;패키지명&gt;
Run복사<br>하지만 도커 컨테이너에서 굳이 이렇게까지 할 필요는 없을 수도 있어.<br>도커 내부에서 자유롭게 패키지를 설치할 수 있도록 --break-system-packages 옵션이나 PEP 668 비활성화 방법을 선택하면 돼. 필요하면 더 물어봐!]]></description><link>root\dev\data-pipe-line\server\python-pip-venv-가상환경-사용하지-않기.html</link><guid isPermaLink="false">root/dev/data pipe line/server/python pip venv 가상환경 사용하지 않기.md</guid><pubDate>Fri, 09 Aug 2024 17:54:35 GMT</pubDate></item><item><title><![CDATA[간단 파이프 라인에 스파크 붙이기]]></title><description><![CDATA[ 
 <br><a data-href="python 간단 파이프라인" href="root\dev\data-pipe-line\데이터-파이프라인-구축\python-간단-파이프라인.html" class="internal-link" target="_self" rel="noopener">python 간단 파이프라인</a><br>
<a data-href="간단 파이프 라인에 카프카 붙이기" href="root\dev\data-pipe-line\데이터-파이프라인-구축\간단-파이프-라인에-카프카-붙이기.html" class="internal-link" target="_self" rel="noopener">간단 파이프 라인에 카프카 붙이기</a><br>Apache Spark를 데이터 파이프라인에 추가하면 데이터 처리 및 분석의 성능을 대폭 향상시킬 수 있어. Spark는 대량의 데이터를 빠르게 처리하고 분석할 수 있는 분산 처리 프레임워크야. 카프카와 스파크를 결합하면 실시간 데이터 스트리밍과 대규모 데이터 처리를 더 강력하게 할 수 있어.<br><br>
<br>CSV에서 카프카로 데이터 전송: CSV 파일에서 카프카로 데이터를 전송 (위에서 다룬 내용).
<br>스파크에서 카프카로부터 데이터 스트리밍: 카프카 토픽에서 실시간으로 데이터를 읽어들이고, Spark로 데이터 처리 및 변환을 수행.
<br>스파크에서 MySQL로 데이터 적재: 처리된 데이터를 MySQL 데이터베이스에 적재.
<br><br>아래는 스파크를 사용하여 카프카에서 데이터를 읽고, 간단한 변환을 수행한 후 MySQL에 적재하는 예제야.<br><br>스파크를 설치하고, 스파크에서 카프카를 사용할 수 있도록 필요한 라이브러리를 설치해:<br>pip install pyspark
pip install kafka-python
복사<br>스파크 클러스터를 로컬에 설치하거나 클라우드에 배포할 수 있어. <a data-tooltip-position="top" aria-label="https://spark.apache.org/docs/latest/" rel="noopener" class="external-link" href="https://spark.apache.org/docs/latest/" target="_blank">스파크 공식 문서</a>를 참조해 스파크를 설정하고, 필요한 경우 스파크 클러스터를 시작해.<br><br>from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# 스파크 세션 설정
spark = SparkSession.builder \
    .appName("KafkaToMySQL") \
    .getOrCreate()

# 카프카에서 데이터 읽기
kafka_stream = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "my_topic") \
    .load()

# 데이터의 스키마 정의
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])

# 카프카 메시지의 값 파싱
parsed_df = kafka_stream.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# 데이터 변환 (예: 나이 컬럼을 1.5배로 변환)
transformed_df = parsed_df.withColumn("age", col("age") * 1.5)

# MySQL로 데이터 적재
def write_to_mysql(df, batch_id):
    df.write \
        .format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/testdb") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .option("dbtable", "users") \
        .option("user", "ubuntu") \
        .option("password", "1234") \
        .mode("append") \
        .save()

query = transformed_df.writeStream \
    .foreachBatch(write_to_mysql) \
    .outputMode("append") \
    .start()

query.awaitTermination()
Run복사<br><br>
<br>스파크 세션 설정: 스파크 세션을 생성해.
<br>카프카에서 데이터 읽기: spark.readStream.format("kafka")를 사용해 카프카에서 데이터를 스트리밍으로 읽어들여.
<br>데이터 파싱 및 변환: 카프카 메시지에서 JSON 값을 파싱하고, 나이 컬럼을 1.5배로 변환해.
<br>MySQL로 데이터 적재: write_to_mysql 함수에서 스파크 데이터프레임을 MySQL 데이터베이스에 적재해.
<br><br>
<br>MySQL JDBC 드라이버: 스파크가 MySQL에 연결할 수 있도록 MySQL JDBC 드라이버가 필요해. com.mysql.cj.jdbc.Driver를 사용해. 드라이버를 스파크 클러스터의 클래스패스에 추가해야 해.
<br>스파크 클러스터: 로컬 모드가 아닌 클러스터 모드에서 실행할 경우, 스파크 클러스터 설정을 맞춰야 해.
<br>이렇게 하면 카프카와 스파크를 결합하여 실시간으로 데이터를 처리하고, MySQL에 적재하는 강력한 데이터 파이프라인을 구현할 수 있어.]]></description><link>root\dev\data-pipe-line\spark\간단-파이프-라인에-스파크-붙이기.html</link><guid isPermaLink="false">root/dev/data pipe line/spark/간단 파이프 라인에 스파크 붙이기.md</guid><pubDate>Sun, 28 Jul 2024 08:28:58 GMT</pubDate></item><item><title><![CDATA[spark에 대한 고찰]]></title><description><![CDATA[ 
 <br>Spark는 빅 데이터 처리와 분석에 널리 사용되는 오픈 소스 분산 컴퓨팅 시스템입니다. Kafka와 마찬가지로 데이터 처리와 분석에 있어 강력한 도구이지만, Spark는 특히 대규모 데이터 처리와 분석에서 빛을 발합니다. Spark를 사용할 때와 사용하지 않을 때의 차이점과 이를 체험할 수 있는 실습 코드를 제공하겠습니다.<br><br>
<br>
속도:

<br>Spark는 메모리 내에서 데이터를 처리하기 때문에, 디스크 기반의 Hadoop MapReduce에 비해 최대 100배 빠른 속도로 데이터를 처리할 수 있습니다.


<br>
유연성:

<br>Spark는 SQL, 데이터 프레임, 스트리밍, 머신러닝, 그래프 처리를 지원하여 다양한 데이터 처리 요구사항을 충족할 수 있습니다.


<br>
확장성:

<br>Spark는 클러스터 컴퓨팅 프레임워크로 설계되어 있으며, 대규모 데이터 처리와 분석 작업을 수천 대의 노드에서 동시에 수행할 수 있습니다.


<br>
배치 및 실시간 스트리밍 처리:

<br>Spark는 배치 처리와 실시간 스트리밍 처리를 모두 지원하여, 실시간 데이터 분석 및 처리에도 적합합니다.


<br><br><br>이 시나리오에서는 데이터를 MySQL에 직접 로드하고, SQL 쿼리를 사용하여 분석하거나 처리합니다.<br><br>
<br>설정이 간단하며, SQL을 사용해 손쉽게 데이터를 처리할 수 있습니다.
<br><br>
<br>대규모 데이터 처리에 있어 성능이 제한됩니다.
<br>실시간 데이터 처리 및 복잡한 데이터 분석 작업에 있어 유연성이 떨어집니다.
<br><br>이 시나리오에서는 데이터를 Spark에 로드하여, 데이터 프레임 API, SQL, 또는 스트리밍 API를 사용하여 데이터를 처리하고 분석합니다.<br><br>
<br>대규모 데이터 세트를 빠르게 처리할 수 있습니다.
<br>실시간 데이터 스트리밍과 배치 처리 모두 지원합니다.
<br>다양한 분석 작업을 효율적으로 수행할 수 있습니다.
<br><br>
<br>초기 설정과 학습 곡선이 있으며, 클러스터 관리와 같은 추가적인 운영 작업이 필요할 수 있습니다.
<br><br><br>
<br>Apache Spark 설치
<br>PySpark 라이브러리 설치
<br><br><br>name,age
Alice,30
Bob,24
Charlie,29
복사<br><br>기본적으로 MySQL로 CSV 데이터를 읽고 처리하는 코드는 이전과 동일하므로 생략합니다.<br><br>from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# SparkSession 생성
spark = SparkSession.builder \
    .appName("CSV to DataFrame") \
    .config("spark.master", "local") \
    .getOrCreate()

# CSV 파일을 Spark DataFrame으로 로드
df = spark.read.csv('users.csv', header=True, inferSchema=True)

# 데이터 출력
df.show()

# 데이터 필터링 (예: 나이가 25 이상인 사용자만 선택)
filtered_df = df.filter(col('age') &gt;= 25)

# 필터링된 데이터 출력
filtered_df.show()

# 필터링된 데이터를 MySQL에 저장 (예제, 실제 동작 시 데이터베이스 연결 필요)
# filtered_df.write \
#     .format("jdbc") \
#     .option("url", "jdbc:mysql://localhost:3306/my_data") \
#     .option("dbtable", "users_filtered") \
#     .option("user", "root") \
#     .option("password", "password") \
#     .save()

# SparkSession 종료
spark.stop()
Run복사<br><br>
<br>
Spark 없이 MySQL을 사용하는 경우:

<br>데이터를 MySQL에 직접 로드하고, SQL을 사용하여 데이터를 분석하거나 처리합니다.


<br>
Spark를 사용하는 경우:

<br>Spark 데이터 프레임 API를 사용하여 데이터를 로드, 처리, 분석합니다. 예를 들어, 특정 조건에 맞는 데이터를 필터링하거나, 데이터를 집계하고, 이를 다시 MySQL에 저장할 수 있습니다.


<br>이 실습을 통해 Spark를 사용했을 때의 장점, 특히 대규모 데이터 처리 속도와 유연성을 직접 경험할 수 있습니다. Spark의 강력한 데이터 처리 및 분석 기능은 특히 대량의 데이터 처리 및 복잡한 분석 작업에서 그 진가를 발휘합니다.]]></description><link>root\dev\data-pipe-line\spark\spark에-대한-고찰.html</link><guid isPermaLink="false">root/dev/data pipe line/spark/spark에 대한 고찰.md</guid><pubDate>Sat, 27 Jul 2024 07:10:26 GMT</pubDate></item><item><title><![CDATA[docker 파이프라인 aws로 이전]]></title><description><![CDATA[ 
 <br>작업에 들어가기 앞서!
<br>(L): local에서 하는 작업

<br>windows: wsl
<br>mac: terminal


<br>(C): container에서 하는 작업
<br><br>확인!ec2 server 1, 2, 3 적용<br><a data-href="docker pipe line 환경 구축 v0.1#ssh 연결" href="root\dev\data-pipe-line\docker-pipe-line-환경-구축-v0.1.html#ssh_연결" class="internal-link" target="_self" rel="noopener">docker pipe line 환경 구축 v0.1 &gt; ssh 연결</a><br><br>ec2_private_ip server1
ec2_private_ip server2
ec2_private_ip server3
복사<br><br>이것이 바로 포인트!server1 에서 생성된 id_rsa, id_rsa.pub를<br>
server2, server3의 ~/.ssh 디렉토리 안에 모두 공유한다.<br>ssh-keygen -t rsa
cd .ssh
cat id_rsa.pub &gt; authorized_keys
복사<br><br>vi ~/.ssh/config
복사<br>Host server*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br><br>ssh server1
ssh server2
ssh server3
복사<br>혹은<br>
<a data-href="shell ssh 여러 개 접속 테스트" href="root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html" class="internal-link" target="_self" rel="noopener">shell ssh 여러 개 접속 테스트</a> 를 작성<br><br><br>확인!ec2 server 1, 2, 3 적용<br><a data-href="pyenv 설치" href="root\dev\data-pipe-line\server\pyenv-설치.html" class="internal-link" target="_self" rel="noopener">pyenv 설치</a><br><br><br>확인!ec2 server 1, 2, 3 적용<br><br><a data-href="도커 설치 및 기본 사용법" href="도커 설치 및 기본 사용법" class="internal-link" target="_self" rel="noopener">도커 설치 및 기본 사용법</a><br><br>docker pull namugach/ubuntu-pipeline:24.04-kafka
복사<br><br>docker network create --subnet=172.18.0.0/16 pipeline
복사<br>도커 네트워크 생성<br><br>Check
<br>
name

<br>ec1 server1 = server1
<br>ec2 server1 = server2
<br>ec3 server1 = server3


<br>
hostname

<br>ec1 server1 = server1
<br>ec2 server1 = server2
<br>ec3 server1 = server3


<br>
ip

<br>ec1 server1 = 172.18.0.10
<br>ec2 server1 = 172.18.0.11
<br>ec3 server1 = 172.18.0.12



인트턴스 서버에 따라 잘 확인하고 입력<br>docker container run -itd \\
  --name server1 \\ # server1, server2, server3
  --hostname server1 \\ # server1, server2, server3
  --add-host server1:server1 \\
  --add-host server2:server2 \\
  --add-host server3:server3 \\
  -p 2222:2222 \\
  -p 2181:2181 \\
  -p 2888:2888 \\
  -p 3888:3888 \\
  -p 9092:9092 \\
  -p 3306:3306 \\
  --net pipeline \\
  --ip 172.18.0.10 \\ # server1 = 10, server2 = 11, server3 = 12
  namugach/ubuntu-pipeline:24.04-kafka
복사<br><br><br>유념!서버 이름 확인하면서 작업하자<br>docker cp ~/.ssh/id_rsa server1:/home/ubuntu/.ssh/id_rsa
docker cp ~/.ssh/id_rsa.pub server1:/home/ubuntu/.ssh/id_rsa.pub
docker cp ~/.ssh/id_rsa.pub server1:/home/ubuntu/.ssh/authorized_keys
복사<br><br>docker exec -it server1 ls ~/.ssh
복사<br><br><br><br>vi ~/.ssh/config
복사<br>Host server1
	HostName ec2_private_ip
	Port 2222
	User ubuntu

Host server2
	HostName ec2_private_ip
	Port 2222
	User ubuntu

Host server3
	HostName ec2_private_ip
	Port 2222
	User ubuntu
복사<br><br>Host server1
	HostName 172.31.14.186
	Port 2222
	User ubuntu

Host server2
	HostName 172.31.10.99
	Port 2222
	User ubuntu

Host server3
	HostName 172.31.1.229
	Port 2222
	User ubuntu
복사<br>ssh server1
복사<br>ubuntu@server1:~$ ssh server2
Warning: Permanently added '[172.31.10.99]:2222' (ED25519) to the list of known hosts.
Welcome to Ubuntu 24.04 LTS (GNU/Linux 6.8.0-1012-aws x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/pro

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Wed Aug  7 05:22:57 2024 from 172.31.14.186
ubuntu@server2:~$ 
복사<br>위처럼 나오면 성공<br><br><br><br><br># Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#
# This configuration file is intended for use in ZK-based mode, where Apache ZooKeeper is required.
# See kafka.server.KafkaConfig for additional details and defaults
#

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=1

############################# Socket Server Settings #############################

# The address the socket server listens on. If not configured, the host name will be equal to the value of
# java.net.InetAddress.getCanonicalHostName(), with PLAINTEXT listener name, and port 9092.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
listeners=PLAINTEXT://server1:9092
# ubuntu@ip-172-31-6-227:~/app/kafka/kafka_2.13-3.6.1/config$

# Listener name, hostname and port the broker will advertise to clients.
# If not set, it uses the value for "listeners".
advertised.listeners=PLAINTEXT://server1:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
# num.partitions=1
num.partitions=3

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=3

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
#log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
# zookeeper.connect=localhost:2181
zookeeper.connect=server1:2181,server2:2181,server3:2181
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=18000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
복사<br><br># Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# the directory where the snapshot is stored.
dataDir=/home/ubuntu/zkdata
# the port at which the clients will connect
clientPort=2181
tickTime=2000
initLimit=10
syncLimit=5
server.1=server1:2888:3888
server.2=server2:2888:3888
server.3=server3:2888:3888
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080
복사<br><a data-href="kafka 설정 및 실행" href="root\dev\data-pipe-line\kafka\kafka-설정-및-실행.html" class="internal-link" target="_self" rel="noopener">kafka 설정 및 실행</a><br><br><br><a data-href="Kafka Consumer와 MySQL에 데이터 적재하기" href="root\dev\data-pipe-line\kafka\kafka-consumer와-mysql에-데이터-적재하기.html" class="internal-link" target="_self" rel="noopener">Kafka Consumer와 MySQL에 데이터 적재하기</a><br>KAFKA_BOOTSTRAP_SERVERS = ['server1:9092', 'server2:9092', 'server3:9092']
Run복사<br>위의 부분을 바꿔줌<br><br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.11:9092 --topic testtopic
복사<br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.12:9092 --topic testtopic
복사<br>cd ~/app/kafka/kafka_2.13-3.6.2
./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.13:9092 --topic testtopic
복사]]></description><link>root\dev\data-pipe-line\docker-파이프라인-aws로-이전.html</link><guid isPermaLink="false">root/dev/data pipe line/docker 파이프라인 aws로 이전.md</guid><pubDate>Sun, 18 Aug 2024 18:15:57 GMT</pubDate></item><item><title><![CDATA[docker in docker로 local에서 클러스터 환경 만들기]]></title><description><![CDATA[ 
 <br>커밋할 때절 때 hostname으로 이름 지은 것으로 하지 말아야함.<br>
그렇지 않고, hostname이 지정된 상태에서 프로그램을 설치하고,<br>
그 상태로 커밋을 한 다면, hostname과 일치하는 컨테이너는 문제가 없을 테지만,<br>
그렇지 못한 컨테이너는 프로그램에 따라서 실행이 안된다.<br>작업에 들어가기 앞서!
<br>(L): local에서 하는 작업

<br>windows: wsl
<br>mac: terminal


<br>(C): container에서 하는 작업
<br>(D): indocker에서 하는 작업
<br><br><br>docker pull ubuntu:24.04
복사<br><br>docker tag ubuntu:24.04 ubuntu-in-docker:24.04
복사<br><br><br><br>docker container run -itd --privileged ubuntu-in-docker:24.04
복사<br>그렇군?--privileged 옵션을 넣어 줘야 docker를 사용할 수 있음<br><br>docker container exec -it 컨테이너_이름 bash
복사<br><br>apt-get update
apt-get install -y sudo
apt-get install -y curl
apt-get install -y net-tools
apt-get install -y git
apt-get install -y wget
apt-get install -y tree
apt-get install -y openssh-server
apt-get install -y vim
curl -sSL get.docker.com | sh
복사<br><br><a data-href="pyenv 설치" href="root\dev\data-pipe-line\server\pyenv-설치.html" class="internal-link" target="_self" rel="noopener">pyenv 설치</a><br><br>vi ~/.bashrc
복사<br>if ! sudo service docker status &gt; /dev/null 2&gt;&amp;1; then
  sudo service docker start
fi
복사<br><br>docker container commit 컨테이너_이름 ubuntu-in-docker:24.04-basic
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br>docker container run -itd \
  --privileged \
  ubuntu-in-docker:24.04-basic \
  /bin/bash -c "sudo service ssh start &amp;&amp; tail -f /dev/null"
복사<br><br>docker container exec -it 컨테이너_이름 bash
복사<br><br>ssh-keygen -t rsa
cd .ssh
cat id_rsa.pub &gt; authorized_keys
복사<br><br>vi ~/.shh/config
복사<br>Host server*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br><br>docker container commit 컨테이너_이름 ubuntu-in-docker:24.04-ssh-key
복사<br><br><br><br>docker network create --gateway 172.20.0.1 --subnet 172.20.0.0/24 basic
복사<br><br>docker container run -itd \
  --privileged \
  --name server1 \
  --network basic \
  --ip 172.20.0.11 \
  --hostname server1 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key


docker container run -itd \
  --privileged \
  --name server2 \
  --network basic \
  --ip 172.20.0.12 \
  --hostname server2 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key


docker container run -itd \
  --privileged \
  --name server3 \
  --network basic \
  --ip 172.20.0.13 \
  --hostname server3 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key
복사<br><br>docker container exec -it server1 bash
복사<br><br>vi ssh_check.sh
복사<br> <a data-href="shell ssh 여러 개 접속 테스트" href="root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html" class="internal-link" target="_self" rel="noopener">shell ssh 여러 개 접속 테스트</a><br>chmod +x ssh_check.sh

./ssh_check.sh
복사<br><br>docker container rm -f server1 server2 server3
복사<br><br><br><br>docker container run -itd \
  --privileged \
  --name server1 \
  --network basic \
  --ip 172.20.0.11 \
  --hostname server1 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key


docker container run -itd \
  --privileged \
  --name server2 \
  --network basic \
  --ip 172.20.0.12 \
  --hostname server2 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key


docker container run -itd \
  --privileged \
  --name server3 \
  --network basic \
  --ip 172.20.0.13 \
  --hostname server3 \
  --add-host server1:172.20.0.11 \
  --add-host server2:172.20.0.12 \
  --add-host server3:172.20.0.13 \
  ubuntu-in-docker:24.04-ssh-key
복사<br>git clone https://github.com/namugach/pipelineShell
복사<br>mv pipelineShell run
복사<br>./run/ssh/check.sh
복사<br>./run/docker/pull.sh namugach/ubuntu-pipeline:24.04-kafka
복사]]></description><link>root\dev\data-pipe-line\docker-in-docker로-local에서-클러스터-환경-만들기.html</link><guid isPermaLink="false">root/dev/data pipe line/docker in docker로 local에서 클러스터 환경 만들기.md</guid><pubDate>Thu, 08 Aug 2024 17:29:09 GMT</pubDate></item><item><title><![CDATA[docker pipe line 환경 구축 v0.1]]></title><description><![CDATA[ 
 <br>커밋할 때절 때 hostname으로 이름 지은 것으로 하지 말아야함.<br>
그렇지 않고, hostname이 지정된 상태에서 프로그램을 설치하고,<br>
그 상태로 커밋을 한 다면, hostname과 일치하는 컨테이너는 문제가 없을 테지만,<br>
그렇지 못한 컨테이너는 프로그램에 따라서 실행이 안된다.<br>작업에 들어가기 앞서!
<br>(L): local에서 하는 작업

<br>windows: wsl
<br>mac: terminal


<br>(C): container에서 하는 작업
<br><br>pull 받으려 하는데 로그인 하라고?
<br><a rel="noopener" class="external-link" href="http://dockerhub.com/" target="_blank">http://dockerhub.com/</a> 에서
<br>아이디 없으면 만들고
<br>있으면 확인하고
<br>shell에서 아래의 명령어를 입력하여 로그인

docker login
복사<br><br>docker pull ubuntu:24.04
복사<br><br>docker tag ubuntu:24.04 ubuntu-pipeline:24.04
복사<br><br>docker container run -it -d ubuntu-pipeline:24.04
복사<br><br><br><br>apt-get update
apt-get install -y sudo
apt-get install -y curl
apt-get install -y net-tools
apt-get install -y git
apt-get install -y wget
apt-get install -y tree
apt-get install -y openssh-server
apt-get install -y vim
복사<br><br>매우 주의!root 계정으로 작업 할 것!<br><br>passwd
복사<br><br>passwd ubuntu
복사<br><br><a data-href="우분투 sudo 비번 생략하기" href="root\dev\data-pipe-line\server\local\우분투-sudo-비번-생략하기.html" class="internal-link" target="_self" rel="noopener">우분투 sudo 비번 생략하기</a><br>echo 'ubuntu ALL=(ALL) NOPASSWD:ALL' &gt;&gt; /etc/sudoers
복사<br><br><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-basic
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br><br>docker container run -itd \
  -u ubuntu \
  -w /home/ubuntu \
  ubuntu-pipeline:24.04-basic
복사<br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-user-setting
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-user-setting 
복사<br><br><a data-href="pyenv 설치" href="root\dev\data-pipe-line\server\pyenv-설치.html" class="internal-link" target="_self" rel="noopener">pyenv 설치</a><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-pyenv
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-pyenv
복사<br><br><a data-href="docker 우분투 mysql 설치" href="root\dev\data-pipe-line\server\docker-우분투-mysql-설치.html" class="internal-link" target="_self" rel="noopener">docker 우분투 mysql 설치</a><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-mysql
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-mysql
복사<br><a data-href="docker container 시작 시 명령어 시작" href="root\dev\data-pipe-line\server\local\docker-container-시작-시-명령어-시작.html" class="internal-link" target="_self" rel="noopener">docker container 시작 시 명령어 시작</a><br><br>docker container run -itd \
  ubuntu-pipeline:24.04-mysql\
  /bin/bash -c "sudo service ssh start &amp;&amp; sudo service mysql start &amp;&amp; tail -f /dev/null"
복사<br><br><br>sudo service --status-all
복사<br><br> [ - ]  dbus
 [ + ]  mysql
 [ - ]  procps
 [ + ]  ssh
 [ - ]  x11-common
복사<br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-service-auto-load
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-service-auto-load
복사<br><br><a data-href="우분투 hosts를 설정하여 ssh 편하게 이동하기" href="root\dev\data-pipe-line\server\local\우분투-hosts를-설정하여-ssh-편하게-이동하기.html" class="internal-link" target="_self" rel="noopener">우분투 hosts를 설정하여 ssh 편하게 이동하기</a><br><br>ssh-keygen -t rsa
cd .ssh
cat id_rsa.pub &gt; authorized_keys
복사<br><br>Host server*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br><a data-href="ssh 접속시 yes 입력 생략하기" href="root\dev\data-pipe-line\server\local\ssh-접속시-yes-입력-생략하기.html" class="internal-link" target="_self" rel="noopener">ssh 접속시 yes 입력 생략하기</a><br><br>cd
mkdir run
cd run
vi ssh_check.sh
복사<br><a data-href="shell ssh 여러 개 접속 테스트" href="root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html" class="internal-link" target="_self" rel="noopener">shell ssh 여러 개 접속 테스트</a> 를 작성<br>sudo chmod +x ssh_check.sh
복사<br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-ssh-key
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br>그리고commit 된 이미로 3개의 컨테이너를 띄움<br>docker run -itd \
  --name server1 \
  --hostname server1 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-ssh-key

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-ssh-key

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-ssh-key
복사<br><br><a data-href="shell ssh 여러 개 접속 테스트" href="root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html" class="internal-link" target="_self" rel="noopener">shell ssh 여러 개 접속 테스트</a><br>./run/ssh_check.sh
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-ssh-key
복사<br><a data-href="kafka 설치#JAVA" href="root\dev\data-pipe-line\kafka\kafka-설치.html#JAVA" class="internal-link" target="_self" rel="noopener">kafka 설치 &gt; JAVA</a><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-java
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br>Checkroot 계정으로 java를 설치 했을 때는<br>
hostname이 java 설치 했을 때와 다르면<br>
컨테이너가 죽는 현상이 발견 됐다.<br>
하지만 이게 과연 root 계정이기 때문에 발생 된 것인지에 대한 건 모른다.<br>
때문에 아래의 테스트를 꼭 거치고 다음 스텝을 밟아야 한다<br><br>docker run -itd \
  --name server1 \
  --hostname server1 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-java

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-java

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-java

복사<br><br>docker container rm -f server1 server2 server3 
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-java
복사<br><a data-href="kafka 설치#install" href="root\dev\data-pipe-line\kafka\kafka-설치.html#install" class="internal-link" target="_self" rel="noopener">kafka 설치 &gt; install</a><br><a data-href="kafka 설정 및 실행" href="root\dev\data-pipe-line\kafka\kafka-설정-및-실행.html" class="internal-link" target="_self" rel="noopener">kafka 설정 및 실행</a><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-kafka
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br>정말 위험해!docker 컨테이너로 서버를 start 할 때 순서를 꼭 치켜야 한다.<br>
그렇지 않으면 ip가 켜진 순서대로 할당 받기 때문에<br>
kafka에서 정상 작동을 기대하기 어려워진다.<br>docker run -itd \
  --name server1 \
  --hostname server1 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka
복사<br><br><br><a data-href="kafka 클러스터 서버 시동" href="root\dev\data-pipe-line\kafka\kafka-클러스터-서버-시동.html" class="internal-link" target="_self" rel="noopener">kafka 클러스터 서버 시동</a><br><a data-href="kafka에 csv로 데이터 흘리기" href="root\dev\data-pipe-line\kafka\kafka에-csv로-데이터-흘리기.html" class="internal-link" target="_self" rel="noopener">kafka에 csv로 데이터 흘리기</a><br><a data-href="Kafka Consumer와 MySQL에 데이터 적재하기" href="root\dev\data-pipe-line\kafka\kafka-consumer와-mysql에-데이터-적재하기.html" class="internal-link" target="_self" rel="noopener">Kafka Consumer와 MySQL에 데이터 적재하기</a>]]></description><link>root\dev\data-pipe-line\docker-pipe-line-환경-구축-v0.1.html</link><guid isPermaLink="false">root/dev/data pipe line/docker pipe line 환경 구축 v0.1.md</guid><pubDate>Thu, 08 Aug 2024 16:55:21 GMT</pubDate></item><item><title><![CDATA[docker pipe line 환경 구축 v0.2]]></title><description><![CDATA[ 
 <br>docker pipe line 환경 구축 v0.2<br>커밋할 때절 때 hostname으로 이름 지은 것으로 하지 말아야함.<br>
그렇지 않고, hostname이 지정된 상태에서 프로그램을 설치하고,<br>
그 상태로 커밋을 한 다면, hostname과 일치하는 컨테이너는 문제가 없을 테지만,<br>
그렇지 못한 컨테이너는 프로그램에 따라서 실행이 안된다.<br>작업에 들어가기 앞서!
<br>(L): local에서 하는 작업

<br>windows: wsl
<br>mac: terminal


<br>(C): container에서 하는 작업
<br><br>docker container run -it -d ubuntu-pipeline:24.04
복사<br><br><br><br>vi .bashrc
복사<br>내용을 전부 지운 뒤<br><a data-href="우분투 root 계정일 때 bash 일반 계정처럼 만들기" href="root\dev\data-pipe-line\server\우분투-root-계정일-때-bash-일반-계정처럼-만들기.html" class="internal-link" target="_self" rel="noopener">우분투 root 계정일 때 bash 일반 계정처럼 만들기</a>를 붙여 넣는다.<br><br>. ~/.bashrc
복사<br><br><br>passwd
복사<br><br><br>apt-get update
apt-get install -y sudo
apt-get install -y curl
apt-get install -y net-tools
apt-get install -y git
apt-get install -y wget
apt-get install -y tree
apt-get install -y vim
apt-get install -y openssh-server
apt-get install -y mysql-server
apt-get install -y openjdk-17-jre-headless
복사<br>중간에 <br>Please select the geographic area in which you live. Subsequent configuration questions
will narrow this down by presenting a list of cities, representing the time zones in
which they are located.

  1. Africa   3. Antarctica  5. Asia     
  2. America  4. Arctic      6. Atlantic 
Geographic area: 5

Please select the city or region corresponding to your time zone.

  49. Macau         67. Samarkand    
  50. Magadan       68. Seoul        
  51. Makassar      69. Shanghai     
Time zone: 68
복사<br>time zones 선택하라고 나온다.<br>
참고로 위의 보기는 많이 생략한 내용임으로 위처럼 나오지 않는다.<br>Check
<br>
첫번째 에서<br>
5. Asia

<br>
두번 째 에서<br>
68. Seoul

<br><br><br>wow!openssh-server 설치하면서 같이 설치됨.. ㅎ...<br><br>ln -s /bin/python3 /bin/python
복사<br>이렇게 링크를 만들어 줘야<br>
python shell에서 명령어를 입력 했을 때, python이 호출 된다.<br><br><a data-href="python pip venv 가상환경 사용하지 않기" href="root\dev\data-pipe-line\server\python-pip-venv-가상환경-사용하지-않기.html" class="internal-link" target="_self" rel="noopener">python pip venv 가상환경 사용하지 않기</a><br>sudo rm /usr/lib/python3.12/EXTERNALLY-MANAGED
복사<br>이걸 끄지 않으면<br>Errorerror: externally-managed-environment
× This environment is externally managed<br>
╰─&gt; To install Python packages system-wide, try apt install<br>
python3-xyz, where xyz is the package you are trying to<br>
install.
If you wish to install a non-Debian-packaged Python package,
create a virtual environment using python3 -m venv path/to/venv.
Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
sure you have python3-full installed.

If you wish to install a non-Debian packaged Python application,
it may be easiest to use pipx install xyz, which will manage a
virtual environment for you. Make sure you have pipx installed.

See /usr/share/doc/python3.12/README.venv for more information.
복사<br>이런 게 나옴.<br>참고로 python3.12 이 부분은 python -V  로 버전을 확인 한 후.<br>
각자의 버전에 알맞게 수정하는 게 이로움.<br>그리고 설정을 끄고 나서 pip install로 패키지를 설치하고 나면<br>WarningWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: <a rel="noopener" class="external-link" href="https://pip.pypa.io/warnings/venv" target="_blank">https://pip.pypa.io/warnings/venv</a><br>이런 게 쓰는데, 신경 쓰지 않아도 괜찮음.<br>
똬카(docker)니까<br><br>apt install python3-pip
복사<br><br><br><a data-href="docker 우분투 mysql 설치#시작" href="root\dev\data-pipe-line\server\docker-우분투-mysql-설치.html#시작" class="internal-link" target="_self" rel="noopener">docker 우분투 mysql 설치 &gt; 시작</a><br>이미 기본 설치는 해버렸기 때문에 시작 부 터 설정만 해주면 된다.<br><br><br><br>service ssh start
복사<br><br><br>vi /etc/ssh/sshd_config
복사<br><br>Port 22
ListenAddress 0.0.0.0
PermitRootLogin yes
복사<br><br>ssh-keygen -t rsa
cd .ssh
cat id_rsa.pub &gt; authorized_keys
복사<br><br><br>vi ~/.ssh/config
복사<br><br>Host server*
  StrictHostKeyChecking no
  UserKnownHostsFile=/dev/null
복사<br><a data-href="ssh 접속시 yes 입력 생략하기" href="root\dev\data-pipe-line\server\local\ssh-접속시-yes-입력-생략하기.html" class="internal-link" target="_self" rel="noopener">ssh 접속시 yes 입력 생략하기</a><br><br><br><br><br>vi ~/.bashrc
복사<br><br>if ! sudo service ssh status &gt; /dev/null 2&gt;&amp;1; then
  sudo service ssh start
fi

if ! sudo service mysql status &gt; /dev/null 2&gt;&amp;1; then
  sudo service mysql start
fi
복사<br><br>. ~/.bashrc
복사<br><br><br>sudo service --status-all
복사<br><br> [ - ]  dbus
 [ + ]  mysql
 [ - ]  procps
 [ + ]  ssh
 [ - ]  x11-common
복사<br><br><br><br><br>vi ~/.bashrc
복사<br><br>Attention둘 중 하나만 적용한다.<br>
amd는 intel 혹은 amd용이고<br>
arm은 m1 시리즈를 사용하고 있는 mac이다.<br>
맨 끝에를 보면 mac용은 arm64라고 적혀져 있는 걸 볼수 있다.
dpkg -s libc6 | grep Arch
복사
위의 명령어로 자신의 아키텍쳐를 확인 할 수 있다.<br><br>export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64
복사<br><br>export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-arm64
복사<br><br>. .bashrc
복사<br><br>java --version
복사<br><br><br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-base
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br><br>docker image ls
복사<br><br>docker image ls -a
복사<br><br>docker image ls | grep base
복사<br>REPOSITORY                 TAG            
ubuntu-pipeline            24.04-base     
복사<br>이런 식으로 나온다면 성공<br><br><br><br>docker run -itd \
  --name server1 \
  --hostname server1 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-base

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-base

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-base
복사<br><br>./run/ssh_check.sh
복사<br><a data-href="shell ssh 여러 개 접속 테스트" href="root\dev\data-pipe-line\자동화\shell-ssh-여러-개-접속-테스트.html" class="internal-link" target="_self" rel="noopener">shell ssh 여러 개 접속 테스트</a><br><br>docker container rm server1 server2 server3
복사<br><br><br><br>docker container run -itd ubuntu-pipeline:24.04-base
복사<br><br><a data-href="kafka 설치#install" href="root\dev\data-pipe-line\kafka\kafka-설치.html#install" class="internal-link" target="_self" rel="noopener">kafka 설치 &gt; install</a><br>cd

mkdir app
cd app

mkdir kafka
cd kafka

wget https://downloads.apache.org/kafka/3.6.2/kafka_2.13-3.6.2.tgz

tar -xzf kafka_2.13-3.6.2.tgz
복사<br><br><a data-href="kafka 설정 및 실행" href="root\dev\data-pipe-line\kafka\kafka-설정-및-실행.html" class="internal-link" target="_self" rel="noopener">kafka 설정 및 실행</a><br><br>cd
mkdir zkdata
cd zkdata
echo 1 &gt; ./myid
복사<br><br>vi /root/app/kafka/kafka_2.13-3.6.2/config/server.properties
복사<br><a data-href="kafka server.properties" href="root\dev\data-pipe-line\kafka\kafka-server.properties.html" class="internal-link" target="_self" rel="noopener">kafka server.properties</a>  내용을 전체 복사 붙여 넣기<br><br>vi /root/app/kafka/kafka_2.13-3.6.2/config/zookeeper.properties
복사<br>
dataDir=/home/ubuntu/zkdata
# the port at which the clients will connect
clientPort=2181
tickTime=2000
initLimit=10
syncLimit=5
server.1=172.17.0.2:2888:3888
server.2=172.17.0.3:2888:3888
server.3=172.17.0.4:2888:3888
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
복사<br>위의 내용을 전체 복사 붙여 넣기<br><br><br>docker container ls
복사<br><br>docker container commit 컨테이너_이름 ubuntu-pipeline:24.04-kafka
복사<br><br>docker container rm -f 컨테이너_이름
복사<br><br>정말 위험해!docker 컨테이너로 서버를 start 할 때 순서를 꼭 치켜야 한다.<br>
그렇지 않으면 ip가 켜진 순서대로 할당 받기 때문에<br>
kafka에서 정상 작동을 기대하기 어려워진다.<br>docker run -itd \
  --name server1 \
  --hostname server1 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka

docker run -itd \
  --name server2 \
  --hostname server2 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka

docker run -itd \
  --name server3 \
  --hostname server3 \
  --add-host server1:172.17.0.2 \
  --add-host server2:172.17.0.3 \
  --add-host server3:172.17.0.4 \
  ubuntu-pipeline:24.04-kafka
복사<br><br><br><a data-href="kafka 클러스터 서버 시동" href="root\dev\data-pipe-line\kafka\kafka-클러스터-서버-시동.html" class="internal-link" target="_self" rel="noopener">kafka 클러스터 서버 시동</a><br><a data-href="kafka에 csv로 데이터 흘리기" href="root\dev\data-pipe-line\kafka\kafka에-csv로-데이터-흘리기.html" class="internal-link" target="_self" rel="noopener">kafka에 csv로 데이터 흘리기</a><br><a data-href="Kafka Consumer와 MySQL에 데이터 적재하기" href="root\dev\data-pipe-line\kafka\kafka-consumer와-mysql에-데이터-적재하기.html" class="internal-link" target="_self" rel="noopener">Kafka Consumer와 MySQL에 데이터 적재하기</a>]]></description><link>root\dev\data-pipe-line\docker-pipe-line-환경-구축-v0.2.html</link><guid isPermaLink="false">root/dev/data pipe line/docker pipe line 환경 구축 v0.2.md</guid><pubDate>Thu, 29 Aug 2024 02:45:15 GMT</pubDate></item><item><title><![CDATA[docker pipe line 환경 구축 v0.3]]></title><description><![CDATA[ 
 <br>더 자세한 메뉴얼은<a data-tooltip-position="top" aria-label="https://github.com/namugach/pipeline_base" rel="noopener" class="external-link" href="https://github.com/namugach/pipeline_base" target="_blank">여기</a>에서 확인!<br><br><br>git clone https://github.com/namugach/pipeline_base
복사<br><br>cd pipeline_base
복사<br><br>./install
복사<br><br><br><br>창을2개 띄워서 나중에 모니터링 할 것도 미리서 준비 해놓자<br>docker container exec -it server1 bash -c "cd /root &amp;&amp; bash"
복사<br><br>./run/kafka/start_cluster.sh
복사<br><br>./test/monitor.sh
복사<br><br>./test/main.sh
복사<br><br><br>mysql -u ubuntu -p1234
복사<br><br>
USE my_data;
SELECT * FROM test_a;
복사]]></description><link>root\dev\data-pipe-line\docker-pipe-line-환경-구축-v0.3.html</link><guid isPermaLink="false">root/dev/data pipe line/docker pipe line 환경 구축 v0.3.md</guid><pubDate>Thu, 29 Aug 2024 04:31:14 GMT</pubDate></item><item><title><![CDATA[Kafka Consumer와 MySQL에 데이터 적재하기v0.2]]></title><description><![CDATA[ 
 <br><br>아래는 주어진 코드에 Kafka의 속도 조절 기능을 추가한 버전이야.<br>from kafka import KafkaProducer, KafkaConsumer
import pandas as pd
import json
from sqlalchemy import create_engine, Table, Column, MetaData, inspect
from sqlalchemy.types import String, BigInteger, Float, DateTime
import pymysql

CSV_DATA = 'data_mini.csv'

# 데이터베이스 연결 설정
DATABASE_USER = 'ubuntu'
DATABASE_PASSWORD = '1234'
DATABASE_HOST = 'localhost'
DATABASE_NAME = 'my_data'

# Kafka 설정
KAFKA_BOOTSTRAP_SERVERS = ['172.20.0.10:9092', '172.20.0.11:9092', '172.20.0.12:9092']
KAFKA_TOPIC = 'my_data'
KAFKA_GROUP_ID = 'my-group'  # group_id 설정

# 데이터베이스 연결 함수
def create_database_if_not_exists():
    connection = pymysql.connect(
        host=DATABASE_HOST,
        user=DATABASE_USER,
        password=DATABASE_PASSWORD
    )
    try:
        with connection.cursor() as cursor:
            cursor.execute(f"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}")
    finally:
        connection.close()

# SQLAlchemy 데이터 타입 매핑 함수
def get_sqlalchemy_type(dtype):
    if pd.api.types.is_integer_dtype(dtype):
        return BigInteger
    elif pd.api.types.is_float_dtype(dtype):
        return Float
    elif pd.api.types.is_datetime64_any_dtype(dtype):
        return DateTime
    else:
        return String(length=255)

# 데이터베이스 및 테이블 설정
def setup_database_and_table(csv_path):
    engine = create_engine(f'mysql+pymysql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}/{DATABASE_NAME}')
    
    df = pd.read_csv(csv_path)
    
    metadata = MetaData()
    
    columns = [Column(col, get_sqlalchemy_type(df[col].dtype)) for col in df.columns]
    table = Table('test_a', metadata, *columns)
    
    inspector = inspect(engine)
    if not inspector.has_table('test_a'):
        metadata.create_all(engine)
        print(f"Table 'test_a' created successfully from CSV file.")
    else:
        print(f"Table 'test_a' already exists.")

def produce_csv_to_kafka(file_path, topic):
    data = pd.read_csv(file_path)
    
    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        linger_ms=1000,  # 메시지를 1초 동안 모아서 배치로 보냄
        batch_size=32 * 1024  # 32KB의 배치를 구성하여 전송
    )
    
    for _, row in data.iterrows():
        message = row.to_dict()
        producer.send(topic, value=message)
    
    producer.flush()
    producer.close()
    print("CSV 데이터를 Kafka에 게시 완료.")

def load_data_to_mysql(data):
    engine = create_engine(f'mysql+pymysql://{DATABASE_USER}:{DATABASE_PASSWORD}@{DATABASE_HOST}/{DATABASE_NAME}')
    df = pd.DataFrame(data)
    df.to_sql('test_a', engine, if_exists='append', index=False)

consumer = KafkaConsumer(
    KAFKA_TOPIC,
    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    group_id=KAFKA_GROUP_ID,  # group_id 추가
    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
    max_poll_records=10  # 한 번의 poll()에서 가져오는 메시지 수를 10개로 제한
)

create_database_if_not_exists()
setup_database_and_table(CSV_DATA)

produce_csv_to_kafka(CSV_DATA, KAFKA_TOPIC)

data_buffer = []
buffer_limit = 100
timeout_ms = 5000  # 5초 동안 새로운 메시지가 없으면 타임아웃

while True:
    message_pack = consumer.poll(timeout_ms=timeout_ms)
    
    if not message_pack:
        break
    
    for tp, messages in message_pack.items():
        for message in messages:
            data_buffer.append(message.value)
            
            if len(data_buffer) &gt;= buffer_limit:
                load_data_to_mysql(data_buffer)
                data_buffer = []
                consumer.commit()

if data_buffer:
    load_data_to_mysql(data_buffer)
    consumer.commit()

consumer.close()

print("Kafka에서 MySQL로 데이터 적재 완료.")
Run복사<br><br>
<br>생산자 측 (KafkaProducer):
<br>
<br>linger.ms=1000: 메시지를 전송하기 전에 최대 1초 동안 데이터를 모아서 한 번에 보내도록 설정했어. 이를 통해 초당 메시지 전송 빈도를 조절할 수 있어.
<br>batch_size=32 * 1024: 32KB의 데이터가 모이면 한 번에 전송하게 설정했어. 이는 생산자가 작은 메시지를 자주 보내지 않고, 일정 크기만큼 모아서 보내도록 하여 네트워크 효율을 높일 수 있어.
<br>
<br>소비자 측 (KafkaConsumer):
<br>
<br>max_poll_records=10: 한 번의 poll() 호출로 가져올 수 있는 메시지 수를 10개로 제한했어. 이를 통해 소비자가 처리해야 할 데이터의 양을 조절하고, 초당 처리량을 낮출 수 있어.
<br>
<br>타임아웃 및 버퍼링:
<br>
<br>timeout_ms=5000: Kafka 소비자에서 데이터를 가져올 때, 5초 동안 새로운 메시지가 없으면 타임아웃되도록 설정했어. 이 값을 통해 소비자가 새로운 데이터를 기다리는 시간을 조절할 수 있어.
<br>버퍼링: 소비자는 100개의 메시지를 버퍼에 모은 후 MySQL로 저장해. 이로 인해 데이터베이스에 대한 트래픽이 줄고, 효율적으로 데이터를 처리할 수 있어.
<br><br>이렇게 설정하면 Kafka를 사용해 데이터를 전송하고 처리하는 속도를 조절할 수 있어. 생산자와 소비자 양쪽에서 데이터를 모아 배치로 처리하거나, 일정 수량만큼 한 번에 가져와 처리할 수 있도록 설정한 것이 주요 포인트야. 이로 인해 시스템의 부하를 줄이면서도 안정적으로 데이터를 처리할 수 있게 돼.]]></description><link>root\dev\data-pipe-line\kafka-consumer와-mysql에-데이터-적재하기v0.2.html</link><guid isPermaLink="false">root/dev/data pipe line/Kafka Consumer와 MySQL에 데이터 적재하기v0.2.md</guid><pubDate>Wed, 21 Aug 2024 17:11:16 GMT</pubDate></item><item><title><![CDATA[데이터 저장소 비교]]></title><description><![CDATA[ 
 <br>데이터 저장소 비교하기<br><br>한국 회사에서 많이 사용하는 데이터 저장소 툴을 취업에 중요한 요소로 고려해 순서대로 정리해 드릴게요. 다만, 특정 도구의 사용 여부는 회사의 규모나 분야에 따라 달라질 수 있으므로, 아래는 전반적인 경향을 바탕으로 한 순위입니다.<br><br>
<br>사용 빈도: 국내 중소, 대기업을 막론하고 SQL 기반의 RDBMS는 여전히 많이 사용됩니다. 특히 금융, 제조, 서비스 업계 등에서 데이터 무결성과 트랜잭션 처리가 중요한 곳에서 PostgreSQL을 포함한 관계형 데이터베이스가 널리 사용됩니다.
<br>취업 시 중요성: 데이터 엔지니어로서 SQL에 대한 이해와 능숙한 사용은 필수적입니다. 관계형 데이터베이스는 여전히 기본 기술로 여겨집니다.
<br><br>
<br>사용 빈도: 클라우드 기술이 빠르게 확산되면서 S3와 같은 클라우드 스토리지를 도입하는 회사가 많아졌습니다. 특히 스타트업과 IT 서비스, 대기업 클라우드 전환 프로젝트에서 많이 사용됩니다.
<br>취업 시 중요성: 클라우드 기술, 특히 AWS에 대한 이해는 매우 중요합니다. 많은 기업이 클라우드 서비스를 적극적으로 사용하고 있기 때문에 클라우드와 관련된 경험이 큰 도움이 됩니다.
<br><br>
<br>사용 빈도: 빅데이터 분석 및 처리 기술을 활용하는 대기업(네이버, 카카오, 삼성 등)에서 많이 사용됩니다. 특히 금융, 통신, 대규모 IT 서비스 회사에서 대량 데이터를 분석하는 데 사용합니다.
<br>취업 시 중요성: 빅데이터 처리는 데이터 엔지니어링의 중요한 부분으로, 특히 대규모 데이터를 다루는 기업에서 Hive와 같은 하둡 생태계를 활용한 기술이 매우 중요합니다.
<br><br>
<br>사용 빈도: 대용량 데이터를 처리하는 회사에서 주로 사용되며, Hive와 함께 사용하는 경우가 많습니다. 하지만 국내에서는 AWS와 같은 클라우드 기반 데이터 저장소로의 전환이 점차 증가하면서 HDFS의 사용은 상대적으로 감소하고 있습니다.
<br>취업 시 중요성: 빅데이터 관련 직무에서 여전히 중요한 기술이지만, 클라우드 기반 스토리지로 전환하는 추세를 감안할 때 S3와 같은 클라우드 기술의 중요성이 더 커지고 있습니다.
]]></description><link>root\dev\데이터-저장소-비교.html</link><guid isPermaLink="false">root/dev/데이터 저장소 비교.md</guid><pubDate>Sun, 08 Sep 2024 05:22:58 GMT</pubDate></item><item><title><![CDATA[main]]></title><description><![CDATA[ 
 <br>Note저장 공유용]]></description><link>root\main.html</link><guid isPermaLink="false">root/main.md</guid><pubDate>Thu, 18 Apr 2024 16:46:09 GMT</pubDate></item></channel></rss>